<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>1주차</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-default { background-color: rgba(84, 72, 49, 0.08); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="22fb8b2c-1c23-80de-8760-d64ce0b5352b" class="page sans"><header><h1 class="page-title">1주차</h1><p class="page-description"></p></header><div class="page-body"><h3 id="233b8b2c-1c23-80da-b7eb-cc34c1e7e9ec" class="">1. 실습 환경 구성</h3><ul id="233b8b2c-1c23-809f-9c70-ff770e236ccd" class="toggle"><li><details open=""><summary><code>mac M</code> 사용자</summary><ol type="1" id="233b8b2c-1c23-80d4-b2da-f3dc65d56879" class="numbered-list" start="1"><li>VirtualBox 설치 - <a href="https://www.virtualbox.org/wiki/Changelog">Release</a><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80e2-9597-df1f0084633c" class="code"><code class="language-Bash"># VirtualBox 설치
brew install --cask virtualbox

VBoxManage --version
7.1.10r169112</code></pre></li></ol><ol type="1" id="233b8b2c-1c23-8046-a2ca-e6c0f95e3f18" class="numbered-list" start="2"><li>Vagrant 설치<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-804f-84f2-e2b58ea1a08e" class="code"><code class="language-Bash"># Vagrant 설치
brew install --cask vagrant

vagrant version    
Installed Version: 2.4.7</code></pre></li></ol><ul id="233b8b2c-1c23-80b2-846a-d82dd57b5a55" class="bulleted-list"><li style="list-style-type:disc"><strong>실습 환경 소개</strong><figure id="233b8b2c-1c23-80b6-a6b6-d7e349657bdc" class="image"><a href="images/k8s-first.png"><img style="width:672px" src="images/k8s-first.png"/></a></figure><ul id="233b8b2c-1c23-800e-975d-cf2fb5daa530" class="bulleted-list"><li style="list-style-type:circle">기본 배포 가상 머신 : k8s-ctr, k8s-w1, k8s-w2<ul id="233b8b2c-1c23-8011-b848-f6488f6344f2" class="bulleted-list"><li style="list-style-type:square">eth0 : 10.0.2.15 모든 노드가 동일</li></ul><ul id="233b8b2c-1c23-80af-b6a7-c548e4f98f8f" class="bulleted-list"><li style="list-style-type:square">eth1 : 192.168.10.100, 101, 102</li></ul></li></ul><ul id="233b8b2c-1c23-801e-8bc7-fb506b3a6d1c" class="bulleted-list"><li style="list-style-type:circle">초기 프로비저닝으로 kubeadm init 과 join 실행됨</li></ul><ul id="233b8b2c-1c23-80cf-adfb-e6570eed1c9c" class="bulleted-list"><li style="list-style-type:circle">CNI 미설치 상태로 배포 완료됨</li></ul><p id="233b8b2c-1c23-807a-9dcb-c31fafa2e6b9" class="">
</p></li></ul><ul id="233b8b2c-1c23-80e9-805d-f49da3946ef2" class="bulleted-list"><li style="list-style-type:disc"><strong>실습 환경 배포 파일 작성</strong><ul id="233b8b2c-1c23-801e-b6bf-c6b10bd64e32" class="toggle"><li><details open=""><summary>Vagrantfile : 가상머신 정의, 부팅 시 초기 프로비저닝 설정</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80f5-a319-e4295da129f5" class="code"><code class="language-Bash"># Variables
K8SV = &#x27;1.33.2-1.1&#x27; # Kubernetes Version : apt list -a kubelet , ex) 1.32.5-1.1
CONTAINERDV = &#x27;1.7.27-1&#x27; # Containerd Version : apt list -a containerd.io , ex) 1.6.33-1
N = 2 # max number of worker nodes

# Base Image  https://portal.cloud.hashicorp.com/vagrant/discover/bento/ubuntu-24.04
## Rocky linux Image https://portal.cloud.hashicorp.com/vagrant/discover/rockylinux
BOX_IMAGE = &quot;bento/ubuntu-24.04&quot;
BOX_VERSION = &quot;202502.21.0&quot;

Vagrant.configure(&quot;2&quot;) do |config|
#-ControlPlane Node
    config.vm.define &quot;k8s-ctr&quot; do |subconfig|
      subconfig.vm.box = BOX_IMAGE
      subconfig.vm.box_version = BOX_VERSION
      subconfig.vm.provider &quot;virtualbox&quot; do |vb|
        vb.customize [&quot;modifyvm&quot;, :id, &quot;--groups&quot;, &quot;/Cilium-Lab&quot;]
        vb.customize [&quot;modifyvm&quot;, :id, &quot;--nicpromisc2&quot;, &quot;allow-all&quot;]
        vb.name = &quot;k8s-ctr&quot;
        vb.cpus = 2
        vb.memory = 2048
        vb.linked_clone = true
      end
      subconfig.vm.host_name = &quot;k8s-ctr&quot;
      subconfig.vm.network &quot;private_network&quot;, ip: &quot;192.168.10.100&quot;
      subconfig.vm.network &quot;forwarded_port&quot;, guest: 22, host: 60000, auto_correct: true, id: &quot;ssh&quot;
      subconfig.vm.synced_folder &quot;./&quot;, &quot;/vagrant&quot;, disabled: true
      subconfig.vm.provision &quot;shell&quot;, path: &quot;init_cfg.sh&quot;, args: [ K8SV, CONTAINERDV]
      subconfig.vm.provision &quot;shell&quot;, path: &quot;k8s-ctr.sh&quot;, args: [ N ]
    end

#-Worker Nodes Subnet1
  (1..N).each do |i|
    config.vm.define &quot;k8s-w#{i}&quot; do |subconfig|
      subconfig.vm.box = BOX_IMAGE
      subconfig.vm.box_version = BOX_VERSION
      subconfig.vm.provider &quot;virtualbox&quot; do |vb|
        vb.customize [&quot;modifyvm&quot;, :id, &quot;--groups&quot;, &quot;/Cilium-Lab&quot;]
        vb.customize [&quot;modifyvm&quot;, :id, &quot;--nicpromisc2&quot;, &quot;allow-all&quot;]
        vb.name = &quot;k8s-w#{i}&quot;
        vb.cpus = 2
        vb.memory = 1536
        vb.linked_clone = true
      end
      subconfig.vm.host_name = &quot;k8s-w#{i}&quot;
      subconfig.vm.network &quot;private_network&quot;, ip: &quot;192.168.10.10#{i}&quot;
      subconfig.vm.network &quot;forwarded_port&quot;, guest: 22, host: &quot;6000#{i}&quot;, auto_correct: true, id: &quot;ssh&quot;
      subconfig.vm.synced_folder &quot;./&quot;, &quot;/vagrant&quot;, disabled: true
      subconfig.vm.provision &quot;shell&quot;, path: &quot;init_cfg.sh&quot;, args: [ K8SV, CONTAINERDV]
      subconfig.vm.provision &quot;shell&quot;, path: &quot;k8s-w.sh&quot;
    end
  end

end
</code></pre><p id="233b8b2c-1c23-80b5-a50f-ed738ac7dfc7" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80a7-989b-ca27ece1bb66" class="toggle"><li><details open=""><summary>init_cfg.sh : args 참고하여 설치</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80ea-adad-ee4dea686a53" class="code"><code class="language-Bash">#!/usr/bin/env bash

echo &quot;&gt;&gt;&gt;&gt; Initial Config Start &lt;&lt;&lt;&lt;&quot;

echo &quot;[TASK 1] Setting Profile &amp; Change Timezone&quot;
echo &#x27;alias vi=vim&#x27; &gt;&gt; /etc/profile
echo &quot;sudo su -&quot; &gt;&gt; /home/vagrant/.bashrc
ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime


echo &quot;[TASK 2] Disable AppArmor&quot;
systemctl stop ufw &amp;&amp; systemctl disable ufw &gt;/dev/null 2&gt;&amp;1
systemctl stop apparmor &amp;&amp; systemctl disable apparmor &gt;/dev/null 2&gt;&amp;1


echo &quot;[TASK 3] Disable and turn off SWAP&quot;
swapoff -a &amp;&amp; sed -i &#x27;/swap/s/^/#/&#x27; /etc/fstab


echo &quot;[TASK 4] Install Packages&quot;
apt update -qq &gt;/dev/null 2&gt;&amp;1
apt-get install apt-transport-https ca-certificates curl gpg -y -qq &gt;/dev/null 2&gt;&amp;1

# Download the public signing key for the Kubernetes package repositories.
mkdir -p -m 755 /etc/apt/keyrings
K8SMMV=$(echo $1 | sed -En &#x27;s/^([0-9]+\.[0-9]+)\..*/\1/p&#x27;)
curl -fsSL https://pkgs.k8s.io/core:/stable:/v$K8SMMV/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo &quot;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$K8SMMV/deb/ /&quot; &gt;&gt; /etc/apt/sources.list.d/kubernetes.list
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null

# packets traversing the bridge are processed by iptables for filtering
echo 1 &gt; /proc/sys/net/ipv4/ip_forward
echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.d/k8s.conf

# enable br_netfilter for iptables 
modprobe br_netfilter
modprobe overlay
echo &quot;br_netfilter&quot; &gt;&gt; /etc/modules-load.d/k8s.conf
echo &quot;overlay&quot; &gt;&gt; /etc/modules-load.d/k8s.conf


echo &quot;[TASK 5] Install Kubernetes components (kubeadm, kubelet and kubectl)&quot;
# Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version
apt update &gt;/dev/null 2&gt;&amp;1

# apt list -a kubelet ; apt list -a containerd.io
apt-get install -y kubelet=$1 kubectl=$1 kubeadm=$1 containerd.io=$2 &gt;/dev/null 2&gt;&amp;1
apt-mark hold kubelet kubeadm kubectl &gt;/dev/null 2&gt;&amp;1

# containerd configure to default and cgroup managed by systemd
containerd config default &gt; /etc/containerd/config.toml
sed -i &#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27; /etc/containerd/config.toml

# avoid WARN&amp;ERRO(default endpoints) when crictl run  
cat &lt;&lt;EOF &gt; /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
EOF

# ready to install for k8s 
systemctl restart containerd &amp;&amp; systemctl enable containerd
systemctl enable --now kubelet


echo &quot;[TASK 6] Install Packages &amp; Helm&quot;
apt-get install -y bridge-utils sshpass net-tools conntrack ngrep tcpdump ipset arping wireguard jq tree bash-completion unzip kubecolor &gt;/dev/null 2&gt;&amp;1
curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash &gt;/dev/null 2&gt;&amp;1


echo &quot;&gt;&gt;&gt;&gt; Initial Config End &lt;&lt;&lt;&lt;&quot;
</code></pre><p id="233b8b2c-1c23-8083-ae54-e2d661318b9b" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-808a-8144-dd5bb0700a00" class="toggle"><li><details open=""><summary>k8s-ctr.sh : kubeadm init (Pod/ServiceCIDR) , 편리성 설정(k, kc)</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-800f-bd6f-fe7a3a4f9a20" class="code"><code class="language-Bash">#!/usr/bin/env bash

echo &quot;&gt;&gt;&gt;&gt; K8S Controlplane config Start &lt;&lt;&lt;&lt;&quot;

echo &quot;[TASK 1] Initial Kubernetes&quot;
kubeadm init --token 123456.1234567890123456 --token-ttl 0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/16 --apiserver-advertise-address=192.168.10.100 --cri-socket=unix:///run/containerd/containerd.sock &gt;/dev/null 2&gt;&amp;1


echo &quot;[TASK 2] Setting kube config file&quot;
mkdir -p /root/.kube
cp -i /etc/kubernetes/admin.conf /root/.kube/config
chown $(id -u):$(id -g) /root/.kube/config


echo &quot;[TASK 3] Source the completion&quot;
echo &#x27;source &lt;(kubectl completion bash)&#x27; &gt;&gt; /etc/profile
echo &#x27;source &lt;(kubeadm completion bash)&#x27; &gt;&gt; /etc/profile


echo &quot;[TASK 4] Alias kubectl to k&quot;
echo &#x27;alias k=kubectl&#x27; &gt;&gt; /etc/profile
echo &#x27;alias kc=kubecolor&#x27; &gt;&gt; /etc/profile
echo &#x27;complete -F __start_kubectl k&#x27; &gt;&gt; /etc/profile


echo &quot;[TASK 5] Install Kubectx &amp; Kubens&quot;
git clone https://github.com/ahmetb/kubectx /opt/kubectx &gt;/dev/null 2&gt;&amp;1
ln -s /opt/kubectx/kubens /usr/local/bin/kubens
ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


echo &quot;[TASK 6] Install Kubeps &amp; Setting PS1&quot;
git clone https://github.com/jonmosco/kube-ps1.git /root/kube-ps1 &gt;/dev/null 2&gt;&amp;1
cat &lt;&lt;&quot;EOT&quot; &gt;&gt; /root/.bash_profile
source /root/kube-ps1/kube-ps1.sh
KUBE_PS1_SYMBOL_ENABLE=true
function get_cluster_short() {
  echo &quot;$1&quot; | cut -d . -f1
}
KUBE_PS1_CLUSTER_FUNCTION=get_cluster_short
KUBE_PS1_SUFFIX=&#x27;) &#x27;
PS1=&#x27;$(kube_ps1)&#x27;$PS1
EOT
kubectl config rename-context &quot;kubernetes-admin@kubernetes&quot; &quot;HomeLab&quot; &gt;/dev/null 2&gt;&amp;1


echo &quot;[TASK 6] Install Kubeps &amp; Setting PS1&quot;
echo &quot;192.168.10.100 k8s-ctr&quot; &gt;&gt; /etc/hosts
for (( i=1; i&lt;=$1; i++  )); do echo &quot;192.168.10.10$i k8s-w$i&quot; &gt;&gt; /etc/hosts; done


echo &quot;&gt;&gt;&gt;&gt; K8S Controlplane Config End &lt;&lt;&lt;&lt;&quot;</code></pre><p id="233b8b2c-1c23-807d-a583-f64cf0b84bb4" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8004-9da1-f034cb81679c" class="toggle"><li><details open=""><summary>k8s-w.sh : kubeadm join</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80bb-b18d-d168e72083ee" class="code"><code class="language-Bash">#!/usr/bin/env bash

echo &quot;&gt;&gt;&gt;&gt; K8S Node config Start &lt;&lt;&lt;&lt;&quot;

echo &quot;[TASK 1] K8S Controlplane Join&quot; 
kubeadm join --token 123456.1234567890123456 --discovery-token-unsafe-skip-ca-verification 192.168.10.100:6443  &gt;/dev/null 2&gt;&amp;1


echo &quot;&gt;&gt;&gt;&gt; K8S Node config End &lt;&lt;&lt;&lt;&quot;</code></pre><p id="233b8b2c-1c23-80f2-b7a2-e8e6e255c7ad" class="">
</p></details></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-801f-a59a-fb091be577b2" class="code"><code class="language-Bash">mkdir cilium-lab &amp;&amp; cd cilium-lab

curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/1w/Vagrantfile

vagrant up</code></pre><ul id="233b8b2c-1c23-80e5-9caa-e753722b407e" class="bulleted-list"><li style="list-style-type:circle">(참고)  </li></ul><p id="233b8b2c-1c23-800d-9adc-e0d20112e02b" class="">
</p></li></ul><ul id="233b8b2c-1c23-80bf-a812-d89cae9f5bd2" class="bulleted-list"><li style="list-style-type:disc"><code>도전과제</code> vagrant box 를 Rocky linux 로 실습 환경 배포 해보기 - <a href="https://portal.cloud.hashicorp.com/vagrant/discover/rockylinux">box_image</a> </li></ul><ul id="233b8b2c-1c23-806e-b085-cbbd91cbcea9" class="toggle"><li><details open=""><summary><strong>실습 환경 배포 &amp; IP 변경 작업</strong> : <code><strong>vagrant up</strong></code></summary><ul id="233b8b2c-1c23-8039-a9a7-dcf31f8dfb3a" class="toggle"><li><details open=""><summary><strong>Vagrant ssh 실행 시 동작</strong> : 가상머신의 eth0 IP는 10.0.2.15 로 모두 동일하며, 외부 인터넷 연결 역할을 함</summary><figure id="233b8b2c-1c23-8099-98b3-d57b4cfabcfa" class="image"><a href="images/Untitled.png"><img style="width:912px" src="images/Untitled.png"/></a></figure><ul id="233b8b2c-1c23-803e-b4a2-f64f7e95a963" class="bulleted-list"><li style="list-style-type:disc">vagrant ssh 접속 시 호스트에 127.0.0.1(2222)를 목적지로 접속 → 이후 포트포워딩(S/DNAT)을 통해서 내부에 VM로 SSH 연결됨</li></ul><ul id="233b8b2c-1c23-80ea-96b3-e1e1751ddf9b" class="bulleted-list"><li style="list-style-type:disc">NAT Mode 에 <strong>10.0.2.2</strong>(GateWay), <strong>10.0.2.3</strong>(DNS Server), <strong>10.0.2.4</strong>(TFTP Server) 용도로 IP가 예약됨</li></ul><figure id="233b8b2c-1c23-8009-8ac2-dad8f45a8e6a" class="image"><a href="images/Untitled%201.png"><img style="width:576px" src="images/Untitled%201.png"/></a></figure><figure id="233b8b2c-1c23-80da-bfe8-d3d7f0de3a8e" class="image"><a href="images/Untitled%202.png"><img style="width:576px" src="images/Untitled%202.png"/></a></figure><p id="233b8b2c-1c23-807c-ac17-e15219e9b8a2" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-808d-a618-ef34c4921ac1" class="bulleted-list"><li style="list-style-type:disc">배포 후 ssh 접속 : <code><strong>vagrant ssh k8s-ctr</strong></code> , vagrant ssh k8s-w1 , vagrant ssh k8s-w2<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8091-8c7b-f241a205fe43" class="code"><code class="language-Bash"># ssh 접속 전, 노드들의 eth0 IP 확인
for i in ctr w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; vagrant ssh k8s-$i -c &#x27;ip -c -4 addr show dev eth0&#x27;; echo; done #

&gt;&gt; node : k8s-ctr &lt;&lt;
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s8
    inet 10.0.2.15/24 metric 100 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86149sec preferred_lft 86149sec

&gt;&gt; node : k8s-w1 &lt;&lt;
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s8
    inet 10.0.2.15/24 metric 100 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86271sec preferred_lft 86271sec

&gt;&gt; node : k8s-w2 &lt;&lt;
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s8
    inet 10.0.2.15/24 metric 100 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86338sec preferred_lft 86338sec</code></pre><p id="233b8b2c-1c23-8017-8ccc-d885fd175caf" class="">
</p></li></ul><ul id="233b8b2c-1c23-8042-87e7-e698c0207225" class="bulleted-list"><li style="list-style-type:disc">[k8s-ctr] 접속 후 기본 정보 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8096-8430-f20607c4d915" class="code"><code class="language-Bash">#
whoami
pwd
hostnamectl
htop

#
cat /etc/hosts
ping -c 1 k8s-w1
ping -c 1 k8s-w2
sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-w1 hostname
sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-w2 hostname


# vagrant ssh 로 접속 시 tcp 연결 정보 : NAT Mode 10.0.2.2(GateWay)
ss -tnp |grep sshd
ESTAB 0      0           [::ffff:10.0.2.15]:22          [::ffff:10.0.2.2]:52791 users:((&quot;sshd&quot;,pid=5176,fd=4),(&quot;sshd&quot;,pid=5129,fd=4))

# nic 정보
ip -c addr

# default 라우팅 정보 
ip -c route

# dns 서버 정보 : NAT Mode 10.0.2.3
resolvectl
</code></pre><p id="233b8b2c-1c23-8076-93c0-ee9dbfb30262" class="">
</p></li></ul><ul id="233b8b2c-1c23-8080-9d8b-dd225b19cbef" class="bulleted-list"><li style="list-style-type:disc">[k8s-ctr] k8s 정보 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8037-aecb-cd278ca3bb85" class="code"><code class="language-Bash"># 
kubectl cluster-info

# 노드 정보 : 상태, INTERNAL-IP 확인
# w1, w2는 ip가 각자의 eth0 ip로 설정되어있음, k8s-ctr만 eth1로 설정되어있고 모두 eth1로 설정해야 서로 통신이 가능
kubectl get node -owide
NAME      STATUS     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   NotReady   control-plane   9m15s   v1.33.2   192.168.10.100   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w1    NotReady   &lt;none&gt;          8m2s    v1.33.2   10.0.2.15        &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w2    NotReady   &lt;none&gt;          6m54s   v1.33.2   10.0.2.15        &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27

# 파드 정보 : 상태, 파드 IP 확인 - kube-proxy 확인
# CNI가 없어서 아직 coredns가 pod ip대역을 못받아와서 pending
kubectl get pod -A -owide
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-674b8bbfcf-tz2z2          0/1     Pending   0          9m19s   &lt;none&gt;      &lt;none&gt;    &lt;none&gt;           &lt;none&gt;
kube-system   coredns-674b8bbfcf-wzck9          0/1     Pending   0          9m19s   &lt;none&gt;      &lt;none&gt;    &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s-ctr                      1/1     Running   0          9m26s   10.0.2.15   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s-ctr            1/1     Running   0          9m26s   10.0.2.15   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s-ctr   1/1     Running   0          9m26s   10.0.2.15   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-698zt                  1/1     Running   0          7m7s    10.0.2.15   k8s-w2    &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-6gr42                  1/1     Running   0          8m15s   10.0.2.15   k8s-w1    &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-t5cm2                  1/1     Running   0          9m20s   10.0.2.15   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s-ctr            1/1     Running   0          9m26s   10.0.2.15   k8s-ctr   &lt;none&gt;           &lt;none&gt;

# 단축어 확인(kc = kubecolor) &amp; coredns 파드 상태 확인
k  describe pod -n kube-system -l k8s-app=kube-dns
kc describe pod -n kube-system -l k8s-app=kube-dns
</code></pre><p id="233b8b2c-1c23-8084-83b9-f7abce8d9e86" class="">
</p></li></ul><ul id="233b8b2c-1c23-800a-8814-d3892a2008cf" class="bulleted-list"><li style="list-style-type:disc">[k8s-ctr] INTERNAL-IP 변경 설정<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8027-b67a-e5a9ba5cfad6" class="code"><code class="language-Bash">#
cat /var/lib/kubelet/kubeadm-flags.env

# INTERNAL-IP 변경 설정
NODEIP=$(ip -4 addr show eth1 | grep -oP &#x27;(?&lt;=inet\s)\d+(\.\d+){3}&#x27;)
sed -i &quot;s/^\(KUBELET_KUBEADM_ARGS=\&quot;\)/\1--node-ip=${NODEIP} /&quot; /var/lib/kubelet/kubeadm-flags.env
systemctl daemon-reexec &amp;&amp; systemctl restart kubelet

cat /var/lib/kubelet/kubeadm-flags.env

#
kubectl get node -owide
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   NotReady   control-plane   24m   v1.33.2   192.168.10.100   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
...</code></pre><ul id="233b8b2c-1c23-80ef-9ba6-d4382ba97494" class="bulleted-list"><li style="list-style-type:circle">[k8s-w1/w2] INTERNAL-IP 변경 설정 할 것 → 설정 완료 후 아래 처럼 확인</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80ac-b8d8-df72c469d13c" class="code"><code class="language-Bash"># k8s-w1 설정
vagrant ssh k8s-w1
NODEIP=$(ip -4 addr show eth1 | grep -oP &#x27;(?&lt;=inet\s)\d+(\.\d+){3}&#x27;)
sed -i &quot;s/^\(KUBELET_KUBEADM_ARGS=\&quot;\)/\1--node-ip=${NODEIP} /&quot; /var/lib/kubelet/kubeadm-flags.env
systemctl daemon-reexec &amp;&amp; systemctl restart kubelet

# k8s-w2 설정
vagrant ssh k8s-w2
NODEIP=$(ip -4 addr show eth1 | grep -oP &#x27;(?&lt;=inet\s)\d+(\.\d+){3}&#x27;)
sed -i &quot;s/^\(KUBELET_KUBEADM_ARGS=\&quot;\)/\1--node-ip=${NODEIP} /&quot; /var/lib/kubelet/kubeadm-flags.env
systemctl daemon-reexec &amp;&amp; systemctl restart kubelet

kubectl get node -owide
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   NotReady   control-plane   26m   v1.33.2   192.168.10.100   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w1    NotReady   &lt;none&gt;          24m   v1.33.2   192.168.10.101   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w2    NotReady   &lt;none&gt;          23m   v1.33.2   192.168.10.102   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27

kubectl get pod -A -owide
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-674b8bbfcf-fl4tr          0/1     Pending   0          26m   &lt;none&gt;           &lt;none&gt;    &lt;none&gt;           &lt;none&gt;
kube-system   coredns-674b8bbfcf-npvs9          0/1     Pending   0          26m   &lt;none&gt;           &lt;none&gt;    &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s-ctr                      1/1     Running   0          26m   10.0.2.15        k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s-ctr            1/1     Running   0          26m   10.0.2.15        k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s-ctr   1/1     Running   0          26m   10.0.2.15        k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-bwwjq                  1/1     Running   0          23m   192.168.10.102   k8s-w2    &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-hqqq7                  1/1     Running   0          26m   192.168.10.100   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-l9t87                  1/1     Running   0          24m   192.168.10.101   k8s-w1    &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s-ctr            1/1     Running   0          26m   10.0.2.15        k8s-ctr   &lt;none&gt;           &lt;none&gt;</code></pre><p id="233b8b2c-1c23-8081-95ed-cae6ad15609b" class="">
</p></li></ul><ul id="233b8b2c-1c23-804f-be7d-de022326fb1e" class="bulleted-list"><li style="list-style-type:disc">[k8s-ctr] static pod 의 IP 변경 설정<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8006-a712-f3e3009e5b45" class="code"><code class="language-Bash">#
tree /etc/kubernetes/manifests
/etc/kubernetes/manifests
├── etcd.yaml
├── kube-apiserver.yaml
├── kube-controller-manager.yaml
└── kube-scheduler.yaml

# etcd 정보 확인
cat /etc/kubernetes/manifests/etcd.yaml
...
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data

tree /var/lib/etcd/
/var/lib/etcd/
└── member
    ├── snap
    │   └── db
    └── wal
        ├── 0000000000000000-0000000000000000.wal
        └── 0.tmp

# k8s-ctr 재부팅
reboot
</code></pre><ul id="233b8b2c-1c23-80cd-a768-c9285504f5fe" class="bulleted-list"><li style="list-style-type:circle"><code><strong>vagrant ssh k8s-ctr</strong></code> 재접속 후 확인</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-806c-b130-e0da6d3e7918" class="code"><code class="language-Bash">#
kubectl get pod -n kube-system -owide
NAME                              READY   STATUS    RESTARTS      AGE     IP               NODE      NOMINATED NODE   READINESS GATES
etcd-k8s-ctr                      1/1     Running   1 (80s ago)   2m27s   192.168.10.100   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-apiserver-k8s-ctr            1/1     Running   1 (80s ago)   2m27s   192.168.10.100   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-controller-manager-k8s-ctr   1/1     Running   1 (80s ago)   2m27s   192.168.10.100   k8s-ctr   &lt;none&gt;           &lt;none&gt;
kube-scheduler-k8s-ctr            1/1     Running   1 (80s ago)   2m27s   192.168.10.100   k8s-ctr   &lt;none&gt;           &lt;none&gt;</code></pre><p id="233b8b2c-1c23-80d4-af82-de92591995fd" class="">
</p></li></ul><p id="233b8b2c-1c23-80f0-b4c8-d28e426ed3b4" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80f2-afca-f4dc0a42d5a4" class="bulleted-list"><li style="list-style-type:disc"><code>도전과제</code> kubeadm Configuration 를 사용해서 node-ip 를 지정해서 init/join 해보자 - <a href="https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/">Docs</a><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80d8-bc46-f3bfeeeacf97" class="code"><code class="language-Bash"># 예시
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  kubeletExtraArgs:
    node-ip: &quot;192.168.10.101&quot;</code></pre><p id="233b8b2c-1c23-80cb-ba37-db342dde7de0" class="">
</p></li></ul><ul id="233b8b2c-1c23-801a-a0f3-dfaad5b3999f" class="toggle"><li><details open=""><summary>(참고) 쿠버네티스 네트워크 및 Flannel CNI 소개 - <a href="https://github.com/flannel-io/flannel">링크</a></summary><figure id="233b8b2c-1c23-80e5-8108-cdab2f0c29f9" class="image"><a href="images/CleanShot_2024-09-01_at_16.34.18.png"><img style="width:1178.984375px" src="images/CleanShot_2024-09-01_at_16.34.18.png"/></a></figure><figure id="233b8b2c-1c23-8006-b3f4-f0f9cdd58075" class="image"><a href="images/CleanShot_2024-09-01_at_16.34.29.png"><img style="width:1178.984375px" src="images/CleanShot_2024-09-01_at_16.34.29.png"/></a></figure><p id="233b8b2c-1c23-80a9-b2f1-d866d45f9fd1" class="">
</p><figure id="233b8b2c-1c23-80cf-b9db-e781eafdd0f4" class="image"><a href="images/CleanShot_2024-09-01_at_16.35.02.png"><img style="width:1178.984375px" src="images/CleanShot_2024-09-01_at_16.35.02.png"/></a></figure><figure id="233b8b2c-1c23-8055-a77f-eddb9b8989eb" class="image"><a href="images/CleanShot_2024-09-01_at_16.35.19.png"><img style="width:1178.984375px" src="images/CleanShot_2024-09-01_at_16.35.19.png"/></a></figure><figure id="233b8b2c-1c23-80a9-9a8b-d98fab0abdcf" class="image"><a href="images/Untitled%203.png"><img style="width:327.9403381347656px" src="images/Untitled%203.png"/></a></figure><ul id="233b8b2c-1c23-8078-8f84-d4837ee7a429" class="bulleted-list"><li style="list-style-type:disc">Flannel runs a small, single binary agent called <code>flanneld</code> on each host, and is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. → 모든 노드에 <code>flanneld</code> 가 동작</li></ul><ul id="233b8b2c-1c23-80a1-bb2b-cf39344b7619" class="bulleted-list"><li style="list-style-type:disc">네트워킹 환경 지원 (Backends) : <mark class="highlight-orange"><strong>VXLAN</strong></mark>, host-gw, UDP, 그외에는 아직 실험적임 - <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md">링크</a> <a href="https://msazure.club/flannel-networking-demystify/">링크2</a><ul id="233b8b2c-1c23-8019-ac37-ec5fe96d6305" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-orange"><strong>VXLAN</strong></mark> (<strong>권장</strong>) : Port(<strong>UDP 8472</strong>), DirecRouting 지원(같은 서브넷 노드와는 host-gw 처럼 동작)<ul id="233b8b2c-1c23-8037-b06e-e12b1e7d5cc2" class="bulleted-list"><li style="list-style-type:square">단, 네트워크 엔지니어분들이 알고 있는 ‘L2 확장’ 이 아니라, 각 노드마다 별도의 서브넷이 있고, 해당 서브넷 대역끼리 NAT 없이 라우팅 처리됨</li></ul></li></ul><ul id="233b8b2c-1c23-803e-bf6d-fda2d35b0a0b" class="bulleted-list"><li style="list-style-type:circle">host-gw : 호스트 L2 모드?, 일반적으로 퍼블릭 클라우드 환경에서는 동작하지 않는다</li></ul><ul id="233b8b2c-1c23-807b-a92e-ff812551e07b" class="bulleted-list"><li style="list-style-type:circle">UDP (비권장) : VXLAN 지원하지 않는 오래된 커널 사용 시, Port(UDP 8285)</li></ul></li></ul><ul id="233b8b2c-1c23-8088-8884-f0120ea28ccb" class="bulleted-list"><li style="list-style-type:disc">노드마다 <mark class="highlight-orange">flannel.1</mark> 생성 : <mark class="highlight-orange">VXLAN VTEP</mark> 역할 , 뒷에 숫자는 VXLAN id 1 에 1을 의미</li></ul><ul id="233b8b2c-1c23-80b5-abc1-feffb3b3a462" class="bulleted-list"><li style="list-style-type:disc">노드마다 <mark class="highlight-orange">cni0</mark> 생성 : <mark class="highlight-orange">bridge</mark> 역할</li></ul><figure id="233b8b2c-1c23-804d-8f38-d78c6384be73" class="image"><a href="images/Untitled%204.png"><img style="width:768px" src="images/Untitled%204.png"/></a><figcaption><a href="https://ikcoo.tistory.com/101">https://ikcoo.tistory.com/101</a></figcaption></figure><figure id="233b8b2c-1c23-80e2-870f-cc86c60da1df" class="image"><a href="images/Untitled%205.png"><img style="width:1008px" src="images/Untitled%205.png"/></a><figcaption><a href="https://ikcoo.tistory.com/101">https://ikcoo.tistory.com/101</a></figcaption></figure><figure id="233b8b2c-1c23-80c4-861a-e0e0839cfbd2" class="image"><a href="images/Untitled%206.png"><img style="width:624px" src="images/Untitled%206.png"/></a><figcaption><a href="https://docs.openshift.com/container-platform/3.4/architecture/additional_concepts/flannel.html">https://docs.openshift.com/container-platform/3.4/architecture/additional_concepts/flannel.html</a></figcaption></figure><p id="233b8b2c-1c23-80e1-b8cb-e75e463bf0ca" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80b3-890e-de9ca20d1b02" class="toggle"><li><details open=""><summary><mark class="highlight-blue"><strong>Flannel CNI</strong></mark><strong> 설치 및 확인</strong> - <a href="https://github.com/flannel-io/flannel">Github</a></summary><ul id="233b8b2c-1c23-80ae-be72-f34790475967" class="bulleted-list"><li style="list-style-type:disc">설치 전 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80d6-a6f4-cf621e66b34d" class="code"><code class="language-Bash">#
kubectl cluster-info dump | grep -m 2 -E &quot;cluster-cidr|service-cluster-ip-range&quot;
                            &quot;--service-cluster-ip-range=10.96.0.0/16&quot;,
                            &quot;--cluster-cidr=10.244.0.0/16&quot;,

#
kubectl get pod -n kube-system -l k8s-app=kube-dns -owide
NAME                       READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
coredns-674b8bbfcf-fl4tr   0/1     Pending   0          39m   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
coredns-674b8bbfcf-npvs9   0/1     Pending   0          39m   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;

#
ip -c link
ip -c route
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:71:19:d8 brd ff:ff:ff:ff:ff:ff
    altname enp0s8
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:25:6c:ff brd ff:ff:ff:ff:ff:ff
    altname enp0s9
default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.100

brctl show

#
iptables-save
iptables -t nat -S
iptables -t filter -S
iptables -t mangle -S

#
tree /etc/cni/net.d/
</code></pre><p id="233b8b2c-1c23-80bb-b954-eeda2a65ad90" class="">
</p></li></ul><ul id="233b8b2c-1c23-80f3-a90f-c72ceee8f5bf" class="bulleted-list"><li style="list-style-type:disc">Flannel CNI 설치 - <a href="https://github.com/flannel-io/flannel">Github</a><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8063-bfed-f398cca45770" class="code"><code class="language-Bash"># Needs manual creation of namespace to avoid helm error
kubectl create ns kube-flannel
kubectl label --overwrite ns kube-flannel pod-security.kubernetes.io/enforce=privileged

helm repo add flannel https://flannel-io.github.io/flannel/
helm repo list
helm search repo flannel
helm show values flannel/flannel

# k8s 관련 트래픽 통신 동작하는 nic 지정
cat &lt;&lt; EOF &gt; flannel-values.yaml
podCidr: &quot;10.244.0.0/16&quot;

flannel:
  args:
  - &quot;--ip-masq&quot;
  - &quot;--kube-subnet-mgr&quot;
  - &quot;--iface=eth1&quot;  
EOF

# helm 설치
helm install flannel --namespace kube-flannel flannel/flannel -f flannel-values.yaml

helm list -A
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
flannel kube-flannel    1               2025-07-18 01:03:39.209614003 +0900 KST deployed        flannel-v0.27.1 v0.27.1   

# 확인 : install-cni-plugin, install-cni
kc describe pod -n kube-flannel -l app=flannel

tree /opt/cni/bin/ # flannel
tree /etc/cni/net.d/
cat /etc/cni/net.d/10-flannel.conflist | jq
kc describe cm -n kube-flannel kube-flannel-cfg
...
net-conf.json:
----
{
  &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
  &quot;EnableNFTables&quot;: false,
  &quot;Backend&quot;: {
    &quot;Type&quot;: &quot;vxlan&quot;
  }
}

# 설치 전과 비교해보자
# flannel.1이라는 interface가 하나 추가된것을 볼 수 있음
ip -c link
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:71:19:d8 brd ff:ff:ff:ff:ff:ff
    altname enp0s8
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:25:6c:ff brd ff:ff:ff:ff:ff:ff
    altname enp0s9
4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/ether 76:a6:1c:78:b3:a9 brd ff:ff:ff:ff:ff:ff

# flannel cni에 의해 pod ip 대역이 route에 추가됨
ip -c route | grep 10.244.
10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 
10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink 
10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink 

ping -c 1 10.244.1.0
ping -c 1 10.244.2.0

brctl show
iptables-save

# Flannel 관련 iptalbes rule이 추가됨
iptables -t nat -S
-A FLANNEL-POSTRTG -m mark --mark 0x4000/0x4000 -m comment --comment &quot;flanneld masq&quot; -j RETURN
-A FLANNEL-POSTRTG -s 10.244.0.0/24 -d 10.244.0.0/16 -m comment --comment &quot;flanneld masq&quot; -j RETURN
-A FLANNEL-POSTRTG -s 10.244.0.0/16 -d 10.244.0.0/24 -m comment --comment &quot;flanneld masq&quot; -j RETURN
-A FLANNEL-POSTRTG ! -s 10.244.0.0/16 -d 10.244.0.0/24 -m comment --comment &quot;flanneld masq&quot; -j RETURN
-A FLANNEL-POSTRTG -s 10.244.0.0/16 ! -d 224.0.0.0/4 -m comment --comment &quot;flanneld masq&quot; -j MASQUERADE --random-fully
-A FLANNEL-POSTRTG ! -s 10.244.0.0/16 -d 10.244.0.0/16 -m comment --comment &quot;flanneld masq&quot; -j MASQUERADE --random-fully
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE --random-fully
-A KUBE-SEP-BWHGELGX6BITPZVO -s 10.244.1.2/32 -m comment --comment &quot;kube-system/kube-dns:dns-tcp&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-BWHGELGX6BITPZVO -p tcp -m comment --comment &quot;kube-system/kube-dns:dns-tcp&quot; -m tcp -j DNAT --to-destination 10.244.1.2:53
-A KUBE-SEP-EJJ3L23ZA35VLW6X -s 10.244.1.3/32 -m comment --comment &quot;kube-system/kube-dns:dns&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-EJJ3L23ZA35VLW6X -p udp -m comment --comment &quot;kube-system/kube-dns:dns&quot; -m udp -j DNAT --to-destination 10.244.1.3:53

iptables -t filter -S

# k8s-w1, k8s-w2 정보 확인
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c link ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c route ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i brctl show ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo iptables -t nat -S ; echo; done
</code></pre></li></ul><p id="233b8b2c-1c23-80a5-be9c-efbfdb80d18a" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8063-894c-e26ac30679b2" class="toggle"><li><details open=""><summary><strong>샘플 애플리케이션 배포 및 확인</strong></summary><ul id="233b8b2c-1c23-8043-aadd-fa2f2e09ec2e" class="bulleted-list"><li style="list-style-type:disc">샘플 애플리케이션 배포<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-806c-afdd-dfa023d1214e" class="code"><code class="language-Bash"># 샘플 애플리케이션 배포
cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: &quot;kubernetes.io/hostname&quot;
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF


# k8s-ctr 노드에 curl-pod 파드 배포
cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
    - name: curl
      image: alpine/curl
      command: [&quot;sleep&quot;, &quot;36000&quot;]
EOF

#
crictl ps
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i sudo crictl ps ; echo; done
</code></pre><p id="233b8b2c-1c23-8001-9335-c301be7da893" class="">
</p></li></ul><ul id="233b8b2c-1c23-80b2-a8c8-dd869f03c3ca" class="bulleted-list"><li style="list-style-type:disc">확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-8059-b385-f3938fb96d5b" class="code"><code class="language-Bash"># 배포 확인
kubectl get deploy,svc,ep webpod -owide
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES           SELECTOR
deployment.apps/webpod   2/2     2            2           73s   webpod       traefik/whoami   app=webpod

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/webpod   ClusterIP   10.96.227.33   &lt;none&gt;        80/TCP    73s   app=webpod

NAME               ENDPOINTS                     AGE
endpoints/webpod   10.244.1.4:80,10.244.2.2:80   73s

#
kubectl api-resources | grep -i endpoint
endpoints                           ep           v1                                true         Endpoints
endpointslices                                   discovery.k8s.io/v1               true         EndpointSlice

kubectl get endpointslices -l app=webpod
NAME           ADDRESSTYPE   PORTS   ENDPOINTS               AGE
webpod-bvkfg   IPv4          80      10.244.2.2,10.244.1.4   92s

kubectl get po -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP           NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          2m14s   10.244.0.2   k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-697b545f57-gkklh   1/1     Running   0          2m21s   10.244.1.4   k8s-w1    &lt;none&gt;           &lt;none&gt;
webpod-697b545f57-gp46f   1/1     Running   0          2m21s   10.244.2.2   k8s-w2    &lt;none&gt;           &lt;none&gt;

# 배포 전과 비교해보자
# kube-ctr node에 curl-pod가 배포되었고, 해당 pod에 대한 veth가 생성됨
ip -c link
6: vethfe964734@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default qlen 1000
    link/ether c6:1e:99:59:36:be brd ff:ff:ff:ff:ff:ff link-netns cni-3fc1bf56-16ac-b0f6-a986-9cb1ba5a312a

# cni0가 bridge역할    
brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.be828f58c3e2       no              vethfe964734

iptables-save
iptables -t nat -S

# k8s-w1, k8s-w2 정보 확인
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c link ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c route ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i brctl show ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo iptables -t nat -S ; echo; done
</code></pre><p id="233b8b2c-1c23-80ed-bf1c-ded04f0210ed" class="">
</p></li></ul><ul id="233b8b2c-1c23-80c4-bbff-e97404ee7f6b" class="bulleted-list"><li style="list-style-type:disc">통신 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-804f-8b47-ef9f4c6ebfe0" class="code"><code class="language-Bash">#
kubectl get pod -l app=webpod -owide
webpod-697b545f57-gkklh   1/1     Running   0          4m2s   10.244.1.4   k8s-w1   &lt;none&gt;           &lt;none&gt;
webpod-697b545f57-gp46f   1/1     Running   0          4m2s   10.244.2.2   k8s-w2   &lt;none&gt;           &lt;none&gt;

POD1IP=10.244.1.4

kubectl exec -it curl-pod -- curl $POD1IP
Hostname: webpod-697b545f57-gkklh
IP: 127.0.0.1
IP: ::1
IP: 10.244.1.4
IP: fe80::f011:69ff:fea5:e930
RemoteAddr: 10.244.0.2:44964
GET / HTTP/1.1
Host: 10.244.1.4
User-Agent: curl/8.14.1
Accept: */*

#
kubectl get svc,ep webpod
kubectl exec -it curl-pod -- curl webpod
kubectl exec -it curl-pod -- curl webpod | grep Hostname
kubectl exec -it curl-pod -- sh -c &#x27;while true; do curl -s webpod | grep Hostname; sleep 1; done&#x27;

# Service 동작 처리에 iptables 규칙 활용 확인 &gt;&gt; Service 가 100개 , 1000개 , 10000개 증가 되면???
# 결국 iptables rule에 100개, 1000개, 10000개가 등록되고 위에서부터 차례로 요청에 맞는 iptables를 찾게된다
kubectl get svc webpod -o jsonpath=&quot;{.spec.clusterIP}&quot;
SVCIP=$(kubectl get svc webpod -o jsonpath=&quot;{.spec.clusterIP}&quot;)
iptables -t nat -S | grep $SVCIP
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo iptables -t nat -S | grep $SVCIP ; echo; done
-A KUBE-SERVICES -d 10.96.255.104/32 -p tcp -m comment --comment &quot;default/webpod cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-CNZCPOCNCNOROALA
-A KUBE-SVC-CNZCPOCNCNOROALA ! -s 10.244.0.0/16 -d 10.96.255.104/32 -p tcp -m comment --comment &quot;default/webpod cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ
</code></pre><ul id="233b8b2c-1c23-8023-bd28-dc9d1c15172d" class="bulleted-list"><li style="list-style-type:circle">대규모 환경에서 IPTable 단점</li></ul><figure id="233b8b2c-1c23-80ca-a32b-eecc417fc97f" class="image"><a href="images/CleanShot_2025-06-15_at_16.00.56.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.00.56.png"/></a></figure></li></ul></details></li></ul></details></li></ul><p id="233b8b2c-1c23-802d-bc0e-f628bd6c1a2d" class="">
</p><h3 id="233b8b2c-1c23-801b-a06f-f0898e555419" class="">2. Cilium CNI 소개</h3><p id="233b8b2c-1c23-80d5-8b69-cba2d612601c" class=""> </p><ul id="233b8b2c-1c23-800a-b67d-f8ada4889857" class="bulleted-list"><li style="list-style-type:disc"><strong>Cilium</strong>은 e<strong>BPF </strong><mark class="highlight-gray">(Berkeley Packet Filter)</mark>를 기반으로 <strong>Pod Network 환경 + 보안 </strong>을 제공하는 <strong>CNI Plugin</strong> 입니다<figure id="233b8b2c-1c23-80e5-a36d-f7c7cdccbfa8" class="image"><a href="images/image%201.png"><img style="width:864px" src="images/image%201.png"/></a><figcaption><a href="https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/">https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/</a></figcaption></figure><p id="233b8b2c-1c23-80e5-a600-e1d01fc318fe" class="">
</p></li></ul><ul id="233b8b2c-1c23-8020-ad1b-cd66e864ec13" class="toggle"><li><details open=""><summary>Cilium <strong>eBPF</strong> 는 추가적인 App 이나 설정 변경 없이 <strong>리눅스 커널</strong>을 자유롭게 <strong>프로그래밍</strong>하여 <strong>동작</strong> 가능 - <a href="https://youtu.be/qsnR-s4XuGo?t=1059">링크</a></summary><ul id="233b8b2c-1c23-80f4-baac-f08a4cd4b563" class="bulleted-list"><li style="list-style-type:disc"><strong>기존 Linux Network Stack</strong> : 리눅스 네트워크 스택의 단점은 복잡하고, <strong>변경에 시간이 걸리고</strong>, 레이어를 건너뛰기 어렵다.</li></ul><ul id="233b8b2c-1c23-8050-b74d-dbeecc35267e" class="toggle"><li><details open=""><summary>IPtables 단점</summary><figure id="233b8b2c-1c23-802e-840d-d3adeeae9078" class="image"><a href="images/Untitled%207.png"><img style="width:912px" src="images/Untitled%207.png"/></a></figure><figure id="233b8b2c-1c23-80e5-a6b4-c1a1bcd3738b" class="image"><a href="images/Untitled%208.png"><img style="width:960px" src="images/Untitled%208.png"/></a></figure><figure id="233b8b2c-1c23-8029-af9c-cb9b90f208e1" class="image"><a href="images/Untitled%209.png"><img style="width:720px" src="images/Untitled%209.png"/></a></figure><figure id="233b8b2c-1c23-8053-a792-e3593430ad7e" class="image"><a href="images/Untitled%2010.png"><img style="width:492.9921875px" src="images/Untitled%2010.png"/></a></figure><p id="233b8b2c-1c23-8008-ad77-f6451fcd2800" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80b2-b534-ed7a08e79581" class="bulleted-list"><li style="list-style-type:disc"><strong>IPTables/Netfilter 방식</strong>과 <strong>eBPF 방식</strong> 비교 - <a href="https://blog.naver.com/kangdorr/222593265958">Blog</a></li></ul><ul id="233b8b2c-1c23-80ba-9191-e9d83f15245d" class="bulleted-list"><li style="list-style-type:disc">Kernel Layer에서 동작하는 Bytecode를 안전하게 Kernel에 Loading(injection) 할 수 있다</li></ul><figure id="233b8b2c-1c23-8008-ac46-d7da0a81ed23" class="image"><a href="images/Untitled%2011.png"><img style="width:960px" src="images/Untitled%2011.png"/></a><figcaption><a href="https://cilium.io/blog/2020/11/10/ebpf-future-of-networking/">https://cilium.io/blog/2020/11/10/ebpf-future-of-networking/</a></figcaption></figure><ul id="233b8b2c-1c23-8061-9e04-d4b01d22cfb8" class="bulleted-list"><li style="list-style-type:disc"><strong>BPF </strong><mark class="highlight-gray">(Berkeley Packet Filter)</mark> <strong>kernel hooks</strong> : BPF 는 <strong>커널</strong>에 삽입하여 <strong>패킷</strong>을 <strong>통제(필터링)</strong> 할 수 있으며, 다양한 영역에서 <strong>Hook</strong> 을 통해서 접근 할 수 있습니다.<figure id="233b8b2c-1c23-80f2-9967-e2218357b0b9" class="image"><a href="images/Untitled%2012.png"><img style="width:672px" src="images/Untitled%2012.png"/></a></figure><figure id="233b8b2c-1c23-8068-b4f5-ed42f22b69f6" class="image"><a href="images/image%202.png"><img style="width:1200px" src="images/image%202.png"/></a><figcaption><a href="https://cilium.io/blog/2021/05/11/cni-benchmark/">https://cilium.io/blog/2021/05/11/cni-benchmark/</a></figcaption></figure><p id="233b8b2c-1c23-808f-9091-df03499b5ee5" class="">
</p></li></ul><ul id="233b8b2c-1c23-80ea-8faf-e86bec9a8250" class="bulleted-list"><li style="list-style-type:disc"><strong>eBPF</strong> is a revolutionary technology that can run <mark class="highlight-red"><strong>sandboxed</strong></mark> <strong>programs</strong> in the<strong> </strong><mark class="highlight-blue"><strong>Linux kernel without changing kernel source code or loading kernel modules.</strong></mark><p id="233b8b2c-1c23-8079-8d13-c7107d02c0c1" class="">→ 커널 내에 (<strong>샌드박스</strong> 내에서) 자유롭게 <strong>프로그래밍</strong>하여 <strong>적용</strong> 할 수 있다</p><figure id="233b8b2c-1c23-8092-94f6-e0a4b88f1667" class="image"><a href="images/Untitled%2013.png"><img style="width:1008px" src="images/Untitled%2013.png"/></a><figcaption><a href="https://ebpf.io/">https://ebpf.io/</a></figcaption></figure></li></ul></details></li></ul><ul id="233b8b2c-1c23-809e-997a-f876d663d624" class="toggle"><li><details open=""><summary><strong>eBPF : </strong>extended Berkeley Packet Filter - <a href="https://ebpf.io/">링크</a> , <strong>zerotay*</strong> - <a href="https://zerotay-blog.vercel.app/4.RESOURCE/KNOWLEDGE/OS/eBPF/">Blog</a></summary><ul id="233b8b2c-1c23-80f8-94da-c624353c63a1" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red"><strong>Dynamically</strong></mark><mark class="highlight-red"> </mark><mark class="highlight-red"><strong>program</strong></mark> the <strong>kernel</strong> for efficient networking, observability, tracing, and security</li></ul><figure id="233b8b2c-1c23-802d-84cc-ff660627d83c" class="image"><a href="images/image%203.png"><img style="width:1152px" src="images/image%203.png"/></a><figcaption><a href="https://ebpf.io/">https://ebpf.io/</a></figcaption></figure><ul id="233b8b2c-1c23-80b8-a4f2-fe0db09e8294" class="bulleted-list"><li style="list-style-type:disc"><strong>BPF</strong>(1992년) 를 <strong>확장</strong>해서 <strong>eBPF</strong>가 (2014년,  Alexei Starovoitov) 가 나왔고, <strong>eBPF</strong> 를 <strong>다양한 영역 </strong><mark class="highlight-gray">(보안, 추적, 네트워킹, 모니터링)</mark>에서 활용하기 시작하였습니다.<figure id="233b8b2c-1c23-805d-be10-ffb727901b2b" class="image"><a href="images/Untitled%2014.png"><img style="width:912px" src="images/Untitled%2014.png"/></a></figure></li></ul><ul id="233b8b2c-1c23-8002-8945-c6c07daf95f0" class="bulleted-list"><li style="list-style-type:disc"><strong>eBPF</strong> is a revolutionary technology that can run <mark class="highlight-red"><strong>sandboxed</strong></mark> <strong>programs</strong> in the<strong> </strong><mark class="highlight-blue"><strong>Linux kernel without changing kernel source code or loading kernel modules.</strong></mark><p id="233b8b2c-1c23-80bb-b6c9-fd4a675d599f" class="">→ 커널 내에 (<strong>샌드박스</strong> 내에서) 자유롭게 <strong>프로그래밍</strong>하여 <strong>적용</strong> 할 수 있다</p><figure id="233b8b2c-1c23-8073-b88c-dcdee6ec6a95" class="image"><a href="images/Untitled%2013.png"><img style="width:1008px" src="images/Untitled%2013.png"/></a><figcaption><a href="https://ebpf.io/">https://ebpf.io/</a></figcaption></figure><figure id="233b8b2c-1c23-800b-b8f9-e33aa8ad665a" class="image"><a href="images/CleanShot_2024-10-20_at_14.13.27.png"><img style="width:1056px" src="images/CleanShot_2024-10-20_at_14.13.27.png"/></a></figure></li></ul><p id="233b8b2c-1c23-80dc-9f48-e281a377b4f0" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8040-ae58-e69df4c9cffa" class="toggle"><li><details open=""><summary><strong>XDP</strong>(<mark class="highlight-gray">e</mark><mark class="highlight-gray"><strong>X</strong></mark><mark class="highlight-gray">press </mark><mark class="highlight-gray"><strong>D</strong></mark><mark class="highlight-gray">ata </mark><mark class="highlight-gray"><strong>P</strong></mark><mark class="highlight-gray">ath</mark>) : <strong>eBPF</strong> based fast data-path <em><mark class="highlight-gray">← 출처 : SOSCON 2019 ‘Faster Packet Processing in Linux: XDP 이호연님</mark></em></summary><ul id="233b8b2c-1c23-80e0-a7c3-fde0cf79f988" class="bulleted-list"><li style="list-style-type:disc">Packet Path in Kernel<figure id="233b8b2c-1c23-8099-aecf-c4f008b22b45" class="image"><a href="images/Untitled%2015.png"><img style="width:1296px" src="images/Untitled%2015.png"/></a></figure></li></ul><p id="233b8b2c-1c23-80a0-8b19-db6bbcc06b88" class="">
</p><ul id="233b8b2c-1c23-80bb-a58e-d698f45e3fe6" class="bulleted-list"><li style="list-style-type:disc">패킷 차단 - 테스트 환경 : pktgen - send UDP packet<figure id="233b8b2c-1c23-80ea-86ef-e14688f78ffa" class="image"><a href="images/Untitled%2016.png"><img style="width:441.9791564941406px" src="images/Untitled%2016.png"/></a></figure><p id="233b8b2c-1c23-80f2-aca2-f97d9b79a671" class="">
</p></li></ul><ul id="233b8b2c-1c23-80ac-82bf-fcd217765bad" class="bulleted-list"><li style="list-style-type:disc">패킷 차단 - 비교<figure id="233b8b2c-1c23-80e4-8394-d435d31ee35c" class="image"><a href="images/Untitled%2017.png"><img style="width:960px" src="images/Untitled%2017.png"/></a></figure></li></ul><p id="233b8b2c-1c23-80b8-8fe5-c59bb6d8c860" class="">
</p><p id="233b8b2c-1c23-80bf-b2f2-c814e22d960b" class=""><strong>(1) Userspace 패킷 차단</strong><div class="indented"><figure id="233b8b2c-1c23-8067-96bd-d98558727522" class="image"><a href="images/Untitled%2018.png"><img style="width:960px" src="images/Untitled%2018.png"/></a></figure><p id="233b8b2c-1c23-8054-9423-d75518920c99" class="">
</p></div></p><p id="233b8b2c-1c23-8017-be62-eb1a940a19ab" class=""><strong>(2) Netfilter 패킷 차단</strong><div class="indented"><figure id="233b8b2c-1c23-8037-bd4f-e7a4ee12f40b" class="image"><a href="images/Untitled%2019.png"><img style="width:960px" src="images/Untitled%2019.png"/></a></figure><p id="233b8b2c-1c23-80b1-83a5-c8b65119544d" class="">
</p></div></p><p id="233b8b2c-1c23-803f-a6ab-c5c6c50334b0" class=""><strong>(3) TC Ingress 패킷 차단 : </strong>Packet Scheduler(Qdisc) - QoS, Filter, NAT, 패킷 Mangle, 패킷 미러, 등<div class="indented"><figure id="233b8b2c-1c23-80ef-9c4a-c4d3a4bb0e0f" class="image"><a href="images/Untitled%2020.png"><img style="width:960px" src="images/Untitled%2020.png"/></a></figure><p id="233b8b2c-1c23-8019-a07b-e174a3c343f9" class="">
</p></div></p><p id="233b8b2c-1c23-8059-a445-f3f9df5a956c" class=""><strong>(4) XDP 패킷 차단</strong><div class="indented"><figure id="233b8b2c-1c23-802e-8bf0-c3300fc28ba9" class="image"><a href="images/Untitled%2021.png"><img style="width:960px" src="images/Untitled%2021.png"/></a></figure></div></p><p id="233b8b2c-1c23-8011-9e69-e6651047057e" class="">
</p><ul id="233b8b2c-1c23-8079-b0bc-c3a7b9129dff" class="bulleted-list"><li style="list-style-type:disc"><strong>XDP Actions</strong> : XDP_DROP, XDP_ABORT, XDP_PASS, XDP_TX, XDP_REDIRECT<figure id="233b8b2c-1c23-8072-bac2-cd68e82b999d" class="image"><a href="images/Untitled%2022.png"><img style="width:960px" src="images/Untitled%2022.png"/></a></figure></li></ul><p id="233b8b2c-1c23-804d-a111-e1e5e89a1dc1" class="">
</p><ul id="233b8b2c-1c23-8001-9fce-ff14246459b2" class="bulleted-list"><li style="list-style-type:disc"><strong>XDP Mode</strong> : Generic(Linux Kernel Network Stack), Native(Network Driver, Intel 등), <strong>Offloaded</strong>(Network Hardware, Netronome)<figure id="233b8b2c-1c23-8022-8fb1-e3303c30e9da" class="image"><a href="images/Untitled%2023.png"><img style="width:960px" src="images/Untitled%2023.png"/></a></figure><figure id="233b8b2c-1c23-80b5-ab85-eb7334e5d96e" class="image"><a href="images/Untitled%2024.png"><img style="width:672px" src="images/Untitled%2024.png"/></a><figcaption><a href="https://www.netronome.com/blog/open-source-packet-filtering-bpf-fosdem19/">https://www.netronome.com/blog/open-source-packet-filtering-bpf-fosdem19/</a></figcaption></figure><p id="233b8b2c-1c23-803d-9f79-f5301d4a8f84" class="">
</p></li></ul><ul id="233b8b2c-1c23-8067-94d0-fbcb4533915f" class="bulleted-list"><li style="list-style-type:disc">패킷 차단 비교<figure id="233b8b2c-1c23-80e8-b29b-e556d4f26aff" class="image"><a href="images/Untitled%2025.png"><img style="width:960px" src="images/Untitled%2025.png"/></a></figure><figure id="233b8b2c-1c23-803a-a004-c5cf01da1159" class="image"><a href="images/Untitled%2026.png"><img style="width:960px" src="images/Untitled%2026.png"/></a></figure></li></ul><p id="233b8b2c-1c23-80d5-a014-fbf60579eeec" class="">
</p><ul id="233b8b2c-1c23-8033-9dcc-f7bf4945e077" class="bulleted-list"><li style="list-style-type:disc"><strong>Native XDP 지원 NIC :</strong> 아래 Netronome 는 XDP Offload 지원<figure id="233b8b2c-1c23-8041-91e7-d679793accc3" class="image"><a href="images/Untitled%2027.png"><img style="width:864px" src="images/Untitled%2027.png"/></a></figure></li></ul><p id="233b8b2c-1c23-80e2-8219-d3c14afe5eee" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8062-8c73-d8cb0b7ab5b0" class="toggle"><li><details open=""><summary>2가지 네트워크 모드를 제공 : <strong>터널</strong> 모드(VXLAN, GENEVE), <strong>네이티브</strong> 라우팅 모드 - <a href="https://docs.cilium.io/en/stable/network/concepts/routing/">Docs</a></summary><ul id="233b8b2c-1c23-809e-899e-d04b3cdf3932" class="bulleted-list"><li style="list-style-type:disc">In the <strong>tunnel mode</strong>, Cilium sets up a number of <strong>VXLAN(UDP 8472)</strong> or <strong>Geneve(UDP 6081)</strong> interfaces and forwards traffic over them.</li></ul><ul id="233b8b2c-1c23-80c3-a0e1-db55bf7d844e" class="bulleted-list"><li style="list-style-type:disc">In the <strong>native-routing mode</strong>, Cilium does nothing to setup reachability, assuming that it will be provided externally. </li></ul><figure id="233b8b2c-1c23-8077-8f16-fab793573616" class="image"><a href="images/Untitled%2028.png"><img style="width:912px" src="images/Untitled%2028.png"/></a></figure><ul id="233b8b2c-1c23-8071-afe2-f4d6dc81a2ae" class="bulleted-list"><li style="list-style-type:disc">native-routeing mode 구성</li></ul><figure id="233b8b2c-1c23-8028-b867-f6b3004a3d17" class="image"><a href="images/image%204.png"><img style="width:912px" src="images/image%204.png"/></a></figure><p id="233b8b2c-1c23-8075-941a-e490c12774b0" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-801a-88a2-f309d373ed40" class="toggle"><li><details open=""><summary>IP Address Management (<strong>IPAM</strong>) : 네트워크 엔드포인트(컨테이너 등)에 제공하는 IP 관리 방안 - <a href="https://docs.cilium.io/en/stable/network/concepts/ipam/">Docs</a> , <a href="https://isovalent.com/blog/post/overcoming-kubernetes-ip-address-exhaustion-with-cilium/">Blog</a></summary><ul id="233b8b2c-1c23-80d5-868e-ef2ad1f17cef" class="bulleted-list"><li style="list-style-type:disc">IPAM mode<ul id="233b8b2c-1c23-80ed-a212-f6461d8d2a6e" class="bulleted-list"><li style="list-style-type:circle"><strong>Kubernetes Host Scope</strong> : Kube Controller Manager 에 의해 노드에 할당되어 있는 PodCIDR → 실습 환경에서 사용<figure id="233b8b2c-1c23-8079-ba82-db498d328dca" class="image"><a href="images/image%205.png"><img style="width:768px" src="images/image%205.png"/></a><figcaption><a href="https://isovalent.com/blog/post/overcoming-kubernetes-ip-address-exhaustion-with-cilium">https://isovalent.com/blog/post/overcoming-kubernetes-ip-address-exhaustion-with-cilium</a></figcaption></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-80c3-913e-d1f18b08e1d0" class="code"><code class="language-Bash">kubectl get ciliumnode kind-worker -o jsonpath=&#x27;{.spec.ipam}&#x27;
{&quot;podCIDRs&quot;:[&quot;10.244.1.0/24&quot;]}</code></pre><p id="233b8b2c-1c23-8067-8a86-c50954f5798d" class="">
</p></li></ul><ul id="233b8b2c-1c23-8003-bb71-d7ea61effe9d" class="bulleted-list"><li style="list-style-type:circle"><strong>Cluster Scope (Default)</strong> : Cilium (CiliumNode CRD) 을 통해 관리하는 Pod CIDRs<figure id="233b8b2c-1c23-80c5-909f-c96284767953" class="image"><a href="images/image%206.png"><img style="width:768px" src="images/image%206.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="233b8b2c-1c23-805d-9451-f3c1ffe23b71" class="code"><code class="language-Bash">cilium config view | grep cluster-pool
cluster-pool-ipv4-cidr                     10.0.0.0/8
cluster-pool-ipv4-mask-size                24
ipam                                       cluster-pool</code></pre><p id="233b8b2c-1c23-802c-a4d6-f8f03344a8d7" class="">
</p></li></ul><ul id="233b8b2c-1c23-801a-8dda-ce6b992e19e8" class="bulleted-list"><li style="list-style-type:circle">Multi-Pool (Beta) : 여러 IPAM 풀에서 PodCIDR을 할당, 동일 노드에서 다른 IP 주소 할당 가능, 필요 시 노드에 동적으로 PodCIDR 추가 가능.<figure id="233b8b2c-1c23-800f-834b-e7df7db89b92" class="image"><a href="images/image%207.png"><img style="width:864px" src="images/image%207.png"/></a></figure><p id="233b8b2c-1c23-8007-8f7a-dfeccdcfa92a" class="">
</p></li></ul><ul id="233b8b2c-1c23-8080-80d6-c75a3fa4c30c" class="bulleted-list"><li style="list-style-type:circle"><strong>AWS ENI</strong> (without and with Prefix Delegation)</li></ul><ul id="233b8b2c-1c23-80df-a940-d8cb804b37c6" class="bulleted-list"><li style="list-style-type:circle"><strong>Azure IPAM</strong></li></ul><ul id="233b8b2c-1c23-80b8-a88f-f82867884705" class="bulleted-list"><li style="list-style-type:circle">LoadBalancer Service IPAM</li></ul><ul id="233b8b2c-1c23-80c6-b181-c43e63fdaf45" class="bulleted-list"><li style="list-style-type:circle">Egress Gateway IPAM</li></ul><ul id="233b8b2c-1c23-80f5-9225-ec71d7178e9b" class="bulleted-list"><li style="list-style-type:circle">등등</li></ul></li></ul><p id="233b8b2c-1c23-804d-af0a-c20114a03500" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8075-a47d-f7c45276ed46" class="toggle"><li><details open=""><summary><mark class="highlight-blue"><strong>Cilium 100% Kube-proxy replacement</strong></mark> : Datapath Optimizations (iptables bypass) - <a href="https://docs.cilium.io/en/latest/network/kubernetes/kubeproxy-free/">Docs</a></summary><ul id="233b8b2c-1c23-8094-97e5-e5254bf744c2" class="bulleted-list"><li style="list-style-type:disc">하지만 iptables 기능을 활용하는 일부 동작들은 이슈가 발생할 수 있음, 물론 지속해서 해당 이슈를 해결하고 있음 (예. istio)</li></ul><figure id="233b8b2c-1c23-80f6-8ac1-d6f03e6fd982" class="image"><a href="images/Untitled%2029.png"><img style="width:720px" src="images/Untitled%2029.png"/></a></figure><ul id="233b8b2c-1c23-8096-a1ca-f32d4f10b01d" class="bulleted-list"><li style="list-style-type:disc">예시) 기존의 IPtables 를 사용하지 않고, eBPF 로 Masqueading(SNAT) 처리<figure id="233b8b2c-1c23-800f-acae-c02eef812759" class="image"><a href="images/Untitled%2030.png"><img style="width:720px" src="images/Untitled%2030.png"/></a></figure></li></ul><p id="233b8b2c-1c23-8005-ad9d-f8507d658520" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80a1-9ae5-d6e7f8856068" class="toggle"><li><details open=""><summary><strong>Cilium 100% Kube-proxy replacement</strong> 동작 소개 1 - <a href="https://www.youtube.com/watch?v=yKPNmhckJHY">Youtube</a></summary><ul id="233b8b2c-1c23-8055-bf2a-e3e0d5a915e5" class="bulleted-list"><li style="list-style-type:disc">Basic container netwokring<figure id="233b8b2c-1c23-80e5-a9a0-e5fa74bb1b88" class="image"><a href="images/CleanShot_2025-06-15_at_16.58.34.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_16.58.34.png"/></a></figure><p id="233b8b2c-1c23-8076-9a2b-c85fc6395ce3" class="">
</p></li></ul><ul id="233b8b2c-1c23-80f7-b58d-e955ad0f781b" class="bulleted-list"><li style="list-style-type:disc">eBPF Host Routing<figure id="233b8b2c-1c23-80d4-872b-fb90f1fa58c3" class="image"><a href="images/CleanShot_2025-06-15_at_16.58.52.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_16.58.52.png"/></a></figure><figure id="233b8b2c-1c23-8086-9e9e-fae5c69b1854" class="image"><a href="images/CleanShot_2025-06-15_at_16.59.13.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_16.59.13.png"/></a></figure><figure id="233b8b2c-1c23-80ec-83a5-e0924eee819a" class="image"><a href="images/CleanShot_2025-06-15_at_16.59.34.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_16.59.34.png"/></a></figure><p id="233b8b2c-1c23-80bd-bd18-d89c441534ad" class="">
</p></li></ul><ul id="233b8b2c-1c23-80dc-aa48-d1e7147d100e" class="bulleted-list"><li style="list-style-type:disc">Netkit devices<figure id="233b8b2c-1c23-8066-a9be-d44f7c1a4d65" class="image"><a href="images/CleanShot_2025-06-15_at_16.59.52.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_16.59.52.png"/></a></figure><figure id="233b8b2c-1c23-8084-a9a9-fa7f9b7832ce" class="image"><a href="images/CleanShot_2025-06-15_at_17.00.09.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_17.00.09.png"/></a></figure><figure id="233b8b2c-1c23-8063-84ee-c36c1120646a" class="image"><a href="images/CleanShot_2025-06-15_at_17.00.24.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_17.00.24.png"/></a></figure><p id="233b8b2c-1c23-8066-8f0f-f81ece2d300a" class="">
</p></li></ul><ul id="233b8b2c-1c23-80cb-b8c9-c19dc5264ac0" class="bulleted-list"><li style="list-style-type:disc">Data plane tuning - <a href="https://docs.cilium.io/en/stable/operations/performance/tuning/">Docs</a><figure id="233b8b2c-1c23-8097-b690-dd44756bf025" class="image"><a href="images/CleanShot_2025-06-15_at_17.00.40.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_17.00.40.png"/></a></figure><p id="233b8b2c-1c23-800e-a45a-fb904dde5a13" class="">
</p></li></ul><ul id="233b8b2c-1c23-801f-909f-e39ec5566f5e" class="bulleted-list"><li style="list-style-type:disc"><code>cilium connectivity perf</code><figure id="233b8b2c-1c23-806e-9348-f4f869096873" class="image"><a href="images/CleanShot_2025-06-15_at_17.01.03.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_17.01.03.png"/></a></figure><figure id="233b8b2c-1c23-80ab-a8ff-e856b5221e5f" class="image"><a href="images/CleanShot_2025-06-15_at_17.01.30.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_17.01.30.png"/></a></figure></li></ul><p id="233b8b2c-1c23-8084-aa6e-dd6b5a44cc11" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-8004-8171-dbb52922b1b1" class="toggle"><li><details open=""><summary><strong>Cilium 100%  Kube-proxy replacement</strong> 동작 소개 2 : <strong>ByteDance 사례*</strong> - <a href="https://www.youtube.com/watch?v=cKPW67D7X10">Youtube</a> , <a href="https://kccncchn2025.sched.com/event/1x5hK/simplifying-the-networking-and-security-stack-with-cilium-hubble-and-tetragon-liyi-huang-isovalent-at-cisco-kaixi-fan-bytedance">CNCF</a></summary><ul id="233b8b2c-1c23-8048-8323-c78b0c1a411e" class="bulleted-list"><li style="list-style-type:disc">k8s 네트워킹 : kube-proxy with IPTables<figure id="233b8b2c-1c23-805a-85ff-e612b2041bb6" class="image"><a href="images/CleanShot_2025-06-15_at_15.55.28.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_15.55.28.png"/></a></figure><figure id="233b8b2c-1c23-80b9-9d2a-c5e2beef9a8b" class="image"><a href="images/CleanShot_2025-06-15_at_15.56.27.png"><img style="width:816px" src="images/CleanShot_2025-06-15_at_15.56.27.png"/></a></figure><p id="233b8b2c-1c23-80bf-9bb5-d0ee8bab474c" class="">
</p></li></ul><ul id="233b8b2c-1c23-806a-94a4-ffacde2e5180" class="bulleted-list"><li style="list-style-type:disc">[Cilium] Kube-proxy replacement : 호스트 네트워크 스택(IPtables/Chains) Skip<figure id="233b8b2c-1c23-8025-8ff4-f918d5b34d57" class="image"><a href="images/CleanShot_2025-06-15_at_15.56.43.png"><img style="width:1200px" src="images/CleanShot_2025-06-15_at_15.56.43.png"/></a></figure><p id="233b8b2c-1c23-80e3-99f8-fd3bd9c96e4c" class="">
</p></li></ul><ul id="233b8b2c-1c23-8027-b0dc-c57a8d46f06c" class="bulleted-list"><li style="list-style-type:disc">[Cilium] eBPF host routing : IPTables POSTROUTING Skip!<figure id="233b8b2c-1c23-80b1-9d34-c8e460ee3407" class="image"><a href="images/CleanShot_2025-06-15_at_15.58.04.png"><img style="width:1248px" src="images/CleanShot_2025-06-15_at_15.58.04.png"/></a></figure><p id="233b8b2c-1c23-8076-a34b-e232a8b2d966" class="">
</p></li></ul><ul id="233b8b2c-1c23-807f-888a-cb191a469279" class="bulleted-list"><li style="list-style-type:disc">[Cilium] Netkit : Pod/Host 간 veth pair Skip! - <a href="https://isovalent.com/blog/post/cilium-netkit-a-new-container-networking-paradigm-for-the-ai-era/">Blog</a><figure id="233b8b2c-1c23-80ea-af3b-f88d54006481" class="image"><a href="images/CleanShot_2025-06-15_at_15.58.49.png"><img style="width:1104px" src="images/CleanShot_2025-06-15_at_15.58.49.png"/></a></figure><p id="233b8b2c-1c23-8036-b945-c5f759392bde" class="">
</p></li></ul><ul id="233b8b2c-1c23-800f-8112-f55e1d92ba04" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] 네트워크 현황 소개<figure id="233b8b2c-1c23-8049-8571-f62b56e5e7f0" class="image"><a href="images/CleanShot_2025-06-15_at_16.00.13.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.00.13.png"/></a></figure><figure id="233b8b2c-1c23-809f-90b8-ff0461aa03e1" class="image"><a href="images/CleanShot_2025-06-15_at_16.00.22.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.00.22.png"/></a></figure><figure id="233b8b2c-1c23-8010-989b-d4dc287153e3" class="image"><a href="images/CleanShot_2025-06-15_at_16.00.40.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.00.40.png"/></a></figure><figure id="233b8b2c-1c23-8021-a300-e0dcdececfbb" class="image"><a href="images/CleanShot_2025-06-15_at_16.00.56.png"><img style="width:1104px" src="images/CleanShot_2025-06-15_at_16.00.56.png"/></a></figure><p id="233b8b2c-1c23-8074-95a8-e2ae56c59e27" class="">
</p></li></ul><ul id="233b8b2c-1c23-80ce-bc65-d832b285349e" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] Kube-proxy replacement<figure id="233b8b2c-1c23-8024-9f2d-f05600b7ff39" class="image"><a href="images/CleanShot_2025-06-15_at_16.02.00.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.02.00.png"/></a></figure><p id="233b8b2c-1c23-8050-a4f5-f6baf48ceeab" class="">
</p></li></ul><ul id="233b8b2c-1c23-806a-b997-e4e52886d714" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] DSR<figure id="233b8b2c-1c23-80cd-b626-d340070ce893" class="image"><a href="images/CleanShot_2025-06-15_at_16.02.17.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.02.17.png"/></a></figure><p id="233b8b2c-1c23-803c-a6a9-d541481cbc34" class="">
</p></li></ul><ul id="233b8b2c-1c23-804e-9524-fb8345fa5558" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] Maglev : 일관성 있는 해싱 지원<figure id="233b8b2c-1c23-80db-9365-cf411e74a660" class="image"><a href="images/CleanShot_2025-06-15_at_16.03.00.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.03.00.png"/></a></figure><p id="233b8b2c-1c23-809c-833d-da8e422f1a1a" class="">
</p></li></ul><ul id="233b8b2c-1c23-80ce-829e-f4bcd9f24569" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] VXLAN + host routing : 오버 헤더, 추가 홉, 복잡<figure id="233b8b2c-1c23-80a1-a30d-d97f4f34a0b8" class="image"><a href="images/CleanShot_2025-06-15_at_16.03.42.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.03.42.png"/></a></figure><p id="233b8b2c-1c23-80d6-8953-d566f142df01" class="">
</p></li></ul><ul id="233b8b2c-1c23-8028-ac5c-ff718c1e301f" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] ebpf host routing<figure id="233b8b2c-1c23-80e9-88e9-fa6759c0f61e" class="image"><a href="images/CleanShot_2025-06-15_at_16.04.51.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.04.51.png"/></a></figure><p id="233b8b2c-1c23-8062-82f4-cc12d8c2d690" class="">
</p></li></ul><ul id="233b8b2c-1c23-8075-b492-f6cc6f40fddc" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] VXLAN vs <strong>Native routing</strong> <figure id="233b8b2c-1c23-806a-96c1-efb0d37e2483" class="image"><a href="images/CleanShot_2025-06-15_at_16.05.22.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.05.22.png"/></a></figure><p id="233b8b2c-1c23-80ed-8328-c7bf0aa4a074" class="">
</p></li></ul><ul id="233b8b2c-1c23-8060-ac49-e7812510cdfb" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] Native routing vs <strong>eBPF host routing</strong><figure id="233b8b2c-1c23-80ef-a31b-fda35c7482dd" class="image"><a href="images/CleanShot_2025-06-15_at_16.05.55.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.05.55.png"/></a></figure><p id="233b8b2c-1c23-808a-b74c-ce6456f74a91" class="">
</p></li></ul><ul id="233b8b2c-1c23-8098-a31b-d10c5c5bfde6" class="bulleted-list"><li style="list-style-type:disc">[ByteDance] netkit : 파드에서 호스트 NIC 로 바로 전달!<figure id="233b8b2c-1c23-80fb-8240-d6a0a4d0982a" class="image"><a href="images/CleanShot_2025-06-15_at_16.06.56.png"><img style="width:864px" src="images/CleanShot_2025-06-15_at_16.06.56.png"/></a></figure></li></ul><p id="233b8b2c-1c23-8007-8b89-dd623fbaa892" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-80bf-9ee5-da353b85a1ab" class="toggle"><li><details open=""><summary><strong>Cilium 구성요소</strong> - <a href="https://docs.cilium.io/en/stable/overview/component-overview/">Docs</a></summary><figure id="233b8b2c-1c23-80ee-867e-e639d2d20fde" class="image"><a href="images/Untitled%2031.png"><img style="width:624px" src="images/Untitled%2031.png"/></a><figcaption><a href="https://github.com/cilium/cilium">https://github.com/cilium/cilium</a></figcaption></figure><ul id="233b8b2c-1c23-808e-ac01-f83b5d881940" class="bulleted-list"><li style="list-style-type:disc">Cilium <strong>Operator</strong> : K8S 클러스터에 대한 한 번씩 처리해야 하는 작업을 관리. </li></ul><ul id="233b8b2c-1c23-8053-9734-c56f6967e878" class="bulleted-list"><li style="list-style-type:disc">Cilium <strong>Agent</strong> : 데몬셋으로 실행, K8S API 설정으로 부터 &#x27;네트워크 설정, 네트워크 정책, 서비스 부하분산, 모니터링&#x27; 등을 수행하며, eBPF 프로그램을 관리한다.</li></ul><ul id="233b8b2c-1c23-8086-81e2-fa86fa2d578c" class="bulleted-list"><li style="list-style-type:disc">Cilium <strong>Client</strong> (CLI) : Cilium 커멘드툴, eBPF maps 에 직접 접속하여 상태를 확인할 수 있다.</li></ul><ul id="233b8b2c-1c23-80b4-8a38-c373e13f907f" class="bulleted-list"><li style="list-style-type:disc"><strong>Hubble</strong> : 네트워크와 보안 모니터링 플랫폼 역할을 하여, &#x27;Server, Relay, Client, Graphical UI&#x27; 로 구성되어 있다.</li></ul><ul id="233b8b2c-1c23-80cf-a950-dbf8225d0bcc" class="bulleted-list"><li style="list-style-type:disc"><strong>Data Store</strong> : Cilium Agent 간의 상태를 저장하고 전파하는 데이터 저장소, 2가지 종류 중 선택(K8S CRDs, Key-Value Store)</li></ul><figure id="233b8b2c-1c23-807a-b232-c698c8b983ae" class="image"><a href="images/Untitled%2032.png"><img style="width:864px" src="images/Untitled%2032.png"/></a><figcaption>Cilium control flow</figcaption></figure><figure id="233b8b2c-1c23-80de-aa18-c09686f2f2fe" class="image"><a href="images/CleanShot_2024-10-20_at_14.24.15.png"><img style="width:1056px" src="images/CleanShot_2024-10-20_at_14.24.15.png"/></a></figure><figure id="233b8b2c-1c23-80ee-a006-cf83aff7efad" class="image"><a href="images/CleanShot_2024-10-20_at_14.24.28.png"><img style="width:816px" src="images/CleanShot_2024-10-20_at_14.24.28.png"/></a></figure><p id="233b8b2c-1c23-8060-9462-f329ea73fbd5" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-802c-ba94-ce41d538ad47" class="bulleted-list"><li style="list-style-type:disc"><strong>Cilium eBPF Datapath</strong> - <a href="https://docs.cilium.io/en/v1.10/concepts/ebpf/">링크</a><ul id="233b8b2c-1c23-8189-98ee-f61f870de6eb" class="toggle"><li><details open=""><summary><strong>Introduction</strong> - <a href="https://docs.cilium.io/en/v1.10/concepts/ebpf/intro/">링크</a></summary><ul id="233b8b2c-1c23-8190-bc3b-de3b1bcd1ec7" class="bulleted-list"><li style="list-style-type:disc">The Linux kernel supports a set of <strong>BPF hooks</strong> in the networking stack that can be used to run <strong>BPF programs</strong>.</li></ul><ul id="233b8b2c-1c23-8105-a6aa-da2a29cfb804" class="toggle"><li><details open=""><summary><strong>XDP</strong> : 네트워킹 드라이버의 가장 앞 단에서 XDP BPF hook 을 통해서 BPF program 을 트리거되기 때문에, 가능한 최고의 패킷 처리 성능을 제공</summary><figure id="233b8b2c-1c23-818e-a2e8-efc1fbb5d142" class="image"><a href="images/Untitled%2033.png"><img style="width:816px" src="images/Untitled%2033.png"/></a><figcaption><a href="https://cilium.io/blog/2020/06/22/cilium-18">https://cilium.io/blog/2020/06/22/cilium-18</a></figcaption></figure><ul id="233b8b2c-1c23-812c-ba6b-c2619773440c" class="bulleted-list"><li style="list-style-type:disc"><strong>Prefilter:</strong> An <strong>XDP</strong> program and provides a set of prefilter rules used to <strong>filter</strong> <strong>traffic</strong> from the network for best performance.</li></ul><ul id="233b8b2c-1c23-81d2-b84c-e4003972472a" class="bulleted-list"><li style="list-style-type:disc"><strong>LoadBalancer &amp; NodePort XDP Acceleration</strong></li></ul><ul id="233b8b2c-1c23-816c-a593-eeb7c35aba2c" class="bulleted-list"><li style="list-style-type:disc"><strong>XDP-based Standalone Load Balancer : Maglev Consistent Hashing , n-Tuple PCAP Recorder</strong><figure id="233b8b2c-1c23-81cf-bd84-c8bf46918b14" class="image"><a href="images/Untitled%2034.png"><img style="width:768px" src="images/Untitled%2034.png"/></a></figure><figure id="233b8b2c-1c23-8119-9726-d5b76a09498f"><a href="https://asciinema.org/a/TIpQRjdAlsQOsbsFtxwLJiVec" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Hubble Recorder Demo</div><div class="bookmark-description">Recorded by user:154740</div></div><div class="bookmark-href"><img src="https://asciinema.org/images/favicon-2d62dafa447cf018340b7121007568e3.png?vsn=d" class="icon bookmark-icon"/>https://asciinema.org/a/TIpQRjdAlsQOsbsFtxwLJiVec</div></div><img src="https://asciinema.org/a/TIpQRjdAlsQOsbsFtxwLJiVec.png" class="bookmark-image"/></a></figure></li></ul><p id="233b8b2c-1c23-8196-b7d8-c39b4430b021" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-81b6-b991-c8834e7bef6d" class="bulleted-list"><li style="list-style-type:disc"><strong>TC</strong> (<strong>T</strong>raffic <strong>C</strong>ontrol) Ingress/Egress ⇒ 현재는 <strong>TCX 사용 </strong><a href="https://nuguni.tistory.com/101">https://nuguni.tistory.com/101</a><ul id="233b8b2c-1c23-810c-b555-caa71f8fd6d0" class="bulleted-list"><li style="list-style-type:circle"><strong>Network Interface</strong> 에 <strong>tc</strong> ingress hook 에서 BPF programs 실행된다.</li></ul><ul id="233b8b2c-1c23-8133-96c7-c08f1837fbf4" class="bulleted-list"><li style="list-style-type:circle">파드와 연결된 veth pair 의 lxc 의 tc ingress hook 에서 BPF programs 실행된다. 노드(호스트)로 in/out 트래픽 모두를 모니터링 및 통제/정책을 적용할 수 있다.</li></ul></li></ul><ul id="233b8b2c-1c23-8172-9e3e-e239b017eb1e" class="bulleted-list"><li style="list-style-type:disc"><strong>Socket operations</strong> : BPF socket operations program 은 root cgroup 에 연결되며 TCP event(ESTABLISHED) 에서 실행됨</li></ul><ul id="233b8b2c-1c23-81d0-ad67-f6c93d4c2d01" class="bulleted-list"><li style="list-style-type:disc"><strong>Socket send/recv</strong> : The socket send/recv hook 은 TCP socket 의 모든 송수신 작업에서 실행, hook 에서 검사/삭제/리다이렉션을 할 수 있다</li></ul><ul id="233b8b2c-1c23-8138-a08f-e18bcb10d2c1" class="bulleted-list"><li style="list-style-type:disc"><strong>Endpoint Policy:</strong> 정책에 따라 패킷을 차단/전달하거나, 서비스로 전달하거나, L7 정책 전달 할 수 있다.<ul id="233b8b2c-1c23-810c-b065-efc9859788c4" class="bulleted-list"><li style="list-style-type:circle">the Cilium datapath responsible for mapping packets to identities and enforcing L3 and L4 policies.</li></ul></li></ul><ul id="233b8b2c-1c23-815c-b909-c0af892562e3" class="bulleted-list"><li style="list-style-type:disc"><strong>Service:</strong> 모든 패킷의 목적지 IP/Port 의 map 조회 시 일치하면 L3/L4 endpoint 로 전달하며, Service block 는 모든 인터페이스의 TC ingress hook 에서 동작할 수 있다.</li></ul><ul id="233b8b2c-1c23-8186-95bc-d49b2ff85396" class="bulleted-list"><li style="list-style-type:disc">L3 Encryption, Socket Layer Enforcement : skip~</li></ul><ul id="233b8b2c-1c23-8104-962c-d50c15770959" class="bulleted-list"><li style="list-style-type:disc"><strong>L7 Policy:</strong> The <strong>L7 Policy</strong> object <strong>redirect</strong> <strong>proxy</strong> <strong>traffic</strong> to a Cilium userspace <strong>proxy</strong> <strong>instance</strong>. Cilium uses an <mark class="highlight-red"><strong>Envoy</strong></mark> instance as its <strong>userspace proxy</strong>. Envoy will then either forward the traffic or generate appropriate reject messages based on the configured L7 policy.</li></ul><p id="233b8b2c-1c23-811c-b65a-f9ac5766be9b" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-81af-a62a-fbb6b23c2ca6" class="toggle"><li><details open=""><summary><mark class="highlight-blue"><strong>Life of a Packet*</strong></mark> - <a href="https://docs.cilium.io/en/v1.10/concepts/ebpf/lifeofapacket/">링크</a></summary><p id="233b8b2c-1c23-8143-8a89-c1df4819f739" class=""><code>Endpoint to Endpoint</code> : 그림처럼 L7 정책 시에는 커널 hookpoint 와 Userspace Proxy 사용으로 성능이 조금 떨어질 수 있다<div class="indented"><figure id="233b8b2c-1c23-814f-b149-ce40eadff0ac" class="image"><a href="images/image%208.png"><img style="width:949.984375px" src="images/image%208.png"/></a></figure><p id="233b8b2c-1c23-810d-82df-fccbf0fdf577" class="">
</p></div></p><p id="233b8b2c-1c23-81d6-9005-c94a9ccf313f" class=""><code>Egress from Endpoint</code><div class="indented"><figure id="233b8b2c-1c23-81f3-8fae-f11235e8de34" class="image"><a href="images/image%209.png"><img style="width:949.984375px" src="images/image%209.png"/></a></figure><p id="233b8b2c-1c23-81d1-9555-ccd19082f894" class="">
</p></div></p><p id="233b8b2c-1c23-81c5-befa-c9b7c58497b5" class=""><code>Ingress to Endpoint</code><div class="indented"><figure id="233b8b2c-1c23-8158-a8a0-c5e4d4326551" class="image"><a href="images/image%2010.png"><img style="width:949.9921875px" src="images/image%2010.png"/></a></figure></div></p></details></li></ul><ul id="233b8b2c-1c23-811f-8e19-ce80efe56269" class="toggle"><li><details open=""><summary><strong>eBPF Maps</strong> - <a href="https://docs.cilium.io/en/v1.10/concepts/ebpf/maps/">링크</a></summary><ul id="233b8b2c-1c23-8197-9c32-d4460c44cb0d" class="bulleted-list"><li style="list-style-type:disc">모든 eBPF Map 은 상한 용량이 있으며, limit 관련 여러 옵션들이 있다.</li></ul><div id="233b8b2c-1c23-81f6-9528-fa1148f5f2df" class="collection-content"><h4 class="collection-title">eBPF Map</h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><div data-testid="/icons/font_gray.svg" style="width:14px;height:14px;flex-shrink:0;transform:scale(1.2);mask:url(/icons/font_gray.svg?mode=light) no-repeat center;-webkit-mask:url(/icons/font_gray.svg?mode=light) no-repeat center;background-color:rgba(71, 70, 68, 0.6);fill:rgba(71, 70, 68, 0.6)"></div></span>Map Name</th><th><span class="icon property-icon"><div data-testid="/icons/arrow-circle-down_gray.svg" style="width:14px;height:14px;flex-shrink:0;transform:scale(1.2);mask:url(/icons/arrow-circle-down_gray.svg?mode=light) no-repeat center;-webkit-mask:url(/icons/arrow-circle-down_gray.svg?mode=light) no-repeat center;background-color:rgba(71, 70, 68, 0.6);fill:rgba(71, 70, 68, 0.6)"></div></span>Scope</th><th><span class="icon property-icon"><div data-testid="/icons/description_gray.svg" style="width:14px;height:14px;flex-shrink:0;transform:scale(1.2);mask:url(/icons/description_gray.svg?mode=light) no-repeat center;-webkit-mask:url(/icons/description_gray.svg?mode=light) no-repeat center;background-color:rgba(71, 70, 68, 0.6);fill:rgba(71, 70, 68, 0.6)"></div></span>Default Limit</th><th><span class="icon property-icon"><div data-testid="/icons/description_gray.svg" style="width:14px;height:14px;flex-shrink:0;transform:scale(1.2);mask:url(/icons/description_gray.svg?mode=light) no-repeat center;-webkit-mask:url(/icons/description_gray.svg?mode=light) no-repeat center;background-color:rgba(71, 70, 68, 0.6);fill:rgba(71, 70, 68, 0.6)"></div></span>Scale Implications</th></tr></thead><tbody><tr id="233b8b2c-1c23-81f2-82db-c33e158d59b3"><td class="cell-title"><a href="https://www.notion.so/Connection-Tracking-233b8b2c1c2381f282dbc33e158d59b3?pvs=21">Connection Tracking</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-pink">node or endpoint</span></td><td class="cell-@iDk">1M TCP/256k UDP</td><td class="cell-ZtD|">Max 1M concurrent TCP connections, max 256k expected UDP answers</td></tr><tr id="233b8b2c-1c23-81c1-aa35-df567a103caf"><td class="cell-title"><a href="https://www.notion.so/NAT-233b8b2c1c2381c1aa35df567a103caf?pvs=21">NAT</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">512k</td><td class="cell-ZtD|">Max 512k NAT entries</td></tr><tr id="233b8b2c-1c23-8162-9180-dbce418136e6"><td class="cell-title"><a href="https://www.notion.so/Neighbor-Table-233b8b2c1c2381629180dbce418136e6?pvs=21">Neighbor Table</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">512k</td><td class="cell-ZtD|">Max 512k neighbor entries</td></tr><tr id="233b8b2c-1c23-8185-9406-d7bf71b38b93"><td class="cell-title"><a href="https://www.notion.so/Endpoints-233b8b2c1c2381859406d7bf71b38b93?pvs=21">Endpoints</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">64k</td><td class="cell-ZtD|">Max 64k local endpoints + host IPs per node</td></tr><tr id="233b8b2c-1c23-818e-a013-c170e2718019"><td class="cell-title"><a href="https://www.notion.so/IP-cache-233b8b2c1c23818ea013c170e2718019?pvs=21">IP cache</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">512k</td><td class="cell-ZtD|">Max 256k endpoints (IPv4+IPv6), max 512k endpoints (IPv4 or IPv6) across all clusters</td></tr><tr id="233b8b2c-1c23-813a-9753-ef24af27d66f"><td class="cell-title"><a href="https://www.notion.so/Load-Balancer-233b8b2c1c23813a9753ef24af27d66f?pvs=21">Load Balancer</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">64k</td><td class="cell-ZtD|">Max 64k cumulative backends across all services across all clusters</td></tr><tr id="233b8b2c-1c23-819c-8b97-e2fb34b81373"><td class="cell-title"><a href="https://www.notion.so/Policy-233b8b2c1c23819c8b97e2fb34b81373?pvs=21">Policy</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">endpoint</span></td><td class="cell-@iDk">16k</td><td class="cell-ZtD|">Max 16k allowed identity + port + protocol pairs for specific endpoint</td></tr><tr id="233b8b2c-1c23-811e-9bdb-cc10f513f9cd"><td class="cell-title"><a href="https://www.notion.so/Proxy-Map-233b8b2c1c23811e9bdbcc10f513f9cd?pvs=21">Proxy Map</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">512k</td><td class="cell-ZtD|">Max 512k concurrent redirected TCP connections to proxy</td></tr><tr id="233b8b2c-1c23-8116-b211-d068705fe0f7"><td class="cell-title"><a href="https://www.notion.so/Tunnel-233b8b2c1c238116b211d068705fe0f7?pvs=21">Tunnel</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">64k</td><td class="cell-ZtD|">Max 32k nodes (IPv4+IPv6) or 64k nodes (IPv4 or IPv6) across all clusters</td></tr><tr id="233b8b2c-1c23-81f8-9256-e960cddd560e"><td class="cell-title"><a href="https://www.notion.so/IPv4-Fragmentation-233b8b2c1c2381f89256e960cddd560e?pvs=21">IPv4 Fragmentation</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">8k</td><td class="cell-ZtD|">Max 8k fragmented datagrams in flight simultaneously on the node</td></tr><tr id="233b8b2c-1c23-81e5-841a-e4c80f545f82"><td class="cell-title"><a href="https://www.notion.so/Session-Affinity-233b8b2c1c2381e5841ae4c80f545f82?pvs=21">Session Affinity</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">64k</td><td class="cell-ZtD|">Max 64k affinities from different clients</td></tr><tr id="233b8b2c-1c23-81a0-9a06-e140dff81737"><td class="cell-title"><a href="https://www.notion.so/IP-Masq-233b8b2c1c2381a09a06e140dff81737?pvs=21">IP Masq</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">16k</td><td class="cell-ZtD|">Max 16k IPv4 cidrs used by BPF-based ip-masq-agent</td></tr><tr id="233b8b2c-1c23-81d6-80b0-f9238ec2a71d"><td class="cell-title"><a href="https://www.notion.so/Service-Source-Ranges-233b8b2c1c2381d680b0f9238ec2a71d?pvs=21">Service Source Ranges</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">node</span></td><td class="cell-@iDk">64k</td><td class="cell-ZtD|">Max 64k cumulative LB source ranges across all services</td></tr><tr id="233b8b2c-1c23-8137-96ed-dd4ff7bcfefd"><td class="cell-title"><a href="https://www.notion.so/Egress-Policy-233b8b2c1c23813796eddd4ff7bcfefd?pvs=21">Egress Policy</a></td><td class="cell-Xx[B"><span class="selected-value select-value-color-green">endpoint</span></td><td class="cell-@iDk">16k</td><td class="cell-ZtD|">Max 16k endpoints across all destination CIDRs across all clusters</td></tr></tbody></table><br/><br/></div><ul id="233b8b2c-1c23-81af-bd76-dff3cc477957" class="bulleted-list"><li style="list-style-type:disc">kube-proxy 는 리눅스 코어에 따라 CT table 최대 수가 결정되며, Cilium 은 BPF Maps 이라는 자체 연결 추적 테이블을 가지고 메모리에 따라 최대 수가 결정됨</li></ul><figure id="233b8b2c-1c23-81b2-99af-c38598ffc64d" class="image"><a href="images/Untitled%2035.png"><img style="width:672px" src="images/Untitled%2035.png"/></a></figure><p id="233b8b2c-1c23-8115-b34f-efa9192b7888" class="">
</p></details></li></ul><ul id="233b8b2c-1c23-81c8-96b7-f071c1dce06f" class="toggle"><li><details open=""><summary><strong>Iptables Usage</strong> - <a href="https://docs.cilium.io/en/v1.10/concepts/ebpf/iptables/">링크</a></summary><ul id="233b8b2c-1c23-8193-942b-c2536ee2c694" class="bulleted-list"><li style="list-style-type:disc">커널 버전이 낮을 경우 iptables 동작으로 구현 될 수 있다. 혹은 cilium 미동작(장애 등 문제) 시, 트래픽 처리 보완 시.</li></ul><ul id="233b8b2c-1c23-8126-a248-ed5064194872" class="bulleted-list"><li style="list-style-type:disc">아래 그림은 cilium 과 iptables 을 같이 사용 시 다이어그램을 나타낸다.</li></ul><figure id="233b8b2c-1c23-813f-aed5-d75fff2e2268" class="image"><a href="images/kubernetes_iptables.svg"><img style="width:1056px" src="images/kubernetes_iptables.svg"/></a></figure><p id="234b8b2c-1c23-80d2-8f1a-fe1f1a7b7275" class="">
</p></details></li></ul></li></ul><h3 id="233b8b2c-1c23-819a-b353-dc625d19aff7" class="">3. Cilium CNI 설치</h3><ul id="234b8b2c-1c23-807f-b542-f741c6cb8245" class="toggle"><li><details open=""><summary>Cilium 시스템 요구 사항 확인 - <a href="https://docs.cilium.io/en/stable/operations/system_requirements/">Docs</a></summary><ul id="234b8b2c-1c23-8060-ac0c-c8977b11de3d" class="bulleted-list"><li style="list-style-type:disc">AMD64 또는 AArch64 CPU 아키텍처를 사용하는 호스트</li></ul><ul id="234b8b2c-1c23-8078-89fd-eb6d645f8e3e" class="bulleted-list"><li style="list-style-type:disc"><a href="https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel">Linux 커널</a> 5.4 이상 또는 동등 버전(예: RHEL 8.6의 경우 4.18)<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-800c-9b7f-c0dc9ae7f404" class="code"><code class="language-Bash">#
arch
aarch64

#
uname -r
6.8.0-53-generic</code></pre></li></ul><ul id="234b8b2c-1c23-803f-aacc-e8d7380421d4" class="bulleted-list"><li style="list-style-type:disc">커널 구성 옵션 활성화<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8083-92b1-c24f7f57a0e9" class="code"><code class="language-Bash"># [커널 구성 옵션] 기본 요구 사항 
grep -E &#x27;CONFIG_BPF|CONFIG_BPF_SYSCALL|CONFIG_NET_CLS_BPF|CONFIG_BPF_JIT|CONFIG_NET_CLS_ACT|CONFIG_NET_SCH_INGRESS|CONFIG_CRYPTO_SHA1|CONFIG_CRYPTO_USER_API_HASH|CONFIG_CGROUPS|CONFIG_CGROUP_BPF|CONFIG_PERF_EVENTS|CONFIG_SCHEDSTATS&#x27; /boot/config-$(uname -r)
CONFIG_BPF=y
CONFIG_BPF_SYSCALL=y
CONFIG_BPF_JIT=y
CONFIG_NET_CLS_BPF=m
CONFIG_NET_CLS_ACT=y
CONFIG_NET_SCH_INGRESS=m
CONFIG_CRYPTO_SHA1=y
CONFIG_CRYPTO_USER_API_HASH=m
CONFIG_CGROUPS=y
CONFIG_CGROUP_BPF=y
CONFIG_PERF_EVENTS=y
CONFIG_SCHEDSTATS=y


# [커널 구성 옵션] Requirements for Tunneling and Routing
grep -E &#x27;CONFIG_VXLAN=y|CONFIG_VXLAN=m|CONFIG_GENEVE=y|CONFIG_GENEVE=m|CONFIG_FIB_RULES=y&#x27; /boot/config-$(uname -r)
CONFIG_FIB_RULES=y # 커널에 내장됨
CONFIG_VXLAN=m # 모듈로 컴파일됨 → 커널에 로드해서 사용
CONFIG_GENEVE=m # 모듈로 컴파일됨 → 커널에 로드해서 사용

## (참고) 커널 로드
lsmod | grep -E &#x27;vxlan|geneve&#x27;
modprobe geneve
lsmod | grep -E &#x27;vxlan|geneve&#x27;


# [커널 구성 옵션] Requirements for L7 and FQDN Policies
grep -E &#x27;CONFIG_NETFILTER_XT_TARGET_TPROXY|CONFIG_NETFILTER_XT_TARGET_MARK|CONFIG_NETFILTER_XT_TARGET_CT|CONFIG_NETFILTER_XT_MATCH_MARK|CONFIG_NETFILTER_XT_MATCH_SOCKET&#x27; /boot/config-$(uname -r)
CONFIG_NETFILTER_XT_TARGET_CT=m
CONFIG_NETFILTER_XT_TARGET_MARK=m
CONFIG_NETFILTER_XT_TARGET_TPROXY=m
CONFIG_NETFILTER_XT_MATCH_MARK=m
CONFIG_NETFILTER_XT_MATCH_SOCKET=m

...

# [커널 구성 옵션] Requirements for Netkit Device Mode
grep -E &#x27;CONFIG_NETKIT=y|CONFIG_NETKIT=m&#x27; /boot/config-$(uname -r)
</code></pre><p id="234b8b2c-1c23-8071-af30-d6bf5a6add40" class="">
</p></li></ul><ul id="234b8b2c-1c23-80a4-90b9-cb5238dd1c83" class="bulleted-list"><li style="list-style-type:disc">고급 기능 동작을 위한 최소 커널 버전 - <a href="https://docs.cilium.io/en/stable/operations/system_requirements/#required-kernel-versions-for-advanced-features">Docs</a><table id="234b8b2c-1c23-808f-ad67-d4eba54dcd8b" class="simple-table"><thead class="simple-table-header"><tr id="234b8b2c-1c23-8047-ad30-d10cd6e7e2db"><th id="zFby" class="simple-table-header-color simple-table-header" style="width:330px"><strong>Cilium Feature</strong></th><th id="iudx" class="simple-table-header-color simple-table-header" style="width:202.8671875px"><strong>Minimum Kernel Version</strong></th></tr></thead><tbody><tr id="234b8b2c-1c23-8001-afb8-c9577bd0ef8c"><td id="zFby" class="" style="width:330px"><a href="https://docs.cilium.io/en/stable/security/network/encryption-wireguard/#encryption-wg">WireGuard Transparent Encryption</a></td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.6</td></tr><tr id="234b8b2c-1c23-80fa-ad3a-ce75182af3ed"><td id="zFby" class="" style="width:330px">Full support for <a href="https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#session-affinity">Session Affinity</a></td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.7</td></tr><tr id="234b8b2c-1c23-80aa-b008-edcf29a8443f"><td id="zFby" class="" style="width:330px">BPF-based proxy redirection</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.7</td></tr><tr id="234b8b2c-1c23-804d-9c32-c5847a699002"><td id="zFby" class="" style="width:330px">Socket-level LB bypass in pod netns</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.7</td></tr><tr id="234b8b2c-1c23-808f-bfd5-faef53ecdd93"><td id="zFby" class="" style="width:330px">L3 devices</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.8</td></tr><tr id="234b8b2c-1c23-8066-932d-c0eb29f43d73"><td id="zFby" class="" style="width:330px"><strong>BPF-based host routing</strong></td><td id="iudx" class="" style="width:202.8671875px"><strong>&gt;= 5.10</strong></td></tr><tr id="234b8b2c-1c23-80d6-8a53-ec2a95cc5caa"><td id="zFby" class="" style="width:330px"><a href="https://docs.cilium.io/en/stable/network/multicast/#enable-multicast">Multicast Support in Cilium (Beta)</a> (AMD64)</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.10</td></tr><tr id="234b8b2c-1c23-806c-bf16-e228d93fe9de"><td id="zFby" class="" style="width:330px">IPv6 BIG TCP support</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 5.19</td></tr><tr id="234b8b2c-1c23-8028-8eae-d0766f8ea4c8"><td id="zFby" class="" style="width:330px"><a href="https://docs.cilium.io/en/stable/network/multicast/#enable-multicast">Multicast Support in Cilium (Beta)</a> (AArch64)</td><td id="iudx" class="" style="width:202.8671875px">&gt;= 6.0</td></tr><tr id="234b8b2c-1c23-800f-a263-eadb96c7ccb1"><td id="zFby" class="" style="width:330px"><strong>IPv4 BIG TCP support</strong></td><td id="iudx" class="" style="width:202.8671875px"><strong>&gt;= 6.3</strong></td></tr></tbody></table><p id="234b8b2c-1c23-803b-84c6-f82616c25f99" class="">
</p></li></ul><ul id="234b8b2c-1c23-8021-aaed-cbe13beb0a3f" class="bulleted-list"><li style="list-style-type:disc">Cilium 동작(Node 간)을 위한 방화벽 규칙 : 해당 포트 인/아웃 허용 필요 - <a href="https://docs.cilium.io/en/stable/operations/system_requirements/#firewall-rules">Docs</a><p id="234b8b2c-1c23-801f-a95c-fdd664c27d76" class="">
</p></li></ul><ul id="234b8b2c-1c23-803b-954d-ce56758b08aa" class="bulleted-list"><li style="list-style-type:disc"><strong>Mounted eBPF filesystem</strong> : 일부 배포판 마운트되어 있음, 혹은 Cilium 설치 시 마운트 시도 - <a href="https://docs.cilium.io/en/stable/operations/system_requirements/#mounted-ebpf-filesystem">Docs</a><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-80ef-b368-c7fac855ad14" class="code"><code class="language-Bash">#
mount | grep /sys/fs/bpf
bpf on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)</code></pre><p id="234b8b2c-1c23-8002-9ca2-e4e5a9db2a07" class="">
</p></li></ul><ul id="234b8b2c-1c23-8083-88b8-e724725cb169" class="bulleted-list"><li style="list-style-type:disc">Privileges : Cilium 동작을 위해서 관리자 수준 권한 필요 - <a href="https://docs.cilium.io/en/stable/operations/system_requirements/#privileges">Docs</a><ul id="234b8b2c-1c23-8002-a609-ec1d34d02d80" class="bulleted-list"><li style="list-style-type:circle">Cilium interacts with the Linux kernel to install eBPF program which will then perform networking tasks and implement security rules. In order to install eBPF programs system-wide, <code>CAP_SYS_ADMIN</code> privileges are required. These privileges must be granted to <code>cilium-agent</code>.<p id="234b8b2c-1c23-80f1-a029-e5bcf9374c61" class="">The quickest way to meet the requirement is to run <code>cilium-agent</code> as root and/or as privileged container.</p></li></ul><ul id="234b8b2c-1c23-805f-a0a1-db65d7f85e37" class="bulleted-list"><li style="list-style-type:circle">Cilium requires access to the host networking namespace. For this purpose, the Cilium pod is scheduled to run in the host networking namespace directly.</li></ul></li></ul><p id="234b8b2c-1c23-8059-b9f2-f1f9427c1204" class="">
</p><p id="234b8b2c-1c23-8098-9d8e-e7c9dcd1ef45" class="">
</p></details></li></ul><ul id="234b8b2c-1c23-80dd-b995-c32fc7e23cce" class="bulleted-list"><li style="list-style-type:disc"><code>도전과제</code> Cilium 시스템 요구 사항을 점검하는 ‘Bash Script 나 Ansible Playbook’ 를 만들어서 적용해보기</li></ul><ul id="234b8b2c-1c23-8063-acbe-f68a3035b195" class="toggle"><li><details open=""><summary><strong>Cilium 설치 Without kube-proxy*</strong> - <a href="https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/">Docs</a></summary><ul id="234b8b2c-1c23-801c-b0b4-dff477450639" class="bulleted-list"><li style="list-style-type:disc">기존 Flannel CNI 제거<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-80a6-95fe-dc35187b5a53" class="code"><code class="language-Bash">#
helm uninstall -n kube-flannel flannel
helm list -A

#
kubectl get all -n kube-flannel
kubectl delete ns kube-flannel

#
kubectl get pod -A -owide
</code></pre><ul id="234b8b2c-1c23-8042-862a-dfafaf3b6617" class="bulleted-list"><li style="list-style-type:circle">k8s-ctr, k8s-w1, k8s-w2 모든 노드에서 아래 실행</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-808a-969f-f530db714918" class="code"><code class="language-Bash"># 제거 전 확인
ip -c link
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c link ; echo; done

brctl show
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i brctl show ; echo; done

ip -c route
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c route ; echo; done


# vnic 제거
ip link del flannel.1
ip link del cni0

for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo ip link del flannel.1 ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo ip link del cni0 ; echo; done

# 제거 확인
ip -c link
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c link ; echo; done

brctl show
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i brctl show ; echo; done

ip -c route
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c route ; echo; done
</code></pre><p id="234b8b2c-1c23-80d0-9285-c0ca5c711a05" class="">
</p></li></ul><ul id="234b8b2c-1c23-80df-b34e-d35a8f3c3c61" class="bulleted-list"><li style="list-style-type:disc">기존 kube-proxy 제거<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-80b8-a0ac-ce029ea21ff2" class="code"><code class="language-Bash">#
kubectl -n kube-system delete ds kube-proxy
kubectl -n kube-system delete cm kube-proxy

# 배포된 파드의 IP는 남겨져 있음
kubectl get pod -A -owide

# cni가 없으므로 pod간 통신이 불가능
kubectl exec -it curl-pod -- curl webpod

# 기존에 kube-proxy에 의해 주입되었던 iptables rule은 그대로 살아있음
iptables-save
</code></pre><ul id="234b8b2c-1c23-80a8-a99f-f41474392c5d" class="bulleted-list"><li style="list-style-type:circle">k8s-ctr, k8s-w1, k8s-w2 모든 노드에서 아래 실행</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8030-aa12-e93da0425ebb" class="code"><code class="language-Bash"># Run on each node with root permissions: -&gt; 기존에 kube-proxy에 의해 생성된 iptables rule 초기화
iptables-save | grep -v KUBE | grep -v FLANNEL | iptables-restore
iptables-save

sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-w1 &quot;sudo iptables-save | grep -v KUBE | grep -v FLANNEL | sudo iptables-restore&quot;
sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-w1 sudo iptables-save

sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-w2 &quot;sudo iptables-save | grep -v KUBE | grep -v FLANNEL | sudo iptables-restore&quot;
sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-w2 sudo iptables-save

#
kubectl get pod -owide</code></pre><p id="234b8b2c-1c23-8019-bdbd-e09b4eb94cad" class="">
</p></li></ul><ul id="234b8b2c-1c23-8099-aa6f-d879c2fef05c" class="bulleted-list"><li style="list-style-type:disc">노드별 파드에 할당되는 IPAM(PodCIDR) 정보 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8049-b120-cdbf7d1c59f5" class="code"><code class="language-Bash">#--allocate-node-cidrs=true 로 설정된 kube-controller-manager에서 CIDR을 자동 할당함
kubectl get nodes -o jsonpath=&#x27;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.spec.podCIDR}{&quot;\n&quot;}{end}&#x27;
k8s-ctr 10.244.0.0/24
k8s-w1  10.244.1.0/24
k8s-w2  10.244.2.0/24

kubectl get pod -owide

#
kc describe pod -n kube-system kube-controller-manager-k8s-ctr
...
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --cluster-cidr=10.244.0.0/16
      --service-cluster-ip-range=10.96.0.0/16
...</code></pre><p id="234b8b2c-1c23-80c5-8868-de09788d5fca" class="">
</p></li></ul><ul id="234b8b2c-1c23-80ca-8542-ea4b1c4b4101" class="bulleted-list"><li style="list-style-type:disc">Cilium 1.17.5 설치 with Helm - <a href="https://docs.cilium.io/en/stable/helm-reference/">Helm</a> , <a href="https://docs.cilium.io/en/stable/network/concepts/masquerading/">Masquering</a> , <a href="https://docs.cilium.io/en/stable/network/concepts/ipam/cluster-pool/">ClusterScope</a> , <a href="https://docs.cilium.io/en/stable/network/concepts/routing/">Routing</a><ul id="234b8b2c-1c23-8046-b7c9-db447ea9ffa4" class="toggle"><li><details open=""><summary>Cilium 1.17.5 Helm Chart - <a href="https://artifacthub.io/packages/helm/cilium/cilium/1.17.5">ArtifactHub</a></summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-804a-9cae-e9fb01a86e85" class="code"><code class="language-Bash"># File generated by install/kubernetes/Makefile; DO NOT EDIT.
# This file is based on install/kubernetes/cilium/*values.yaml.tmpl.


# @schema
# type: [null, string]
# @schema
# -- namespaceOverride allows to override the destination namespace for Cilium resources.
# This property allows to use Cilium as part of an Umbrella Chart with different targets.
namespaceOverride: &quot;&quot;
# @schema
# type: [null, object]
# @schema
# -- commonLabels allows users to add common labels for all Cilium resources.
commonLabels: {}
# @schema
# type: [null, string]
# @schema
# -- upgradeCompatibility helps users upgrading to ensure that the configMap for
# Cilium will not change critical values to ensure continued operation
# This flag is not required for new installations.
# For example: &#x27;1.7&#x27;, &#x27;1.8&#x27;, &#x27;1.9&#x27;
upgradeCompatibility: null
debug:
  # -- Enable debug logging
  enabled: false
  # @schema
  # type: [null, string]
  # @schema
  # -- Configure verbosity levels for debug logging
  # This option is used to enable debug messages for operations related to such
  # sub-system such as (e.g. kvstore, envoy, datapath or policy), and flow is
  # for enabling debug messages emitted per request, message and connection.
  # Multiple values can be set via a space-separated string (e.g. &quot;datapath envoy&quot;).
  #
  # Applicable values:
  # - flow
  # - kvstore
  # - envoy
  # - datapath
  # - policy
  verbose: ~
rbac:
  # -- Enable creation of Resource-Based Access Control configuration.
  create: true
# -- Configure image pull secrets for pulling container images
imagePullSecrets: []
# - name: &quot;image-pull-secret&quot;

# -- Configure iptables--random-fully. Disabled by default. View https://github.com/cilium/cilium/issues/13037 for more information.
iptablesRandomFully: false
# -- (string) Kubernetes config path
# @default -- `&quot;~/.kube/config&quot;`
kubeConfigPath: &quot;&quot;
# -- (string) Kubernetes service host - use &quot;auto&quot; for automatic lookup from the cluster-info ConfigMap
k8sServiceHost: &quot;&quot;
# @schema
# type: [string, integer]
# @schema
# -- (string) Kubernetes service port
k8sServicePort: &quot;&quot;
# @schema
# type: [null, string]
# @schema
# -- (string) When `k8sServiceHost=auto`, allows to customize the configMap name. It defaults to `cluster-info`.
k8sServiceLookupConfigMapName: &quot;&quot;
# @schema
# type: [null, string]
# @schema
# -- (string) When `k8sServiceHost=auto`, allows to customize the namespace that contains `k8sServiceLookupConfigMapName`. It defaults to `kube-public`.
k8sServiceLookupNamespace: &quot;&quot;
# -- Configure the client side rate limit for the agent
#
# If the amount of requests to the Kubernetes API server exceeds the configured
# rate limit, the agent will start to throttle requests by delaying
# them until there is budget or the request times out.
k8sClientRateLimit:
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) The sustained request rate in requests per second.
  # @default -- 10
  qps:
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) The burst request rate in requests per second.
  # The rate limiter will allow short bursts with a higher rate.
  # @default -- 20
  burst:
  # -- Configure the client side rate limit for the Cilium Operator
  operator:
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) The sustained request rate in requests per second.
    # @default -- 100
    qps:
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) The burst request rate in requests per second.
    # The rate limiter will allow short bursts with a higher rate.
    # @default -- 200
    burst:
cluster:
  # -- Name of the cluster. Only required for Cluster Mesh and mutual authentication with SPIRE.
  # It must respect the following constraints:
  # * It must contain at most 32 characters;
  # * It must begin and end with a lower case alphanumeric character;
  # * It may contain lower case alphanumeric characters and dashes between.
  # The &quot;default&quot; name cannot be used if the Cluster ID is different from 0.
  name: default
  # -- (int) Unique ID of the cluster. Must be unique across all connected
  # clusters and in the range of 1 to 255. Only required for Cluster Mesh,
  # may be 0 if Cluster Mesh is not used.
  id: 0
# -- Define serviceAccount names for components.
# @default -- Component&#x27;s fully qualified name.
serviceAccounts:
  cilium:
    create: true
    name: cilium
    automount: true
    annotations: {}
  nodeinit:
    create: true
    # -- Enabled is temporary until https://github.com/cilium/cilium-cli/issues/1396 is implemented.
    # Cilium CLI doesn&#x27;t create the SAs for node-init, thus the workaround. Helm is not affected by
    # this issue. Name and automount can be configured, if enabled is set to true.
    # Otherwise, they are ignored. Enabled can be removed once the issue is fixed.
    # Cilium-nodeinit DS must also be fixed.
    enabled: false
    name: cilium-nodeinit
    automount: true
    annotations: {}
  envoy:
    create: true
    name: cilium-envoy
    automount: true
    annotations: {}
  operator:
    create: true
    name: cilium-operator
    automount: true
    annotations: {}
  preflight:
    create: true
    name: cilium-pre-flight
    automount: true
    annotations: {}
  relay:
    create: true
    name: hubble-relay
    automount: false
    annotations: {}
  ui:
    create: true
    name: hubble-ui
    automount: true
    annotations: {}
  clustermeshApiserver:
    create: true
    name: clustermesh-apiserver
    automount: true
    annotations: {}
  # -- Clustermeshcertgen is used if clustermesh.apiserver.tls.auto.method=cronJob
  clustermeshcertgen:
    create: true
    name: clustermesh-apiserver-generate-certs
    automount: true
    annotations: {}
  # -- Hubblecertgen is used if hubble.tls.auto.method=cronJob
  hubblecertgen:
    create: true
    name: hubble-generate-certs
    automount: true
    annotations: {}
# -- Configure termination grace period for cilium-agent DaemonSet.
terminationGracePeriodSeconds: 1
# -- Install the cilium agent resources.
agent: true
# -- Agent container name.
name: cilium
# -- Roll out cilium agent pods automatically when configmap is updated.
rollOutCiliumPods: false
# -- Agent container image.
image:
  # @schema
  # type: [null, string]
  # @schema
  override: ~
  repository: &quot;quay.io/cilium/cilium&quot;
  tag: &quot;v1.17.5&quot;
  pullPolicy: &quot;IfNotPresent&quot;
  # cilium-digest
  digest: &quot;sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6&quot;
  useDigest: true
# -- Scheduling configurations for cilium pods
scheduling:
  # @schema
  # enum: [&quot;anti-affinity&quot;, &quot;kube-scheduler&quot;]
  # @schema
  # -- Mode specifies how Cilium daemonset pods should be scheduled to Nodes.
  # `anti-affinity` mode applies a pod anti-affinity rule to the cilium daemonset.
  # Pod anti-affinity may significantly impact scheduling throughput for large clusters.
  # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # `kube-scheduler` mode forgoes the anti-affinity rule for full scheduling throughput.
  # Kube-scheduler avoids host port conflict when scheduling pods.
  # @default -- Defaults to apply a pod anti-affinity rule to the agent pod - `anti-affinity`
  mode: anti-affinity
# -- Affinity for cilium-agent.
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            k8s-app: cilium
# -- Node selector for cilium-agent.
nodeSelector:
  kubernetes.io/os: linux
# -- Node tolerations for agent scheduling to nodes with taints
# ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
tolerations:
  - operator: Exists
    # - key: &quot;key&quot;
    #   operator: &quot;Equal|Exists&quot;
    #   value: &quot;value&quot;
    #   effect: &quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)&quot;
# -- The priority class to use for cilium-agent.
priorityClassName: &quot;&quot;
# -- DNS policy for Cilium agent pods.
# Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
dnsPolicy: &quot;&quot;
# -- Additional containers added to the cilium DaemonSet.
extraContainers: []
# -- Additional initContainers added to the cilium Daemonset.
extraInitContainers: []
# -- Additional agent container arguments.
extraArgs: []
# -- Additional agent container environment variables.
extraEnv: []
# -- Additional agent hostPath mounts.
extraHostPathMounts: []
# - name: host-mnt-data
#   mountPath: /host/mnt/data
#   hostPath: /mnt/data
#   hostPathType: Directory
#   readOnly: true
#   mountPropagation: HostToContainer

# -- Additional agent volumes.
extraVolumes: []
# -- Additional agent volumeMounts.
extraVolumeMounts: []
# -- extraConfig allows you to specify additional configuration parameters to be
# included in the cilium-config configmap.
extraConfig: {}
#  my-config-a: &quot;1234&quot;
#  my-config-b: |-
#    test 1
#    test 2
#    test 3

# -- Annotations to be added to all top-level cilium-agent objects (resources under templates/cilium-agent)
annotations: {}
# -- Security Context for cilium-agent pods.
podSecurityContext:
  # -- AppArmorProfile options for the `cilium-agent` and init containers
  appArmorProfile:
    type: &quot;Unconfined&quot;
# -- Annotations to be added to agent pods
podAnnotations: {}
# -- Labels to be added to agent pods
podLabels: {}
# -- Agent resource limits &amp; requests
# ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
resources: {}
#   limits:
#     cpu: 4000m
#     memory: 4Gi
#   requests:
#     cpu: 100m
#     memory: 512Mi

# -- resources &amp; limits for the agent init containers
initResources: {}
securityContext:
  # -- User to run the pod with
  # runAsUser: 0
  # -- Run the pod with elevated privileges
  privileged: false
  # -- SELinux options for the `cilium-agent` and init containers
  seLinuxOptions:
    level: &#x27;s0&#x27;
    # Running with spc_t since we have removed the privileged mode.
    # Users can change it to a different type as long as they have the
    # type available on the system.
    type: &#x27;spc_t&#x27;
  capabilities:
    # -- Capabilities for the `cilium-agent` container
    ciliumAgent:
      # Use to set socket permission
      - CHOWN
      # Used to terminate envoy child process
      - KILL
      # Used since cilium modifies routing tables, etc...
      - NET_ADMIN
      # Used since cilium creates raw sockets, etc...
      - NET_RAW
      # Used since cilium monitor uses mmap
      - IPC_LOCK
      # Used in iptables. Consider removing once we are iptables-free
      - SYS_MODULE
      # Needed to switch network namespaces (used for health endpoint, socket-LB).
      # We need it for now but might not need it for &gt;= 5.11 specially
      # for the &#x27;SYS_RESOURCE&#x27;.
      # In &gt;= 5.8 there&#x27;s already BPF and PERMON capabilities
      - SYS_ADMIN
      # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
      - SYS_RESOURCE
      # Both PERFMON and BPF requires kernel 5.8, container runtime
      # cri-o &gt;= v1.22.0 or containerd &gt;= v1.5.0.
      # If available, SYS_ADMIN can be removed.
      #- PERFMON
      #- BPF
      # Allow discretionary access control (e.g. required for package installation)
      - DAC_OVERRIDE
      # Allow to set Access Control Lists (ACLs) on arbitrary files (e.g. required for package installation)
      - FOWNER
      # Allow to execute program that changes GID (e.g. required for package installation)
      - SETGID
      # Allow to execute program that changes UID (e.g. required for package installation)
      - SETUID
    # -- Capabilities for the `mount-cgroup` init container
    mountCgroup:
      # Only used for &#x27;mount&#x27; cgroup
      - SYS_ADMIN
      # Used for nsenter
      - SYS_CHROOT
      - SYS_PTRACE
    # -- capabilities for the `apply-sysctl-overwrites` init container
    applySysctlOverwrites:
      # Required in order to access host&#x27;s /etc/sysctl.d dir
      - SYS_ADMIN
      # Used for nsenter
      - SYS_CHROOT
      - SYS_PTRACE
    # -- Capabilities for the `clean-cilium-state` init container
    cleanCiliumState:
      # Most of the capabilities here are the same ones used in the
      # cilium-agent&#x27;s container because this container can be used to
      # uninstall all Cilium resources, and therefore it is likely that
      # will need the same capabilities.
      # Used since cilium modifies routing tables, etc...
      - NET_ADMIN
      # Used in iptables. Consider removing once we are iptables-free
      - SYS_MODULE
      # We need it for now but might not need it for &gt;= 5.11 specially
      # for the &#x27;SYS_RESOURCE&#x27;.
      # In &gt;= 5.8 there&#x27;s already BPF and PERMON capabilities
      - SYS_ADMIN
      # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
      - SYS_RESOURCE
      # Both PERFMON and BPF requires kernel 5.8, container runtime
      # cri-o &gt;= v1.22.0 or containerd &gt;= v1.5.0.
      # If available, SYS_ADMIN can be removed.
      #- PERFMON
      #- BPF
# -- Cilium agent update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    # @schema
    # type: [integer, string]
    # @schema
    maxUnavailable: 2
# Configuration Values for cilium-agent
aksbyocni:
  # -- Enable AKS BYOCNI integration.
  # Note that this is incompatible with AKS clusters not created in BYOCNI mode:
  # use Azure integration (`azure.enabled`) instead.
  enabled: false
# @schema
# type: [boolean, string]
# @schema
# -- Enable installation of PodCIDR routes between worker
# nodes if worker nodes share a common L2 network segment.
autoDirectNodeRoutes: false
# -- Enable skipping of PodCIDR routes between worker
# nodes if the worker nodes are in a different L2 network segment.
directRoutingSkipUnreachable: false
# -- Annotate k8s node upon initialization with Cilium&#x27;s metadata.
annotateK8sNode: false
azure:
  # -- Enable Azure integration.
  # Note that this is incompatible with AKS clusters created in BYOCNI mode: use
  # AKS BYOCNI integration (`aksbyocni.enabled`) instead.
  enabled: false
  # usePrimaryAddress: false
  # resourceGroup: group1
  # subscriptionID: 00000000-0000-0000-0000-000000000000
  # tenantID: 00000000-0000-0000-0000-000000000000
  # clientID: 00000000-0000-0000-0000-000000000000
  # clientSecret: 00000000-0000-0000-0000-000000000000
  # userAssignedIdentityID: 00000000-0000-0000-0000-000000000000
alibabacloud:
  # -- Enable AlibabaCloud ENI integration
  enabled: false
# -- Enable bandwidth manager to optimize TCP and UDP workloads and allow
# for rate-limiting traffic from individual Pods with EDT (Earliest Departure
# Time) through the &quot;kubernetes.io/egress-bandwidth&quot; Pod annotation.
bandwidthManager:
  # -- Enable bandwidth manager infrastructure (also prerequirement for BBR)
  enabled: false
  # -- Activate BBR TCP congestion control for Pods
  bbr: false
# -- Configure standalone NAT46/NAT64 gateway
nat46x64Gateway:
  # -- Enable RFC8215-prefixed translation
  enabled: false
# -- EnableHighScaleIPcache enables the special ipcache mode for high scale
# clusters. The ipcache content will be reduced to the strict minimum and
# traffic will be encapsulated to carry security identities.
highScaleIPcache:
  # -- Enable the high scale mode for the ipcache.
  enabled: false
# -- Configure L2 announcements
l2announcements:
  # -- Enable L2 announcements
  enabled: false
  # -- If a lease is not renewed for X duration, the current leader is considered dead, a new leader is picked
  # leaseDuration: 15s
  # -- The interval at which the leader will renew the lease
  # leaseRenewDeadline: 5s
  # -- The timeout between retries if renewal fails
  # leaseRetryPeriod: 2s
# -- Configure L2 pod announcements
l2podAnnouncements:
  # -- Enable L2 pod announcements
  enabled: false
  # -- Interface used for sending Gratuitous ARP pod announcements
  interface: &quot;eth0&quot;
# -- This feature set enables virtual BGP routers to be created via
# CiliumBGPPeeringPolicy CRDs.
bgpControlPlane:
  # -- Enables the BGP control plane.
  enabled: false
  # -- SecretsNamespace is the namespace which BGP support will retrieve secrets from.
  secretsNamespace:
    # -- Create secrets namespace for BGP secrets.
    create: false
    # -- The name of the secret namespace to which Cilium agents are given read access
    name: kube-system
  # -- Status reporting settings (BGPv2 only)
  statusReport:
    # -- Enable/Disable BGPv2 status reporting
    # It is recommended to enable status reporting in general, but if you have any issue
    # such as high API server load, you can disable it by setting this to false.
    enabled: true
pmtuDiscovery:
  # -- Enable path MTU discovery to send ICMP fragmentation-needed replies to
  # the client.
  enabled: false
bpf:
  autoMount:
    # -- Enable automatic mount of BPF filesystem
    # When `autoMount` is enabled, the BPF filesystem is mounted at
    # `bpf.root` path on the underlying host and inside the cilium agent pod.
    # If users disable `autoMount`, it&#x27;s expected that users have mounted
    # bpffs filesystem at the specified `bpf.root` volume, and then the
    # volume will be mounted inside the cilium agent pod at the same path.
    enabled: true
  # -- Configure the mount point for the BPF filesystem
  root: /sys/fs/bpf
  # -- Enables pre-allocation of eBPF map values. This increases
  # memory usage but can reduce latency.
  preallocateMaps: false
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) Configure the maximum number of entries in auth map.
  # @default -- `524288`
  authMapMax: ~
  # -- Enable CT accounting for packets and bytes
  ctAccounting: false
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) Configure the maximum number of entries in the TCP connection tracking
  # table.
  # @default -- `524288`
  ctTcpMax: ~
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) Configure the maximum number of entries for the non-TCP connection
  # tracking table.
  # @default -- `262144`
  ctAnyMax: ~
  # -- Control to use a distributed per-CPU backend memory for the core BPF LRU maps
  # which Cilium uses. This improves performance significantly, but it is also
  # recommended to increase BPF map sizing along with that.
  distributedLRU:
    # -- Enable distributed LRU backend memory. For compatibility with existing
    # installations it is off by default.
    enabled: false
  # -- Control events generated by the Cilium datapath exposed to Cilium monitor and Hubble.
  # Helm configuration for BPF events map rate limiting is experimental and might change
  # in upcoming releases.
  events:
    # -- Default settings for all types of events except dbg and pcap.
    default:
      # -- (int) Configure the limit of messages per second that can be written to
      # BPF events map. The number of messages is averaged, meaning that if no messages
      # were written to the map over 5 seconds, it&#x27;s possible to write more events
      # in the 6th second. If rateLimit is greater than 0, non-zero value for burstLimit must
      # also be provided lest the configuration is considered invalid. Setting both burstLimit
      # and rateLimit to 0 disables BPF events rate limiting.
      # @default -- `0`
      rateLimit: ~
      # -- (int) Configure the maximum number of messages that can be written to BPF events
      # map in 1 second. If burstLimit is greater than 0, non-zero value for rateLimit must
      # also be provided lest the configuration is considered invalid. Setting both burstLimit
      # and rateLimit to 0 disables BPF events rate limiting.
      # @default -- `0`
      burstLimit: ~
    drop:
      # -- Enable drop events.
      enabled: true
    policyVerdict:
      # -- Enable policy verdict events.
      enabled: true
    trace:
      # -- Enable trace events.
      enabled: true
  # @schema
  # type: [null, integer]
  # @schema
  # -- Configure the maximum number of service entries in the
  # load balancer maps.
  lbMapMax: 65536
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) Configure the maximum number of entries for the NAT table.
  # @default -- `524288`
  natMax: ~
  # @schema
  # type: [null, integer]
  # @schema
  # -- (int) Configure the maximum number of entries for the neighbor table.
  # @default -- `524288`
  neighMax: ~
  # @schema
  # type: [null, integer]
  # @schema
  # @default -- `16384`
  # -- (int) Configures the maximum number of entries for the node table.
  nodeMapMax: ~
  # -- Configure the maximum number of entries in endpoint policy map (per endpoint).
  # @schema
  # type: [null, integer]
  # @schema
  policyMapMax: 16384
  # @schema
  # type: [null, number, string]
  # @schema
  # -- (float64) Configure auto-sizing for all BPF maps based on available memory.
  # ref: https://docs.cilium.io/en/stable/network/ebpf/maps/
  # @default -- `0.0025`
  mapDynamicSizeRatio: ~
  # -- Configure the level of aggregation for monitor notifications.
  # Valid options are none, low, medium, maximum.
  monitorAggregation: medium
  # -- Configure the typical time between monitor notifications for
  # active connections.
  monitorInterval: &quot;5s&quot;
  # -- Configure which TCP flags trigger notifications when seen for the
  # first time in a connection.
  monitorFlags: &quot;all&quot;
  # -- (bool) Allow cluster external access to ClusterIP services.
  # @default -- `false`
  lbExternalClusterIP: false
  # -- (bool) Enable loadBalancerSourceRanges CIDR filtering for all service
  # types, not just LoadBalancer services. The corresponding NodePort and
  # ClusterIP (if enabled for cluster-external traffic) will also apply the
  # CIDR filter.
  # @default -- `false`
  lbSourceRangeAllTypes: false
  # -- (bool) Enable the option to define the load balancing algorithm on
  # a per-service basis through service.cilium.io/lb-algorithm annotation.
  # @default -- `false`
  lbAlgorithmAnnotation: false
  # -- (bool) Enable the option to define the load balancing mode (SNAT or DSR)
  # on a per-service basis through service.cilium.io/forwarding-mode annotation.
  # @default -- `false`
  lbModeAnnotation: false
  # @schema
  # type: [null, boolean]
  # @schema
  # -- (bool) Enable native IP masquerade support in eBPF
  # @default -- `false`
  masquerade: ~
  # @schema
  # type: [null, boolean]
  # @schema
  # -- (bool) Configure whether direct routing mode should route traffic via
  # host stack (true) or directly and more efficiently out of BPF (false) if
  # the kernel supports it. The latter has the implication that it will also
  # bypass netfilter in the host namespace.
  # @default -- `false`
  hostLegacyRouting: ~
  # @schema
  # type: [null, boolean]
  # @schema
  # -- (bool) Configure the eBPF-based TPROXY (beta) to reduce reliance on iptables rules
  # for implementing Layer 7 policy.
  # @default -- `false`
  tproxy: ~
  # @schema
  # type: [null, array]
  # @schema
  # -- (list) Configure explicitly allowed VLAN id&#x27;s for bpf logic bypass.
  # [0] will allow all VLAN id&#x27;s without any filtering.
  # @default -- `[]`
  vlanBypass: ~
  # -- (bool) Disable ExternalIP mitigation (CVE-2020-8554)
  # @default -- `false`
  disableExternalIPMitigation: false
  # -- (bool) Attach endpoint programs using tcx instead of legacy tc hooks on
  # supported kernels.
  # @default -- `true`
  enableTCX: true
  # -- (string) Mode for Pod devices for the core datapath (veth, netkit, netkit-l2, lb-only)
  # @default -- `veth`
  datapathMode: veth
# -- Enable BPF clock source probing for more efficient tick retrieval.
bpfClockProbe: false
# -- Clean all eBPF datapath state from the initContainer of the cilium-agent
# DaemonSet.
#
# WARNING: Use with care!
cleanBpfState: false
# -- Clean all local Cilium state from the initContainer of the cilium-agent
# DaemonSet. Implies cleanBpfState: true.
#
# WARNING: Use with care!
cleanState: false
# -- Wait for KUBE-PROXY-CANARY iptables rule to appear in &quot;wait-for-kube-proxy&quot;
# init container before launching cilium-agent.
# More context can be found in the commit message of below PR
# https://github.com/cilium/cilium/pull/20123
waitForKubeProxy: false
cni:
  # -- Install the CNI configuration and binary files into the filesystem.
  install: true
  # -- Remove the CNI configuration and binary files on agent shutdown. Enable this
  # if you&#x27;re removing Cilium from the cluster. Disable this to prevent the CNI
  # configuration file from being removed during agent upgrade, which can cause
  # nodes to go unmanageable.
  uninstall: false
  # @schema
  # type: [null, string]
  # @schema
  # -- Configure chaining on top of other CNI plugins. Possible values:
  #  - none
  #  - aws-cni
  #  - flannel
  #  - generic-veth
  #  - portmap
  chainingMode: ~
  # @schema
  # type: [null, string]
  # @schema
  # -- A CNI network name in to which the Cilium plugin should be added as a chained plugin.
  # This will cause the agent to watch for a CNI network with this network name. When it is
  # found, this will be used as the basis for Cilium&#x27;s CNI configuration file. If this is
  # set, it assumes a chaining mode of generic-veth. As a special case, a chaining mode
  # of aws-cni implies a chainingTarget of aws-cni.
  chainingTarget: ~
  # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the
  # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.
  # This ensures no Pods can be scheduled using other CNI plugins during Cilium
  # agent downtime.
  exclusive: true
  # -- Configure the log file for CNI logging with retention policy of 7 days.
  # Disable CNI file logging by setting this field to empty explicitly.
  logFile: /var/run/cilium/cilium-cni.log
  # -- Skip writing of the CNI configuration. This can be used if
  # writing of the CNI configuration is performed by external automation.
  customConf: false
  # -- Configure the path to the CNI configuration directory on the host.
  confPath: /etc/cni/net.d
  # -- Configure the path to the CNI binary directory on the host.
  binPath: /opt/cni/bin
  # -- Specify the path to a CNI config to read from on agent start.
  # This can be useful if you want to manage your CNI
  # configuration outside of a Kubernetes environment. This parameter is
  # mutually exclusive with the &#x27;cni.configMap&#x27; parameter. The agent will
  # write this to 05-cilium.conflist on startup.
  # readCniConf: /host/etc/cni/net.d/05-sample.conflist.input

  # -- When defined, configMap will mount the provided value as ConfigMap and
  # interpret the cniConf variable as CNI configuration file and write it
  # when the agent starts up
  # configMap: cni-configuration

  # -- Configure the key in the CNI ConfigMap to read the contents of
  # the CNI configuration from.
  configMapKey: cni-config
  # -- Configure the path to where to mount the ConfigMap inside the agent pod.
  confFileMountPath: /tmp/cni-configuration
  # -- Configure the path to where the CNI configuration directory is mounted
  # inside the agent pod.
  hostConfDirMountPath: /host/etc/cni/net.d
  # -- Specifies the resources for the cni initContainer
  resources:
    requests:
      cpu: 100m
      memory: 10Mi
  # -- Enable route MTU for pod netns when CNI chaining is used
  enableRouteMTUForCNIChaining: false
# -- (string) Configure how frequently garbage collection should occur for the datapath
# connection tracking table.
# @default -- `&quot;0s&quot;`
conntrackGCInterval: &quot;&quot;
# -- (string) Configure the maximum frequency for the garbage collection of the
# connection tracking table. Only affects the automatic computation for the frequency
# and has no effect when &#x27;conntrackGCInterval&#x27; is set. This can be set to more frequently
# clean up unused identities created from ToFQDN policies.
conntrackGCMaxInterval: &quot;&quot;
# -- (string) Configure timeout in which Cilium will exit if CRDs are not available
# @default -- `&quot;5m&quot;`
crdWaitTimeout: &quot;&quot;
# -- Tail call hooks for custom eBPF programs.
customCalls:
  # -- Enable tail call hooks for custom eBPF programs.
  enabled: false
daemon:
  # -- Configure where Cilium runtime state should be stored.
  runPath: &quot;/var/run/cilium&quot;
  # @schema
  # type: [null, string]
  # @schema
  # -- Configure a custom list of possible configuration override sources
  # The default is &quot;config-map:cilium-config,cilium-node-config&quot;. For supported
  # values, see the help text for the build-config subcommand.
  # Note that this value should be a comma-separated string.
  configSources: ~
  # @schema
  # type: [null, string]
  # @schema
  # -- allowedConfigOverrides is a list of config-map keys that can be overridden.
  # That is to say, if this value is set, config sources (excepting the first one) can
  # only override keys in this list.
  #
  # This takes precedence over blockedConfigOverrides.
  #
  # By default, all keys may be overridden. To disable overrides, set this to &quot;none&quot; or
  # change the configSources variable.
  allowedConfigOverrides: ~
  # @schema
  # type: [null, string]
  # @schema
  # -- blockedConfigOverrides is a list of config-map keys that may not be overridden.
  # In other words, if any of these keys appear in a configuration source excepting the
  # first one, they will be ignored
  #
  # This is ignored if allowedConfigOverrides is set.
  #
  # By default, all keys may be overridden.
  blockedConfigOverrides: ~
  # @schema
  # type: [null, boolean]
  # @schema
  # -- enableSourceIPVerification is a boolean flag to enable or disable the Source IP verification
  # of endpoints. This flag is useful when Cilium is chained with other CNIs.
  #
  # By default, this functionality is enabled
  enableSourceIPVerification: true
# -- Specify which network interfaces can run the eBPF datapath. This means
# that a packet sent from a pod to a destination outside the cluster will be
# masqueraded (to an output device IPv4 address), if the output device runs the
# program. When not specified, probing will automatically detect devices that have
# a non-local route. This should be used only when autodetection is not suitable.
# devices: &quot;&quot;

# -- Enables experimental support for the detection of new and removed datapath
# devices. When devices change the eBPF datapath is reloaded and services updated.
# If &quot;devices&quot; is set then only those devices, or devices matching a wildcard will
# be considered.
#
# This option has been deprecated and is a no-op.
enableRuntimeDeviceDetection: true
# -- Forces the auto-detection of devices, even if specific devices are explicitly listed
forceDeviceDetection: false
# -- Chains to ignore when installing feeder rules.
# disableIptablesFeederRules: &quot;&quot;

# -- Limit iptables-based egress masquerading to interface selector.
# egressMasqueradeInterfaces: &quot;&quot;

# -- Enable setting identity mark for local traffic.
# enableIdentityMark: true

# -- Enable Kubernetes EndpointSlice feature in Cilium if the cluster supports it.
# enableK8sEndpointSlice: true

# -- Enable CiliumEndpointSlice feature (deprecated, please use `ciliumEndpointSlice.enabled` instead).
enableCiliumEndpointSlice: false
ciliumEndpointSlice:
  # -- Enable Cilium EndpointSlice feature.
  enabled: false
  # -- List of rate limit options to be used for the CiliumEndpointSlice controller.
  # Each object in the list must have the following fields:
  # nodes: Count of nodes at which to apply the rate limit.
  # limit: The sustained request rate in requests per second. The maximum rate that can be configured is 50.
  # burst: The burst request rate in requests per second. The maximum burst that can be configured is 100.
  rateLimits:
    - nodes: 0
      limit: 10
      burst: 20
    - nodes: 100
      limit: 50
      burst: 100
  # @schema
  # enum: [&quot;identity&quot;, &quot;fcfs&quot;]
  # @schema
  # -- The slicing mode to use for CiliumEndpointSlices.
  # identity groups together CiliumEndpoints that share the same identity.
  # fcfs groups together CiliumEndpoints in a first-come-first-serve basis, filling in the largest non-full slice first.
  sliceMode: identity
envoyConfig:
  # -- Enable CiliumEnvoyConfig CRD
  # CiliumEnvoyConfig CRD can also be implicitly enabled by other options.
  enabled: false
  # -- SecretsNamespace is the namespace in which envoy SDS will retrieve secrets from.
  secretsNamespace:
    # -- Create secrets namespace for CiliumEnvoyConfig CRDs.
    create: true
    # -- The name of the secret namespace to which Cilium agents are given read access.
    name: cilium-secrets
  # -- Interval in which an attempt is made to reconcile failed EnvoyConfigs. If the duration is zero, the retry is deactivated.
  retryInterval: 15s
ingressController:
  # -- Enable cilium ingress controller
  # This will automatically set enable-envoy-config as well.
  enabled: false
  # -- Set cilium ingress controller to be the default ingress controller
  # This will let cilium ingress controller route entries without ingress class set
  default: false
  # -- Default ingress load balancer mode
  # Supported values: shared, dedicated
  # For granular control, use the following annotations on the ingress resource:
  # &quot;ingress.cilium.io/loadbalancer-mode: dedicated&quot; (or &quot;shared&quot;).
  loadbalancerMode: dedicated
  # -- Enforce https for host having matching TLS host in Ingress.
  # Incoming traffic to http listener will return 308 http error code with respective location in header.
  enforceHttps: true
  # -- Enable proxy protocol for all Ingress listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
  enableProxyProtocol: false
  # -- IngressLBAnnotations are the annotation and label prefixes, which are used to filter annotations and/or labels to propagate from Ingress to the Load Balancer service
  ingressLBAnnotationPrefixes: [&#x27;lbipam.cilium.io&#x27;, &#x27;nodeipam.cilium.io&#x27;, &#x27;service.beta.kubernetes.io&#x27;, &#x27;service.kubernetes.io&#x27;, &#x27;cloud.google.com&#x27;]
  # @schema
  # type: [null, string]
  # @schema
  # -- Default secret namespace for ingresses without .spec.tls[].secretName set.
  defaultSecretNamespace:
  # @schema
  # type: [null, string]
  # @schema
  # -- Default secret name for ingresses without .spec.tls[].secretName set.
  defaultSecretName:
  # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
  secretsNamespace:
    # -- Create secrets namespace for Ingress.
    create: true
    # -- Name of Ingress secret namespace.
    name: cilium-secrets
    # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
    # If disabled, TLS secrets must be maintained externally.
    sync: true
  # -- Load-balancer service in shared mode.
  # This is a single load-balancer service for all Ingress resources.
  service:
    # -- Service name
    name: cilium-ingress
    # -- Labels to be added for the shared LB service
    labels: {}
    # -- Annotations to be added for the shared LB service
    annotations: {}
    # -- Service type for the shared LB service
    type: LoadBalancer
    # @schema
    # type: [null, integer]
    # @schema
    # -- Configure a specific nodePort for insecure HTTP traffic on the shared LB service
    insecureNodePort: ~
    # @schema
    # type: [null, integer]
    # @schema
    # -- Configure a specific nodePort for secure HTTPS traffic on the shared LB service
    secureNodePort: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- Configure a specific loadBalancerClass on the shared LB service (requires Kubernetes 1.24+)
    loadBalancerClass: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- Configure a specific loadBalancerIP on the shared LB service
    loadBalancerIP: ~
    # @schema
    # type: [null, boolean]
    # @schema
    # -- Configure if node port allocation is required for LB service
    # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
    allocateLoadBalancerNodePorts: ~
    # -- Control how traffic from external sources is routed to the LoadBalancer Kubernetes Service for Cilium Ingress in shared mode.
    # Valid values are &quot;Cluster&quot; and &quot;Local&quot;.
    # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#external-traffic-policy
    externalTrafficPolicy: Cluster
  # Host Network related configuration
  hostNetwork:
    # -- Configure whether the Envoy listeners should be exposed on the host network.
    enabled: false
    # -- Configure a specific port on the host network that gets used for the shared listener.
    sharedListenerPort: 8080
    # Specify the nodes where the Ingress listeners should be exposed
    nodes:
      # -- Specify the labels of the nodes where the Ingress listeners should be exposed
      #
      # matchLabels:
      #   kubernetes.io/os: linux
      #   kubernetes.io/hostname: kind-worker
      matchLabels: {}
gatewayAPI:
  # -- Enable support for Gateway API in cilium
  # This will automatically set enable-envoy-config as well.
  enabled: false
  # -- Enable proxy protocol for all GatewayAPI listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
  enableProxyProtocol: false
  # -- Enable Backend Protocol selection support (GEP-1911) for Gateway API via appProtocol.
  enableAppProtocol: false
  # -- Enable ALPN for all listeners configured with Gateway API. ALPN will attempt HTTP/2, then HTTP 1.1.
  # Note that this will also enable `appProtocol` support, and services that wish to use HTTP/2 will need to indicate that via their `appProtocol`.
  enableAlpn: false
  # -- The number of additional GatewayAPI proxy hops from the right side of the HTTP header to trust when determining the origin client&#x27;s IP address.
  xffNumTrustedHops: 0
  # -- Control how traffic from external sources is routed to the LoadBalancer Kubernetes Service for all Cilium GatewayAPI Gateway instances. Valid values are &quot;Cluster&quot; and &quot;Local&quot;.
  # Note that this value will be ignored when `hostNetwork.enabled == true`.
  # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#external-traffic-policy
  externalTrafficPolicy: Cluster
  gatewayClass:
    # -- Enable creation of GatewayClass resource
    # The default value is &#x27;auto&#x27; which decides according to presence of gateway.networking.k8s.io/v1/GatewayClass in the cluster.
    # Other possible values are &#x27;true&#x27; and &#x27;false&#x27;, which will either always or never create the GatewayClass, respectively.
    create: auto
  # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
  secretsNamespace:
    # -- Create secrets namespace for Gateway API.
    create: true
    # -- Name of Gateway API secret namespace.
    name: cilium-secrets
    # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
    # If disabled, TLS secrets must be maintained externally.
    sync: true
  # Host Network related configuration
  hostNetwork:
    # -- Configure whether the Envoy listeners should be exposed on the host network.
    enabled: false
    # Specify the nodes where the Ingress listeners should be exposed
    nodes:
      # -- Specify the labels of the nodes where the Ingress listeners should be exposed
      #
      # matchLabels:
      #   kubernetes.io/os: linux
      #   kubernetes.io/hostname: kind-worker
      matchLabels: {}
# -- Enables the fallback compatibility solution for when the xt_socket kernel
# module is missing and it is needed for the datapath L7 redirection to work
# properly. See documentation for details on when this can be disabled:
# https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel.
enableXTSocketFallback: true
encryption:
  # -- Enable transparent network encryption.
  enabled: false
  # -- Encryption method. Can be either ipsec or wireguard.
  type: ipsec
  # -- Enable encryption for pure node to node traffic.
  # This option is only effective when encryption.type is set to &quot;wireguard&quot;.
  nodeEncryption: false
  # -- Configure the WireGuard Pod2Pod strict mode.
  strictMode:
    # -- Enable WireGuard Pod2Pod strict mode.
    enabled: false
    # -- CIDR for the WireGuard Pod2Pod strict mode.
    cidr: &quot;&quot;
    # -- Allow dynamic lookup of remote node identities.
    # This is required when tunneling is used or direct routing is used and the node CIDR and pod CIDR overlap.
    allowRemoteNodeIdentities: false
  ipsec:
    # -- Name of the key file inside the Kubernetes secret configured via secretName.
    keyFile: keys
    # -- Path to mount the secret inside the Cilium pod.
    mountPath: /etc/ipsec
    # -- Name of the Kubernetes secret containing the encryption keys.
    secretName: cilium-ipsec-keys
    # -- The interface to use for encrypted traffic.
    interface: &quot;&quot;
    # -- Enable the key watcher. If disabled, a restart of the agent will be
    # necessary on key rotations.
    keyWatcher: true
    # -- Maximum duration of the IPsec key rotation. The previous key will be
    # removed after that delay.
    keyRotationDuration: &quot;5m&quot;
    # -- Enable IPsec encrypted overlay
    encryptedOverlay: false
  wireguard:
    # -- Controls WireGuard PersistentKeepalive option. Set 0s to disable.
    persistentKeepalive: 0s
endpointHealthChecking:
  # -- Enable connectivity health checking between virtual endpoints.
  enabled: true
endpointRoutes:
  # @schema
  # type: [boolean, string]
  # @schema
  # -- Enable use of per endpoint routes instead of routing via
  # the cilium_host interface.
  enabled: false
k8sNetworkPolicy:
  # -- Enable support for K8s NetworkPolicy
  enabled: true
# -- Enable endpoint lockdown on policy map overflow.
endpointLockdownOnMapOverflow: false
eni:
  # -- Enable Elastic Network Interface (ENI) integration.
  enabled: false
  # -- Update ENI Adapter limits from the EC2 API
  updateEC2AdapterLimitViaAPI: true
  # -- Release IPs not used from the ENI
  awsReleaseExcessIPs: false
  # -- Enable ENI prefix delegation
  awsEnablePrefixDelegation: false
  # -- EC2 API endpoint to use
  ec2APIEndpoint: &quot;&quot;
  # -- Tags to apply to the newly created ENIs
  eniTags: {}
  # -- Interval for garbage collection of unattached ENIs. Set to &quot;0s&quot; to disable.
  # @default -- `&quot;5m&quot;`
  gcInterval: &quot;&quot;
  # -- Additional tags attached to ENIs created by Cilium.
  # Dangling ENIs with this tag will be garbage collected
  # @default -- `{&quot;io.cilium/cilium-managed&quot;:&quot;true,&quot;io.cilium/cluster-name&quot;:&quot;&lt;auto-detected&gt;&quot;}`
  gcTags: {}
  # -- If using IAM role for Service Accounts will not try to
  # inject identity values from cilium-aws kubernetes secret.
  # Adds annotation to service account if managed by Helm.
  # See https://github.com/aws/amazon-eks-pod-identity-webhook
  iamRole: &quot;&quot;
  # -- Filter via subnet IDs which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetIDsFilter: []
  # -- Filter via tags (k=v) which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetTagsFilter: []
  # -- Filter via AWS EC2 Instance tags (k=v) which will dictate which AWS EC2 Instances
  # are going to be used to create new ENIs
  instanceTagsFilter: []
externalIPs:
  # -- Enable ExternalIPs service support.
  enabled: false
# fragmentTracking enables IPv4 fragment tracking support in the datapath.
# fragmentTracking: true
gke:
  # -- Enable Google Kubernetes Engine integration
  enabled: false
# -- Enable connectivity health checking.
healthChecking: true
# -- TCP port for the agent health API. This is not the port for cilium-health.
healthPort: 9879
# -- Number of ICMP requests sent for each health check before marking a node or endpoint unreachable.
healthCheckICMPFailureThreshold: 3
# -- Configure the host firewall.
hostFirewall:
  # -- Enables the enforcement of host policies in the eBPF datapath.
  enabled: false
hostPort:
  # -- Enable hostPort service support.
  enabled: false
# -- Configure socket LB
socketLB:
  # -- Enable socket LB
  enabled: false
  # -- Disable socket lb for non-root ns. This is used to enable Istio routing rules.
  # hostNamespaceOnly: false
  # -- Enable terminating pod connections to deleted service backends.
  # terminatePodConnections: true
  # -- Enables tracing for socket-based load balancing.
  # tracing: true
# -- Configure certificate generation for Hubble integration.
# If hubble.tls.auto.method=cronJob, these values are used
# for the Kubernetes CronJob which will be scheduled regularly to
# (re)generate any certificates not provided manually.
certgen:
  # -- When set to true the certificate authority secret is created.
  generateCA: true
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: &quot;quay.io/cilium/certgen&quot;
    tag: &quot;v0.2.1&quot;
    digest: &quot;sha256:ab6b1928e9c5f424f6b0f51c68065b9fd85e2f8d3e5f21fbd1a3cb27e6fb9321&quot;
    useDigest: true
    pullPolicy: &quot;IfNotPresent&quot;
  # -- Seconds after which the completed job pod will be deleted
  ttlSecondsAfterFinished: 1800
  # -- Labels to be added to hubble-certgen pods
  podLabels: {}
  # -- Annotations to be added to the hubble-certgen initial Job and CronJob
  annotations:
    job: {}
    cronJob: {}
  # -- Node selector for certgen
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector: {}
  # -- Priority class for certgen
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
  priorityClassName: &quot;&quot;
  # -- Node tolerations for pod assignment on nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  # -- Additional certgen volumes.
  extraVolumes: []
  # -- Additional certgen volumeMounts.
  extraVolumeMounts: []
  # -- Affinity for certgen
  affinity: {}
hubble:
  # -- Enable Hubble (true by default).
  enabled: true
  # -- Annotations to be added to all top-level hubble objects (resources under templates/hubble)
  annotations: {}
  # -- Buffer size of the channel Hubble uses to receive monitor events. If this
  # value is not set, the queue size is set to the default monitor queue size.
  # eventQueueSize: &quot;&quot;

  # -- Number of recent flows for Hubble to cache. Defaults to 4095.
  # Possible values are:
  #   1, 3, 7, 15, 31, 63, 127, 255, 511, 1023,
  #   2047, 4095, 8191, 16383, 32767, 65535
  # eventBufferCapacity: &quot;4095&quot;

  # -- Hubble metrics configuration.
  # See https://docs.cilium.io/en/stable/observability/metrics/#hubble-metrics
  # for more comprehensive documentation about Hubble metrics.
  metrics:
    # @schema
    # type: [null, array]
    # @schema
    # -- Configures the list of metrics to collect. If empty or null, metrics
    # are disabled.
    # Example:
    #
    #   enabled:
    #   - dns:query;ignoreAAAA
    #   - drop
    #   - tcp
    #   - flow
    #   - icmp
    #   - http
    #
    # You can specify the list of metrics from the helm CLI:
    #
    #   --set hubble.metrics.enabled=&quot;{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}&quot;
    #
    enabled: ~
    # -- Enables exporting hubble metrics in OpenMetrics format.
    enableOpenMetrics: false
    # -- Configure the port the hubble metric server listens on.
    port: 9965
    tls:
      # Enable hubble metrics server TLS.
      enabled: false
      # Configure hubble metrics server TLS.
      server:
        # -- Name of the Secret containing the certificate and key for the Hubble metrics server.
        # If specified, cert and key are ignored.
        existingSecret: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble metrics server certificate (deprecated).
        # Use existingSecret instead.
        cert: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble metrics server key (deprecated).
        # Use existingSecret instead.
        key: &quot;&quot;
        # -- Extra DNS names added to certificate when it&#x27;s auto generated
        extraDnsNames: []
        # -- Extra IP addresses added to certificate when it&#x27;s auto generated
        extraIpAddresses: []
        # -- Configure mTLS for the Hubble metrics server.
        mtls:
          # When set to true enforces mutual TLS between Hubble Metrics server and its clients.
          # False allow non-mutual TLS connections.
          # This option has no effect when TLS is disabled.
          enabled: false
          useSecret: false
          # -- Name of the ConfigMap containing the CA to validate client certificates against.
          # If mTLS is enabled and this is unspecified, it will default to the
          # same CA used for Hubble metrics server certificates.
          name: ~
          # -- Entry of the ConfigMap containing the CA.
          key: ca.crt
    # -- Annotations to be added to hubble-metrics service.
    serviceAnnotations: {}
    serviceMonitor:
      # -- Create ServiceMonitor resources for Prometheus Operator.
      # This requires the prometheus CRDs to be available.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: false
      # -- Labels to add to ServiceMonitor hubble
      labels: {}
      # -- Annotations to add to ServiceMonitor hubble
      annotations: {}
      # -- jobLabel to add for ServiceMonitor hubble
      jobLabel: &quot;&quot;
      # -- Interval for scrape metrics.
      interval: &quot;10s&quot;
      # -- Relabeling configs for the ServiceMonitor hubble
      relabelings:
        - sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: node
          replacement: ${1}
      # @schema
      # type: [null, array]
      # @schema
      # -- Metrics relabeling configs for the ServiceMonitor hubble
      metricRelabelings: ~
      # Configure TLS for the ServiceMonitor.
      # Note, when using TLS you will either need to specify
      # tlsConfig.insecureSkipVerify or specify a CA to use.
      tlsConfig: {}
    # -- Grafana dashboards for hubble
    # grafana can import dashboards based on the label and value
    # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
    dashboards:
      enabled: false
      label: grafana_dashboard
      # @schema
      # type: [null, string]
      # @schema
      namespace: ~
      labelValue: &quot;1&quot;
      annotations: {}
    # Dynamic metrics  may be reconfigured without a need of agent restarts.
    dynamic:
      enabled: false
      config:
        # ---- Name of configmap with configuration that may be altered to reconfigure metric handlers within a running agent.
        configMapName: cilium-dynamic-metrics-config
        # ---- True if helm installer should create config map.
        # Switch to false if you want to self maintain the file content.
        createConfigMap: true
        # ---- Exporters configuration in YAML format.
        content: []
        # - name: dns
        #   contextOptions: []
        #   includeFilters: []
        #   excludeFilters: []
  # -- Unix domain socket path to listen to when Hubble is enabled.
  socketPath: /var/run/cilium/hubble.sock
  # -- Enables redacting sensitive information present in Layer 7 flows.
  redact:
    enabled: false
    http:
      # -- Enables redacting URL query (GET) parameters.
      # Example:
      #
      #   redact:
      #     enabled: true
      #     http:
      #       urlQuery: true
      #
      # You can specify the options from the helm CLI:
      #
      #   --set hubble.redact.enabled=&quot;true&quot;
      #   --set hubble.redact.http.urlQuery=&quot;true&quot;
      urlQuery: false
      # -- Enables redacting user info, e.g., password when basic auth is used.
      # Example:
      #
      #   redact:
      #     enabled: true
      #     http:
      #       userInfo: true
      #
      # You can specify the options from the helm CLI:
      #
      #   --set hubble.redact.enabled=&quot;true&quot;
      #   --set hubble.redact.http.userInfo=&quot;true&quot;
      userInfo: true
      headers:
        # -- List of HTTP headers to allow: headers not matching will be redacted. Note: `allow` and `deny` lists cannot be used both at the same time, only one can be present.
        # Example:
        #   redact:
        #     enabled: true
        #     http:
        #       headers:
        #         allow:
        #           - traceparent
        #           - tracestate
        #           - Cache-Control
        #
        # You can specify the options from the helm CLI:
        #   --set hubble.redact.enabled=&quot;true&quot;
        #   --set hubble.redact.http.headers.allow=&quot;traceparent,tracestate,Cache-Control&quot;
        allow: []
        # -- List of HTTP headers to deny: matching headers will be redacted. Note: `allow` and `deny` lists cannot be used both at the same time, only one can be present.
        # Example:
        #   redact:
        #     enabled: true
        #     http:
        #       headers:
        #         deny:
        #           - Authorization
        #           - Proxy-Authorization
        #
        # You can specify the options from the helm CLI:
        #   --set hubble.redact.enabled=&quot;true&quot;
        #   --set hubble.redact.http.headers.deny=&quot;Authorization,Proxy-Authorization&quot;
        deny: []
    kafka:
      # -- Enables redacting Kafka&#x27;s API key.
      # Example:
      #
      #   redact:
      #     enabled: true
      #     kafka:
      #       apiKey: true
      #
      # You can specify the options from the helm CLI:
      #
      #   --set hubble.redact.enabled=&quot;true&quot;
      #   --set hubble.redact.kafka.apiKey=&quot;true&quot;
      apiKey: true
  # -- An additional address for Hubble to listen to.
  # Set this field &quot;:4244&quot; if you are enabling Hubble Relay, as it assumes that
  # Hubble is listening on port 4244.
  listenAddress: &quot;:4244&quot;
  # -- Whether Hubble should prefer to announce IPv6 or IPv4 addresses if both are available.
  preferIpv6: false
  # @schema
  # type: [null, boolean]
  # @schema
  # -- (bool) Skip Hubble events with unknown cgroup ids
  # @default -- `true`
  skipUnknownCGroupIDs: ~
  peerService:
    # -- Service Port for the Peer service.
    # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
    # port 80 if not.
    # servicePort: 80
    # -- Target Port for the Peer service, must match the hubble.listenAddress&#x27;
    # port.
    targetPort: 4244
    # -- The cluster domain to use to query the Hubble Peer service. It should
    # be the local cluster.
    clusterDomain: cluster.local
  # -- TLS configuration for Hubble
  tls:
    # -- Enable mutual TLS for listenAddress. Setting this value to false is
    # highly discouraged as the Hubble API provides access to potentially
    # sensitive network flow metadata and is exposed on the host network.
    enabled: true
    # -- Configure automatic TLS certificates generation.
    auto:
      # -- Auto-generate certificates.
      # When set to true, automatically generate a CA and certificates to
      # enable mTLS between Hubble server and Hubble Relay instances. If set to
      # false, the certs for Hubble server need to be provided by setting
      # appropriate values below.
      enabled: true
      # -- Set the method to auto-generate certificates. Supported values:
      # - helm:         This method uses Helm to generate all certificates.
      # - cronJob:      This method uses a Kubernetes CronJob the generate any
      #                 certificates not provided by the user at installation
      #                 time.
      # - certmanager:  This method use cert-manager to generate &amp; rotate certificates.
      method: helm
      # -- Generated certificates validity duration in days.
      #
      # Defaults to 365 days (1 year) because MacOS does not accept
      # self-signed certificates with expirations &gt; 825 days.
      certValidityDuration: 365
      # -- Schedule for certificates regeneration (regardless of their expiration date).
      # Only used if method is &quot;cronJob&quot;. If nil, then no recurring job will be created.
      # Instead, only the one-shot job is deployed to generate the certificates at
      # installation time.
      #
      # Defaults to midnight of the first day of every fourth month. For syntax, see
      # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
      schedule: &quot;0 0 1 */4 *&quot;
      # [Example]
      # certManagerIssuerRef:
      #   group: cert-manager.io
      #   kind: ClusterIssuer
      #   name: ca-issuer
      # -- certmanager issuer used when hubble.tls.auto.method=certmanager.
      certManagerIssuerRef: {}
    # -- The Hubble server certificate and private key
    server:
      # -- Name of the Secret containing the certificate and key for the Hubble server.
      # If specified, cert and key are ignored.
      existingSecret: &quot;&quot;
      # -- base64 encoded PEM values for the Hubble server certificate (deprecated).
      # Use existingSecret instead.
      cert: &quot;&quot;
      # -- base64 encoded PEM values for the Hubble server key (deprecated).
      # Use existingSecret instead.
      key: &quot;&quot;
      # -- Extra DNS names added to certificate when it&#x27;s auto generated
      extraDnsNames: []
      # -- Extra IP addresses added to certificate when it&#x27;s auto generated
      extraIpAddresses: []
  relay:
    # -- Enable Hubble Relay (requires hubble.enabled=true)
    enabled: false
    # -- Roll out Hubble Relay pods automatically when configmap is updated.
    rollOutPods: false
    # -- Hubble-relay container image.
    image:
      # @schema
      # type: [null, string]
      # @schema
      override: ~
      repository: &quot;quay.io/cilium/hubble-relay&quot;
      tag: &quot;v1.17.5&quot;
      # hubble-relay-digest
      digest: &quot;sha256:fbb8a6afa8718200fca9381ad274ed695792dbadd2417b0e99c36210ae4964ff&quot;
      useDigest: true
      pullPolicy: &quot;IfNotPresent&quot;
    # -- Specifies the resources for the hubble-relay pods
    resources: {}
    # -- Number of replicas run for the hubble-relay deployment.
    replicas: 1
    # -- Affinity for hubble-replay
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                k8s-app: cilium
    # -- Pod topology spread constraints for hubble-relay
    topologySpreadConstraints: []
    #   - maxSkew: 1
    #     topologyKey: topology.kubernetes.io/zone
    #     whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []
    # -- Additional hubble-relay environment variables.
    extraEnv: []
    # -- Annotations to be added to all top-level hubble-relay objects (resources under templates/hubble-relay)
    annotations: {}
    # -- Annotations to be added to hubble-relay pods
    podAnnotations: {}
    # -- Labels to be added to hubble-relay pods
    podLabels: {}
    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it&#x27;s set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1
    # -- The priority class to use for hubble-relay
    priorityClassName: &quot;&quot;
    # -- Configure termination grace period for hubble relay Deployment.
    terminationGracePeriodSeconds: 1
    # -- hubble-relay update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        # @schema
        # type: [integer, string]
        # @schema
        maxUnavailable: 1
    # -- Additional hubble-relay volumes.
    extraVolumes: []
    # -- Additional hubble-relay volumeMounts.
    extraVolumeMounts: []
    # -- hubble-relay pod security context
    podSecurityContext:
      fsGroup: 65532
    # -- hubble-relay container security context
    securityContext:
      # readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 65532
      runAsGroup: 65532
      capabilities:
        drop:
          - ALL
    # -- hubble-relay service configuration.
    service:
      # --- The type of service used for Hubble Relay access, either ClusterIP, NodePort or LoadBalancer.
      type: ClusterIP
      # --- The port to use when the service type is set to NodePort.
      nodePort: 31234
    # -- Host to listen to. Specify an empty string to bind to all the interfaces.
    listenHost: &quot;&quot;
    # -- Port to listen to.
    listenPort: &quot;4245&quot;
    # -- TLS configuration for Hubble Relay
    tls:
      # -- The hubble-relay client certificate and private key.
      # This keypair is presented to Hubble server instances for mTLS
      # authentication and is required when hubble.tls.enabled is true.
      # These values need to be set manually if hubble.tls.auto.enabled is false.
      client:
        # -- Name of the Secret containing the certificate and key for the Hubble metrics server.
        # If specified, cert and key are ignored.
        existingSecret: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble relay client certificate (deprecated).
        # Use existingSecret instead.
        cert: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble relay client key (deprecated).
        # Use existingSecret instead.
        key: &quot;&quot;
      # -- The hubble-relay server certificate and private key
      server:
        # When set to true, enable TLS on for Hubble Relay server
        # (ie: for clients connecting to the Hubble Relay API).
        enabled: false
        # When set to true enforces mutual TLS between Hubble Relay server and its clients.
        # False allow non-mutual TLS connections.
        # This option has no effect when TLS is disabled.
        mtls: false
        # -- Name of the Secret containing the certificate and key for the Hubble relay server.
        # If specified, cert and key are ignored.
        existingSecret: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble relay server certificate (deprecated).
        # Use existingSecret instead.
        cert: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble relay server key (deprecated).
        # Use existingSecret instead.
        key: &quot;&quot;
        # -- extra DNS names added to certificate when its auto gen
        extraDnsNames: []
        # -- extra IP addresses added to certificate when its auto gen
        extraIpAddresses: []
        # DNS name used by the backend to connect to the relay
        # This is a simple workaround as the relay certificates are currently hardcoded to
        # *.hubble-relay.cilium.io
        # See https://github.com/cilium/cilium/pull/28709#discussion_r1371792546
        # For GKE Dataplane V2 this should be set to relay.kube-system.svc.cluster.local
        relayName: &quot;ui.hubble-relay.cilium.io&quot;
    # @schema
    # type: [null, string]
    # @schema
    # -- Dial timeout to connect to the local hubble instance to receive peer information (e.g. &quot;30s&quot;).
    #
    # This option has been deprecated and is a no-op.
    dialTimeout: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- Backoff duration to retry connecting to the local hubble instance in case of failure (e.g. &quot;30s&quot;).
    retryTimeout: ~
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) Max number of flows that can be buffered for sorting before being sent to the
    # client (per request) (e.g. 100).
    sortBufferLenMax: ~
    # @schema
    # type: [null, string]
    # @schema
    # -- When the per-request flows sort buffer is not full, a flow is drained every
    # time this timeout is reached (only affects requests in follow-mode) (e.g. &quot;1s&quot;).
    sortBufferDrainTimeout: ~
    # -- Port to use for the k8s service backed by hubble-relay pods.
    # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
    # port 80 if not.
    # servicePort: 80

    # -- Enable prometheus metrics for hubble-relay on the configured port at
    # /metrics
    prometheus:
      enabled: false
      port: 9966
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor hubble-relay
        labels: {}
        # -- Annotations to add to ServiceMonitor hubble-relay
        annotations: {}
        # -- Interval for scrape metrics.
        interval: &quot;10s&quot;
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: &quot;&quot;
        # @schema
        # type: [null, array]
        # @schema
        # -- Relabeling configs for the ServiceMonitor hubble-relay
        relabelings: ~
        # @schema
        # type: [null, array]
        # @schema
        # -- Metrics relabeling configs for the ServiceMonitor hubble-relay
        metricRelabelings: ~
    gops:
      # -- Enable gops for hubble-relay
      enabled: true
      # -- Configure gops listen port for hubble-relay
      port: 9893
    pprof:
      # -- Enable pprof for hubble-relay
      enabled: false
      # -- Configure pprof listen address for hubble-relay
      address: localhost
      # -- Configure pprof listen port for hubble-relay
      port: 6062
  ui:
    # -- Whether to enable the Hubble UI.
    enabled: false
    standalone:
      # -- When true, it will allow installing the Hubble UI only, without checking dependencies.
      # It is useful if a cluster already has cilium and Hubble relay installed and you just
      # want Hubble UI to be deployed.
      # When installed via helm, installing UI should be done via `helm upgrade` and when installed via the cilium cli, then `cilium hubble enable --ui`
      enabled: false
      tls:
        # -- When deploying Hubble UI in standalone, with tls enabled for Hubble relay, it is required
        # to provide a volume for mounting the client certificates.
        certsVolume: {}
        #   projected:
        #     defaultMode: 0400
        #     sources:
        #     - secret:
        #         name: hubble-ui-client-certs
        #         items:
        #         - key: tls.crt
        #           path: client.crt
        #         - key: tls.key
        #           path: client.key
        #         - key: ca.crt
        #           path: hubble-relay-ca.crt
    # -- Roll out Hubble-ui pods automatically when configmap is updated.
    rollOutPods: false
    tls:
      client:
        # -- Name of the Secret containing the client certificate and key for Hubble UI
        # If specified, cert and key are ignored.
        existingSecret: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble UI client certificate (deprecated).
        # Use existingSecret instead.
        cert: &quot;&quot;
        # -- base64 encoded PEM values for the Hubble UI client key (deprecated).
        # Use existingSecret instead.
        key: &quot;&quot;
    backend:
      # -- Hubble-ui backend image.
      image:
        # @schema
        # type: [null, string]
        # @schema
        override: ~
        repository: &quot;quay.io/cilium/hubble-ui-backend&quot;
        tag: &quot;v0.13.2&quot;
        digest: &quot;sha256:a034b7e98e6ea796ed26df8f4e71f83fc16465a19d166eff67a03b822c0bfa15&quot;
        useDigest: true
        pullPolicy: &quot;IfNotPresent&quot;
      # -- Hubble-ui backend security context.
      securityContext: {}
      # -- Additional hubble-ui backend environment variables.
      extraEnv: []
      # -- Additional hubble-ui backend volumes.
      extraVolumes: []
      # -- Additional hubble-ui backend volumeMounts.
      extraVolumeMounts: []
      livenessProbe:
        # -- Enable liveness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
        enabled: false
      readinessProbe:
        # -- Enable readiness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
        enabled: false
      # -- Resource requests and limits for the &#x27;backend&#x27; container of the &#x27;hubble-ui&#x27; deployment.
      resources: {}
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi
    frontend:
      # -- Hubble-ui frontend image.
      image:
        # @schema
        # type: [null, string]
        # @schema
        override: ~
        repository: &quot;quay.io/cilium/hubble-ui&quot;
        tag: &quot;v0.13.2&quot;
        digest: &quot;sha256:9e37c1296b802830834cc87342a9182ccbb71ffebb711971e849221bd9d59392&quot;
        useDigest: true
        pullPolicy: &quot;IfNotPresent&quot;
      # -- Hubble-ui frontend security context.
      securityContext: {}
      # -- Additional hubble-ui frontend environment variables.
      extraEnv: []
      # -- Additional hubble-ui frontend volumes.
      extraVolumes: []
      # -- Additional hubble-ui frontend volumeMounts.
      extraVolumeMounts: []
      # -- Resource requests and limits for the &#x27;frontend&#x27; container of the &#x27;hubble-ui&#x27; deployment.
      resources: {}
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi
      server:
        # -- Controls server listener for ipv6
        ipv6:
          enabled: true
    # -- The number of replicas of Hubble UI to deploy.
    replicas: 1
    # -- Annotations to be added to all top-level hubble-ui objects (resources under templates/hubble-ui)
    annotations: {}
    # -- Additional labels to be added to &#x27;hubble-ui&#x27; deployment object
    labels: {}
    # -- Annotations to be added to hubble-ui pods
    podAnnotations: {}
    # -- Labels to be added to hubble-ui pods
    podLabels: {}
    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it&#x27;s set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1
    # -- Affinity for hubble-ui
    affinity: {}
    # -- Pod topology spread constraints for hubble-ui
    topologySpreadConstraints: []
    #   - maxSkew: 1
    #     topologyKey: topology.kubernetes.io/zone
    #     whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []
    # -- The priority class to use for hubble-ui
    priorityClassName: &quot;&quot;
    # -- hubble-ui update strategy.
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        # @schema
        # type: [integer, string]
        # @schema
        maxUnavailable: 1
    # -- Security context to be added to Hubble UI pods
    securityContext:
      runAsUser: 1001
      runAsGroup: 1001
      fsGroup: 1001
    # -- hubble-ui service configuration.
    service:
      # -- Annotations to be added for the Hubble UI service
      annotations: {}
      # --- The type of service used for Hubble UI access, either ClusterIP or NodePort.
      type: ClusterIP
      # --- The port to use when the service type is set to NodePort.
      nodePort: 31235
    # -- Defines base url prefix for all hubble-ui http requests.
    # It needs to be changed in case if ingress for hubble-ui is configured under some sub-path.
    # Trailing `/` is required for custom path, ex. `/service-map/`
    baseUrl: &quot;/&quot;
    # -- hubble-ui ingress configuration.
    ingress:
      enabled: false
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: &quot;true&quot;
      className: &quot;&quot;
      hosts:
        - chart-example.local
      labels: {}
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local
  # -- Hubble flows export.
  export:
    # --- Defines max file size of output file before it gets rotated.
    fileMaxSizeMb: 10
    # --- Defines max number of backup/rotated files.
    fileMaxBackups: 5
    # --- Static exporter configuration.
    # Static exporter is bound to agent lifecycle.
    static:
      enabled: false
      filePath: /var/run/cilium/hubble/events.log
      fieldMask: []
      # - time
      # - source
      # - destination
      # - verdict
      allowList: []
      # - &#x27;{&quot;verdict&quot;:[&quot;DROPPED&quot;,&quot;ERROR&quot;]}&#x27;
      denyList: []
      # - &#x27;{&quot;source_pod&quot;:[&quot;kube-system/&quot;]}&#x27;
      # - &#x27;{&quot;destination_pod&quot;:[&quot;kube-system/&quot;]}&#x27;
    # --- Dynamic exporters configuration.
    # Dynamic exporters may be reconfigured without a need of agent restarts.
    dynamic:
      enabled: false
      config:
        # ---- Name of configmap with configuration that may be altered to reconfigure exporters within a running agents.
        configMapName: cilium-flowlog-config
        # ---- True if helm installer should create config map.
        # Switch to false if you want to self maintain the file content.
        createConfigMap: true
        # ---- Exporters configuration in YAML format.
        content:
          - name: all
            fieldMask: []
            includeFilters: []
            excludeFilters: []
            filePath: &quot;/var/run/cilium/hubble/events.log&quot;
            #   - name: &quot;test002&quot;
            #     filePath: &quot;/var/log/network/flow-log/pa/test002.log&quot;
            #     fieldMask: [&quot;source.namespace&quot;, &quot;source.pod_name&quot;, &quot;destination.namespace&quot;, &quot;destination.pod_name&quot;, &quot;verdict&quot;]
            #     includeFilters:
            #     - source_pod: [&quot;default/&quot;]
            #       event_type:
            #       - type: 1
            #     - destination_pod: [&quot;frontend/nginx-975996d4c-7hhgt&quot;]
            #     excludeFilters: []
            #     end: &quot;2023-10-09T23:59:59-07:00&quot;
  # -- Emit v1.Events related to pods on detection of packet drops.
  #    This feature is alpha, please provide feedback at https://github.com/cilium/cilium/issues/33975.
  dropEventEmitter:
    enabled: false
    # --- Minimum time between emitting same events.
    interval: 2m
    # --- Drop reasons to emit events for.
    # ref: https://docs.cilium.io/en/stable/_api/v1/flow/README/#dropreason
    reasons:
      - auth_required
      - policy_denied
# -- Method to use for identity allocation (`crd`, `kvstore` or `doublewrite-readkvstore` / `doublewrite-readcrd` for migrating between identity backends).
identityAllocationMode: &quot;crd&quot;
# -- (string) Time to wait before using new identity on endpoint identity change.
# @default -- `&quot;5s&quot;`
identityChangeGracePeriod: &quot;&quot;
# -- Install Iptables rules to skip netfilter connection tracking on all pod
# traffic. This option is only effective when Cilium is running in direct
# routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
# is running in a managed Kubernetes environment or in a chained CNI setup.
installNoConntrackIptablesRules: false
ipam:
  # -- Configure IP Address Management mode.
  # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
  mode: &quot;cluster-pool&quot;
  # -- Maximum rate at which the CiliumNode custom resource is updated.
  ciliumNodeUpdateRate: &quot;15s&quot;
  # -- Pre-allocation settings for IPAM in Multi-Pool mode
  multiPoolPreAllocation: &quot;&quot;
  # -- Install ingress/egress routes through uplink on host for Pods when working with delegated IPAM plugin.
  installUplinkRoutesForDelegatedIPAM: false
  operator:
    # @schema
    # type: [array, string]
    # @schema
    # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
    clusterPoolIPv4PodCIDRList: [&quot;10.0.0.0/8&quot;]
    # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
    clusterPoolIPv4MaskSize: 24
    # @schema
    # type: [array, string]
    # @schema
    # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
    clusterPoolIPv6PodCIDRList: [&quot;fd00::/104&quot;]
    # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
    clusterPoolIPv6MaskSize: 120
    # -- IP pools to auto-create in multi-pool IPAM mode.
    autoCreateCiliumPodIPPools: {}
    #   default:
    #     ipv4:
    #       cidrs:
    #         - 10.10.0.0/8
    #       maskSize: 24
    #   other:
    #     ipv6:
    #       cidrs:
    #         - fd00:100::/80
    #       maskSize: 96
    # @schema
    # type: [null, integer]
    # @schema
    # -- (int) The maximum burst size when rate limiting access to external APIs.
    # Also known as the token bucket capacity.
    # @default -- `20`
    externalAPILimitBurstSize: ~
    # @schema
    # type: [null, number]
    # @schema
    # -- (float) The maximum queries per second when rate limiting access to
    # external APIs. Also known as the bucket refill rate, which is used to
    # refill the bucket up to the burst size capacity.
    # @default -- `4.0`
    externalAPILimitQPS: ~
# -- defaultLBServiceIPAM indicates the default LoadBalancer Service IPAM when
# no LoadBalancer class is set. Applicable values: lbipam, nodeipam, none
# @schema
# type: [string]
# @schema
defaultLBServiceIPAM: lbipam
nodeIPAM:
  # -- Configure Node IPAM
  # ref: https://docs.cilium.io/en/stable/network/node-ipam/
  enabled: false
# @schema
# type: [null, string]
# @schema
# -- The api-rate-limit option can be used to overwrite individual settings of the default configuration for rate limiting calls to the Cilium Agent API
apiRateLimit: ~
# -- Configure the eBPF-based ip-masq-agent
ipMasqAgent:
  enabled: false
# the config of nonMasqueradeCIDRs
# config:
#   nonMasqueradeCIDRs: []
#   masqLinkLocal: false
#   masqLinkLocalIPv6: false

# iptablesLockTimeout defines the iptables &quot;--wait&quot; option when invoked from Cilium.
# iptablesLockTimeout: &quot;5s&quot;
ipv4:
  # -- Enable IPv4 support.
  enabled: true
ipv6:
  # -- Enable IPv6 support.
  enabled: false
# -- Configure Kubernetes specific configuration
k8s:
  # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
  # range via the Kubernetes node resource
  requireIPv4PodCIDR: false
  # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
  # range via the Kubernetes node resource
  requireIPv6PodCIDR: false
# -- Keep the deprecated selector labels when deploying Cilium DaemonSet.
keepDeprecatedLabels: false
# -- Keep the deprecated probes when deploying Cilium DaemonSet
keepDeprecatedProbes: false
startupProbe:
  # -- failure threshold of startup probe.
  # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
  failureThreshold: 105
  # -- interval between checks of the startup probe
  periodSeconds: 2
livenessProbe:
  # -- failure threshold of liveness probe
  failureThreshold: 10
  # -- interval between checks of the liveness probe
  periodSeconds: 30
  # -- whether to require k8s connectivity as part of the check.
  requireK8sConnectivity: false
readinessProbe:
  # -- failure threshold of readiness probe
  failureThreshold: 3
  # -- interval between checks of the readiness probe
  periodSeconds: 30
# -- Configure the kube-proxy replacement in Cilium BPF datapath
# Valid options are &quot;true&quot; or &quot;false&quot;.
# ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
#kubeProxyReplacement: &quot;false&quot;

# -- healthz server bind address for the kube-proxy replacement.
# To enable set the value to &#x27;0.0.0.0:10256&#x27; for all ipv4
# addresses and this &#x27;[::]:10256&#x27; for all ipv6 addresses.
# By default it is disabled.
kubeProxyReplacementHealthzBindAddr: &quot;&quot;
l2NeighDiscovery:
  # -- Enable L2 neighbor discovery in the agent
  enabled: true
  # -- Override the agent&#x27;s default neighbor resolution refresh period.
  refreshPeriod: &quot;30s&quot;
# -- Enable Layer 7 network policy.
l7Proxy: true
# -- Enable Local Redirect Policy.
localRedirectPolicy: false
# To include or exclude matched resources from cilium identity evaluation
# labels: &quot;&quot;

# logOptions allows you to define logging options. eg:
# logOptions:
#   format: json

# -- Enables periodic logging of system load
logSystemLoad: false
# -- Configure maglev consistent hashing
maglev: {}
# -- tableSize is the size (parameter M) for the backend table of one
# service entry
# tableSize:

# -- hashSeed is the cluster-wide base64 encoded seed for the hashing
# hashSeed:

# -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
enableIPv4Masquerade: true
# -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
enableIPv6Masquerade: true
# -- Enables masquerading to the source of the route for traffic leaving the node from endpoints.
enableMasqueradeRouteSource: false
# -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
enableIPv4BIGTCP: false
# -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
enableIPv6BIGTCP: false
nat:
  # -- Number of the top-k SNAT map connections to track in Cilium statedb.
  mapStatsEntries: 32
  # -- Interval between how often SNAT map is counted for stats.
  mapStatsInterval: 30s
egressGateway:
  # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
  # cluster.
  enabled: false
  # -- Time between triggers of egress gateway state reconciliations
  reconciliationTriggerInterval: 1s
  # -- Maximum number of entries in egress gateway policy map
  # maxPolicyEntries: 16384
vtep:
  # -- Enables VXLAN Tunnel Endpoint (VTEP) Integration (beta) to allow
  # Cilium-managed pods to talk to third party VTEP devices over Cilium tunnel.
  enabled: false
  # -- A space separated list of VTEP device endpoint IPs, for example &quot;1.1.1.1  1.1.2.1&quot;
  endpoint: &quot;&quot;
  # -- A space separated list of VTEP device CIDRs, for example &quot;1.1.1.0/24 1.1.2.0/24&quot;
  cidr: &quot;&quot;
  # -- VTEP CIDRs Mask that applies to all VTEP CIDRs, for example &quot;255.255.255.0&quot;
  mask: &quot;&quot;
  # -- A space separated list of VTEP device MAC addresses (VTEP MAC), for example &quot;x:x:x:x:x:x  y:y:y:y:y:y:y&quot;
  mac: &quot;&quot;
# -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv4NativeRoutingCIDR: &quot;&quot;
# -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv6NativeRoutingCIDR: &quot;&quot;
# -- cilium-monitor sidecar.
monitor:
  # -- Enable the cilium-monitor sidecar.
  enabled: false
# -- Configure service load balancing
loadBalancer:
  # -- standalone enables the standalone L4LB which does not connect to
  # kube-apiserver.
  # standalone: false

  # -- algorithm is the name of the load balancing algorithm for backend
  # selection e.g. random or maglev
  # algorithm: random

  # -- mode is the operation mode of load balancing for remote backends
  # e.g. snat, dsr, hybrid
  # mode: snat

  # -- acceleration is the option to accelerate service handling via XDP
  # Applicable values can be: disabled (do not use XDP), native (XDP BPF
  # program is run directly out of the networking driver&#x27;s early receive
  # path), or best-effort (use native mode XDP acceleration on devices
  # that support it).
  acceleration: disabled
  # -- dsrDispatch configures whether IP option or IPIP encapsulation is
  # used to pass a service IP and port to remote backend
  # dsrDispatch: opt

  # -- serviceTopology enables K8s Topology Aware Hints -based service
  # endpoints filtering
  # serviceTopology: false

  # -- experimental enables support for the experimental load-balancing
  # control-plane.
  experimental: false
  # -- L7 LoadBalancer
  l7:
    # -- Enable L7 service load balancing via envoy proxy.
    # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
    # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
    # Please refer to docs for supported annotations for more configuration.
    #
    # Applicable values:
    #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
    #   - disabled: Disable L7 load balancing by way of service annotation.
    backend: disabled
    # -- List of ports from service to be automatically redirected to above backend.
    # Any service exposing one of these ports will be automatically redirected.
    # Fine-grained control can be achieved by using the service annotation.
    ports: []
    # -- Default LB algorithm
    # The default LB algorithm to be used for services, which can be overridden by the
    # service annotation (e.g. service.cilium.io/lb-l7-algorithm)
    # Applicable values: round_robin, least_request, random
    algorithm: round_robin
# -- Configure N-S k8s service loadbalancing
nodePort:
  # -- Enable the Cilium NodePort service implementation.
  enabled: false
  # -- Port range to use for NodePort services.
  # range: &quot;30000,32767&quot;

  # @schema
  # type: [null, string, array]
  # @schema
  # -- List of CIDRs for choosing which IP addresses assigned to native devices are used for NodePort load-balancing.
  # By default this is empty and the first suitable, preferably private, IPv4 and IPv6 address assigned to each device is used.
  #
  # Example:
  #
  #   addresses: [&quot;192.168.1.0/24&quot;, &quot;2001::/64&quot;]
  #
  addresses: ~
  # -- Set to true to prevent applications binding to service ports.
  bindProtection: true
  # -- Append NodePort range to ip_local_reserved_ports if clash with ephemeral
  # ports is detected.
  autoProtectPortRange: true
  # -- Enable healthcheck nodePort server for NodePort services
  enableHealthCheck: true
  # -- Enable access of the healthcheck nodePort on the LoadBalancerIP. Needs
  # EnableHealthCheck to be enabled
  enableHealthCheckLoadBalancerIP: false
# policyAuditMode: false

# -- The agent can be put into one of the three policy enforcement modes:
# default, always and never.
# ref: https://docs.cilium.io/en/stable/security/policy/intro/#policy-enforcement-modes
policyEnforcementMode: &quot;default&quot;
# @schema
# type: [null, string, array]
# @schema
# -- policyCIDRMatchMode is a list of entities that may be selected by CIDR selector.
# The possible value is &quot;nodes&quot;.
policyCIDRMatchMode:
pprof:
  # -- Enable pprof for cilium-agent
  enabled: false
  # -- Configure pprof listen address for cilium-agent
  address: localhost
  # -- Configure pprof listen port for cilium-agent
  port: 6060
# -- Configure prometheus metrics on the configured port at /metrics
prometheus:
  metricsService: false
  enabled: false
  port: 9962
  serviceMonitor:
    # -- Enable service monitors.
    # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
    enabled: false
    # -- Labels to add to ServiceMonitor cilium-agent
    labels: {}
    # -- Annotations to add to ServiceMonitor cilium-agent
    annotations: {}
    # -- jobLabel to add for ServiceMonitor cilium-agent
    jobLabel: &quot;&quot;
    # -- Interval for scrape metrics.
    interval: &quot;10s&quot;
    # -- Specify the Kubernetes namespace where Prometheus expects to find
    # service monitors configured.
    # namespace: &quot;&quot;
    # -- Relabeling configs for the ServiceMonitor cilium-agent
    relabelings:
      - sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: node
        replacement: ${1}
    # @schema
    # type: [null, array]
    # @schema
    # -- Metrics relabeling configs for the ServiceMonitor cilium-agent
    metricRelabelings: ~
    # -- Set to `true` and helm will not check for monitoring.coreos.com/v1 CRDs before deploying
    trustCRDsExist: false
  # @schema
  # type: [null, array]
  # @schema
  # -- Metrics that should be enabled or disabled from the default metric list.
  # The list is expected to be separated by a space. (+metric_foo to enable
  # metric_foo , -metric_bar to disable metric_bar).
  # ref: https://docs.cilium.io/en/stable/observability/metrics/
  metrics: ~
  # --- Enable controller group metrics for monitoring specific Cilium
  # subsystems. The list is a list of controller group names. The special
  # values of &quot;all&quot; and &quot;none&quot; are supported. The set of controller
  # group names is not guaranteed to be stable between Cilium versions.
  controllerGroupMetrics:
    - write-cni-file
    - sync-host-ips
    - sync-lb-maps-with-k8s-services
# -- Grafana dashboards for cilium-agent
# grafana can import dashboards based on the label and value
# ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
dashboards:
  enabled: false
  label: grafana_dashboard
  # @schema
  # type: [null, string]
  # @schema
  namespace: ~
  labelValue: &quot;1&quot;
  annotations: {}
# Configure Cilium Envoy options.
envoy:
  # @schema
  # type: [null, boolean]
  # @schema
  # -- Enable Envoy Proxy in standalone DaemonSet.
  # This field is enabled by default for new installation.
  # @default -- `true` for new installation
  enabled: ~
  # -- (int)
  # Set Envoy&#x27;--base-id&#x27; to use when allocating shared memory regions.
  # Only needs to be changed if multiple Envoy instances will run on the same node and may have conflicts. Supported values: 0 - 4294967295. Defaults to &#x27;0&#x27;
  baseID: 0
  log:
    # @schema
    # type: [null, string]
    # @schema
    # -- The format string to use for laying out the log message metadata of Envoy. If specified, Envoy will use text format output.
    # This setting is mutually exclusive with envoy.log.format_json.
    format: &quot;[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v&quot;
    # @schema
    # type: [null, object]
    # @schema
    # -- The JSON logging format to use for Envoy. This setting is mutually exclusive with envoy.log.format.
    # ref: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/bootstrap/v3/bootstrap.proto#envoy-v3-api-field-config-bootstrap-v3-bootstrap-applicationlogconfig-logformat-json-format
    format_json: null
    # date: &quot;%Y-%m-%dT%T.%e&quot;
    # thread_id: &quot;%t&quot;
    # source_line: &quot;%s:%#&quot;
    # level: &quot;%l&quot;
    # logger: &quot;%n&quot;
    # message: &quot;%j&quot;
    # -- Path to a separate Envoy log file, if any. Defaults to /dev/stdout.
    path: &quot;&quot;
    # @schema
    # oneOf:
    # - type: [null]
    # - enum: [trace,debug,info,warning,error,critical,off]
    # @schema
    # -- Default log level of Envoy application log that is configured if Cilium debug / verbose logging isn&#x27;t enabled.
    # This option allows to have a different log level than the Cilium Agent - e.g. lower it to `critical`.
    # Possible values: trace, debug, info, warning, error, critical, off
    # @default -- Defaults to the default log level of the Cilium Agent - `info`
    defaultLevel: ~
    # @schema
    # type: [null, integer]
    # @schema
    # -- Size of the Envoy access log buffer created within the agent in bytes.
    # Tune this value up if you encounter &quot;Envoy: Discarded truncated access log message&quot; errors.
    # Large request/response header sizes (e.g. 16KiB) will require a larger buffer size.
    accessLogBufferSize: 4096
  # -- Time in seconds after which a TCP connection attempt times out
  connectTimeoutSeconds: 2
  # -- Time in seconds after which the initial fetch on an xDS stream is considered timed out
  initialFetchTimeoutSeconds: 30
  # -- Maximum number of concurrent retries on Envoy clusters
  maxConcurrentRetries: 128
  # -- Maximum number of retries for each HTTP request
  httpRetryCount: 3
  # -- ProxyMaxRequestsPerConnection specifies the max_requests_per_connection setting for Envoy
  maxRequestsPerConnection: 0
  # -- Set Envoy HTTP option max_connection_duration seconds. Default 0 (disable)
  maxConnectionDurationSeconds: 0
  # -- Set Envoy upstream HTTP idle connection timeout seconds.
  # Does not apply to connections with pending requests. Default 60s
  idleTimeoutDurationSeconds: 60
  # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the ingress L7 policy enforcement Envoy listeners.
  xffNumTrustedHopsL7PolicyIngress: 0
  # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the egress L7 policy enforcement Envoy listeners.
  xffNumTrustedHopsL7PolicyEgress: 0
  # @schema
  # type: [null, string]
  # @schema
  # -- Max duration to wait for endpoint policies to be restored on restart. Default &quot;3m&quot;.
  policyRestoreTimeoutDuration: null
  # -- Envoy container image.
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: &quot;quay.io/cilium/cilium-envoy&quot;
    tag: &quot;v1.32.6-1749271279-0864395884b263913eac200ee2048fd985f8e626&quot;
    pullPolicy: &quot;IfNotPresent&quot;
    digest: &quot;sha256:9f69e290a7ea3d4edf9192acd81694089af048ae0d8a67fb63bd62dc1d72203e&quot;
    useDigest: true
  # -- Additional containers added to the cilium Envoy DaemonSet.
  extraContainers: []
  # -- Additional envoy container arguments.
  extraArgs: []
  # -- Additional envoy container environment variables.
  extraEnv: []
  # -- Additional envoy hostPath mounts.
  extraHostPathMounts: []
  # - name: host-mnt-data
  #   mountPath: /host/mnt/data
  #   hostPath: /mnt/data
  #   hostPathType: Directory
  #   readOnly: true
  #   mountPropagation: HostToContainer

  # -- Additional envoy volumes.
  extraVolumes: []
  # -- Additional envoy volumeMounts.
  extraVolumeMounts: []
  # -- Configure termination grace period for cilium-envoy DaemonSet.
  terminationGracePeriodSeconds: 1
  # -- TCP port for the health API.
  healthPort: 9878
  # -- cilium-envoy update strategy
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      # @schema
      # type: [integer, string]
      # @schema
      maxUnavailable: 2
  # -- Roll out cilium envoy pods automatically when configmap is updated.
  rollOutPods: false
  # -- ADVANCED OPTION: Bring your own custom Envoy bootstrap ConfigMap. Provide the name of a ConfigMap with a `bootstrap-config.json` key.
  # When specified, Envoy will use this ConfigMap instead of the default provided by the chart.
  # WARNING: Use of this setting has the potential to prevent cilium-envoy from starting up, and can cause unexpected behavior (e.g. due to
  # syntax error or semantically incorrect configuration). Before submitting an issue, please ensure you have disabled this feature, as support
  # cannot be provided for custom Envoy bootstrap configs.
  # @schema
  # type: [null, string]
  # @schema
  bootstrapConfigMap: ~
  # -- Annotations to be added to all top-level cilium-envoy objects (resources under templates/cilium-envoy)
  annotations: {}
  # -- Security Context for cilium-envoy pods.
  podSecurityContext:
    # -- AppArmorProfile options for the `cilium-agent` and init containers
    appArmorProfile:
      type: &quot;Unconfined&quot;
  # -- Annotations to be added to envoy pods
  podAnnotations: {}
  # -- Labels to be added to envoy pods
  podLabels: {}
  # -- Envoy resource limits &amp; requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
  #   limits:
  #     cpu: 4000m
  #     memory: 4Gi
  #   requests:
  #     cpu: 100m
  #     memory: 512Mi

  startupProbe:
    # -- failure threshold of startup probe.
    # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
    failureThreshold: 105
    # -- interval between checks of the startup probe
    periodSeconds: 2
  livenessProbe:
    # -- failure threshold of liveness probe
    failureThreshold: 10
    # -- interval between checks of the liveness probe
    periodSeconds: 30
  readinessProbe:
    # -- failure threshold of readiness probe
    failureThreshold: 3
    # -- interval between checks of the readiness probe
    periodSeconds: 30
  securityContext:
    # -- User to run the pod with
    # runAsUser: 0
    # -- Run the pod with elevated privileges
    privileged: false
    # -- SELinux options for the `cilium-envoy` container
    seLinuxOptions:
      level: &#x27;s0&#x27;
      # Running with spc_t since we have removed the privileged mode.
      # Users can change it to a different type as long as they have the
      # type available on the system.
      type: &#x27;spc_t&#x27;
    capabilities:
      # -- Capabilities for the `cilium-envoy` container.
      # Even though granted to the container, the cilium-envoy-starter wrapper drops
      # all capabilities after forking the actual Envoy process.
      # `NET_BIND_SERVICE` is the only capability that can be passed to the Envoy process by
      # setting `envoy.securityContext.capabilities.keepNetBindService=true` (in addition to granting the
      # capability to the container).
      # Note: In case of embedded envoy, the capability must  be granted to the cilium-agent container.
      envoy:
        # Used since cilium proxy uses setting IPPROTO_IP/IP_TRANSPARENT
        - NET_ADMIN
        # We need it for now but might not need it for &gt;= 5.11 specially
        # for the &#x27;SYS_RESOURCE&#x27;.
        # In &gt;= 5.8 there&#x27;s already BPF and PERMON capabilities
        - SYS_ADMIN
        # Both PERFMON and BPF requires kernel 5.8, container runtime
        # cri-o &gt;= v1.22.0 or containerd &gt;= v1.5.0.
        # If available, SYS_ADMIN can be removed.
        #- PERFMON
        #- BPF
      # -- Keep capability `NET_BIND_SERVICE` for Envoy process.
      keepCapNetBindService: false
  # -- Affinity for cilium-envoy.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium-envoy
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: cilium.io/no-schedule
                operator: NotIn
                values:
                  - &quot;true&quot;
  # -- Node selector for cilium-envoy.
  nodeSelector:
    kubernetes.io/os: linux
  # -- Node tolerations for envoy scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
    - operator: Exists
      # - key: &quot;key&quot;
      #   operator: &quot;Equal|Exists&quot;
      #   value: &quot;value&quot;
      #   effect: &quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)&quot;
  # @schema
  # type: [null, string]
  # @schema
  # -- The priority class to use for cilium-envoy.
  priorityClassName: ~
  # @schema
  # type: [null, string]
  # @schema
  # -- DNS policy for Cilium envoy pods.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: ~
  debug:
    admin:
      # -- Enable admin interface for cilium-envoy.
      # This is useful for debugging and should not be enabled in production.
      enabled: false
      # -- Port number (bound to loopback interface).
      # kubectl port-forward can be used to access the admin interface.
      port: 9901
  # -- Configure Cilium Envoy Prometheus options.
  # Note that some of these apply to either cilium-agent or cilium-envoy.
  prometheus:
    # -- Enable prometheus metrics for cilium-envoy
    enabled: true
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      # Note that this setting applies to both cilium-envoy _and_ cilium-agent
      # with Envoy enabled.
      enabled: false
      # -- Labels to add to ServiceMonitor cilium-envoy
      labels: {}
      # -- Annotations to add to ServiceMonitor cilium-envoy
      annotations: {}
      # -- Interval for scrape metrics.
      interval: &quot;10s&quot;
      # -- Specify the Kubernetes namespace where Prometheus expects to find
      # service monitors configured.
      # namespace: &quot;&quot;
      # -- Relabeling configs for the ServiceMonitor cilium-envoy
      # or for cilium-agent with Envoy configured.
      relabelings:
        - sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: node
          replacement: ${1}
      # @schema
      # type: [null, array]
      # @schema
      # -- Metrics relabeling configs for the ServiceMonitor cilium-envoy
      # or for cilium-agent with Envoy configured.
      metricRelabelings: ~
    # -- Serve prometheus metrics for cilium-envoy on the configured port
    port: &quot;9964&quot;
# -- Enable/Disable use of node label based identity
nodeSelectorLabels: false
# -- Enable resource quotas for priority classes used in the cluster.
resourceQuotas:
  enabled: false
  cilium:
    hard:
      # 5k nodes * 2 DaemonSets (Cilium and cilium node init)
      pods: &quot;10k&quot;
  operator:
    hard:
      # 15 &quot;clusterwide&quot; Cilium Operator pods for HA
      pods: &quot;15&quot;
# Need to document default
##################
#sessionAffinity: false

# -- Do not run Cilium agent when running with clean mode. Useful to completely
# uninstall Cilium as it will stop Cilium from starting and create artifacts
# in the node.
sleepAfterInit: false
# -- Enable check of service source ranges (currently, only for LoadBalancer).
svcSourceRangeCheck: true
# -- Synchronize Kubernetes nodes to kvstore and perform CNP GC.
synchronizeK8sNodes: true
# -- Configure TLS configuration in the agent.
tls:
  # @schema
  # type: [null, string]
  # @schema
  # -- This configures how the Cilium agent loads the secrets used TLS-aware CiliumNetworkPolicies
  # (namely the secrets referenced by terminatingTLS and originatingTLS).
  # This value is DEPRECATED and will be removed in a future version.
  # Use `tls.readSecretsOnlyFromSecretsNamespace` instead.
  # Possible values:
  #   - local
  #   - k8s
  secretsBackend: ~
  # @schema
  # type: [null, boolean]
  # @schema
  # -- Configure if the Cilium Agent will only look in `tls.secretsNamespace` for
  #    CiliumNetworkPolicy relevant Secrets.
  #    If false, the Cilium Agent will be granted READ (GET/LIST/WATCH) access
  #    to _all_ secrets in the entire cluster. This is not recommended and is
  #    included for backwards compatibility.
  #    This value obsoletes `tls.secretsBackend`, with `true` == `local` in the old
  #    setting, and `false` == `k8s`.
  readSecretsOnlyFromSecretsNamespace: ~
  # -- Configures where secrets used in CiliumNetworkPolicies will be looked for
  secretsNamespace:
    # -- Create secrets namespace for TLS Interception secrets.
    create: true
    # -- Name of TLS Interception secret namespace.
    name: cilium-secrets
  # -- Configures settings for synchronization of TLS Interception Secrets
  secretSync:
    # @schema
    # type: [null, boolean]
    # @schema
    # -- Enable synchronization of Secrets for TLS Interception. If disabled and
    # tls.readSecretsOnlyFromSecretsNamespace is set to &#x27;false&#x27;, then secrets will be read directly by the agent.
    enabled: ~
  # -- Base64 encoded PEM values for the CA certificate and private key.
  # This can be used as common CA to generate certificates used by hubble and clustermesh components.
  # It is neither required nor used when cert-manager is used to generate the certificates.
  ca:
    # -- Optional CA cert. If it is provided, it will be used by cilium to
    # generate all other certificates. Otherwise, an ephemeral CA is generated.
    cert: &quot;&quot;
    # -- Optional CA private key. If it is provided, it will be used by cilium to
    # generate all other certificates. Otherwise, an ephemeral CA is generated.
    key: &quot;&quot;
    # -- Generated certificates validity duration in days. This will be used for auto generated CA.
    certValidityDuration: 1095
  # -- Configure the CA trust bundle used for the validation of the certificates
  # leveraged by hubble and clustermesh. When enabled, it overrides the content of the
  # &#x27;ca.crt&#x27; field of the respective certificates, allowing for CA rotation with no down-time.
  caBundle:
    # -- Enable the use of the CA trust bundle.
    enabled: false
    # -- Name of the ConfigMap containing the CA trust bundle.
    name: cilium-root-ca.crt
    # -- Entry of the ConfigMap containing the CA trust bundle.
    key: ca.crt
    # -- Use a Secret instead of a ConfigMap.
    useSecret: false
    # If uncommented, creates the ConfigMap and fills it with the specified content.
    # Otherwise, the ConfigMap is assumed to be already present in .Release.Namespace.
    #
    # content: |
    #   -----BEGIN CERTIFICATE-----
    #   ...
    #   -----END CERTIFICATE-----
    #   -----BEGIN CERTIFICATE-----
    #   ...
    #   -----END CERTIFICATE-----
# -- Tunneling protocol to use in tunneling mode and for ad-hoc tunnels.
# Possible values:
#   - &quot;&quot;
#   - vxlan
#   - geneve
# @default -- `&quot;vxlan&quot;`
tunnelProtocol: &quot;&quot;
# -- Enable native-routing mode or tunneling mode.
# Possible values:
#   - &quot;&quot;
#   - native
#   - tunnel
# @default -- `&quot;tunnel&quot;`
routingMode: &quot;&quot;
# -- Configure VXLAN and Geneve tunnel port.
# @default -- Port 8472 for VXLAN, Port 6081 for Geneve
tunnelPort: 0
# -- Configure VXLAN and Geneve tunnel source port range hint.
# @default -- 0-0 to let the kernel driver decide the range
tunnelSourcePortRange: 0-0
# -- Configure what the response should be to traffic for a service without backends.
# Possible values:
#  - reject (default)
#  - drop
serviceNoBackendResponse: reject
# -- Configure the underlying network MTU to overwrite auto-detected MTU.
# This value doesn&#x27;t change the host network interface MTU i.e. eth0 or ens0.
# It changes the MTU for cilium_net@cilium_host, cilium_host@cilium_net,
# cilium_vxlan and lxc_health interfaces.
MTU: 0
# -- Disable the usage of CiliumEndpoint CRD.
disableEndpointCRD: false
wellKnownIdentities:
  # -- Enable the use of well-known identities.
  enabled: false
etcd:
  # -- Enable etcd mode for the agent.
  enabled: false
  # -- List of etcd endpoints
  endpoints:
    - https://CHANGE-ME:2379
  # -- Enable use of TLS/SSL for connectivity to etcd.
  ssl: false
operator:
  # -- Enable the cilium-operator component (required).
  enabled: true
  # -- Roll out cilium-operator pods automatically when configmap is updated.
  rollOutPods: false
  # -- cilium-operator image.
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: &quot;quay.io/cilium/operator&quot;
    tag: &quot;v1.17.5&quot;
    # operator-generic-digest
    genericDigest: &quot;sha256:f954c97eeb1b47ed67d08cc8fb4108fb829f869373cbb3e698a7f8ef1085b09e&quot;
    # operator-azure-digest
    azureDigest: &quot;sha256:add78783fdaced7453a324612eeb9ebecf56002b56c14c73596b3b4923321026&quot;
    # operator-aws-digest
    awsDigest: &quot;sha256:3e189ec1e286f1bf23d47c45bdeac6025ef7ec3d2dc16190ee768eb94708cbc3&quot;
    # operator-alibabacloud-digest
    alibabacloudDigest: &quot;sha256:654db67929f716b6178a34a15cb8f95e391465085bcf48cdba49819a56fcd259&quot;
    useDigest: true
    pullPolicy: &quot;IfNotPresent&quot;
    suffix: &quot;&quot;
  # -- Number of replicas to run for the cilium-operator deployment
  replicas: 2
  # -- The priority class to use for cilium-operator
  priorityClassName: &quot;&quot;
  # -- DNS policy for Cilium operator pods.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: &quot;&quot;
  # -- cilium-operator update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      # @schema
      # type: [integer, string]
      # @schema
      maxSurge: 25%
      # @schema
      # type: [integer, string]
      # @schema
      maxUnavailable: 50%
  # -- Affinity for cilium-operator
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              io.cilium/app: operator
  # -- Pod topology spread constraints for cilium-operator
  topologySpreadConstraints: []
  #   - maxSkew: 1
  #     topologyKey: topology.kubernetes.io/zone
  #     whenUnsatisfiable: DoNotSchedule

  # -- Node labels for cilium-operator pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  # -- Node tolerations for cilium-operator scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
    - operator: Exists
      # - key: &quot;key&quot;
      #   operator: &quot;Equal|Exists&quot;
      #   value: &quot;value&quot;
      #   effect: &quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)&quot;
  # -- Additional cilium-operator container arguments.
  extraArgs: []
  # -- Additional cilium-operator environment variables.
  extraEnv: []
  # -- Additional cilium-operator hostPath mounts.
  extraHostPathMounts: []
  # - name: host-mnt-data
  #   mountPath: /host/mnt/data
  #   hostPath: /mnt/data
  #   hostPathType: Directory
  #   readOnly: true
  #   mountPropagation: HostToContainer

  # -- Additional cilium-operator volumes.
  extraVolumes: []
  # -- Additional cilium-operator volumeMounts.
  extraVolumeMounts: []
  # -- Annotations to be added to all top-level cilium-operator objects (resources under templates/cilium-operator)
  annotations: {}
  # -- HostNetwork setting
  hostNetwork: true
  # -- Security context to be added to cilium-operator pods
  podSecurityContext: {}
  # -- Annotations to be added to cilium-operator pods
  podAnnotations: {}
  # -- Labels to be added to cilium-operator pods
  podLabels: {}
  # PodDisruptionBudget settings
  podDisruptionBudget:
    # -- enable PodDisruptionBudget
    # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    enabled: false
    # @schema
    # type: [null, integer, string]
    # @schema
    # -- Minimum number/percentage of pods that should remain scheduled.
    # When it&#x27;s set, maxUnavailable must be disabled by `maxUnavailable: null`
    minAvailable: null
    # @schema
    # type: [null, integer, string]
    # @schema
    # -- Maximum number/percentage of pods that may be made unavailable
    maxUnavailable: 1
  # -- cilium-operator resource limits &amp; requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
  #   limits:
  #     cpu: 1000m
  #     memory: 1Gi
  #   requests:
  #     cpu: 100m
  #     memory: 128Mi

  # -- Security context to be added to cilium-operator pods
  securityContext: {}
  # runAsUser: 0

  # -- Interval for endpoint garbage collection.
  endpointGCInterval: &quot;5m0s&quot;
  # -- Interval for cilium node garbage collection.
  nodeGCInterval: &quot;5m0s&quot;
  # -- Interval for identity garbage collection.
  identityGCInterval: &quot;15m0s&quot;
  # -- Timeout for identity heartbeats.
  identityHeartbeatTimeout: &quot;30m0s&quot;
  pprof:
    # -- Enable pprof for cilium-operator
    enabled: false
    # -- Configure pprof listen address for cilium-operator
    address: localhost
    # -- Configure pprof listen port for cilium-operator
    port: 6061
  # -- Enable prometheus metrics for cilium-operator on the configured port at
  # /metrics
  prometheus:
    metricsService: false
    enabled: true
    port: 9963
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: false
      # -- Labels to add to ServiceMonitor cilium-operator
      labels: {}
      # -- Annotations to add to ServiceMonitor cilium-operator
      annotations: {}
      # -- jobLabel to add for ServiceMonitor cilium-operator
      jobLabel: &quot;&quot;
      # -- Interval for scrape metrics.
      interval: &quot;10s&quot;
      # @schema
      # type: [null, array]
      # @schema
      # -- Relabeling configs for the ServiceMonitor cilium-operator
      relabelings: ~
      # @schema
      # type: [null, array]
      # @schema
      # -- Metrics relabeling configs for the ServiceMonitor cilium-operator
      metricRelabelings: ~
  # -- Grafana dashboards for cilium-operator
  # grafana can import dashboards based on the label and value
  # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
  dashboards:
    enabled: false
    label: grafana_dashboard
    # @schema
    # type: [null, string]
    # @schema
    namespace: ~
    labelValue: &quot;1&quot;
    annotations: {}
  # -- Skip CRDs creation for cilium-operator
  skipCRDCreation: false
  # -- Remove Cilium node taint from Kubernetes nodes that have a healthy Cilium
  # pod running.
  removeNodeTaints: true
  # @schema
  # type: [null, boolean]
  # @schema
  # -- Taint nodes where Cilium is scheduled but not running. This prevents pods
  # from being scheduled to nodes where Cilium is not the default CNI provider.
  # @default -- same as removeNodeTaints
  setNodeTaints: ~
  # -- Set Node condition NetworkUnavailable to &#x27;false&#x27; with the reason
  # &#x27;CiliumIsUp&#x27; for nodes that have a healthy Cilium pod.
  setNodeNetworkStatus: true
  unmanagedPodWatcher:
    # -- Restart any pod that are not managed by Cilium.
    restart: true
    # -- Interval, in seconds, to check if there are any pods that are not
    # managed by Cilium.
    intervalSeconds: 15
nodeinit:
  # -- Enable the node initialization DaemonSet
  enabled: false
  # -- node-init image.
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: &quot;quay.io/cilium/startup-script&quot;
    tag: &quot;c54c7edeab7fde4da68e59acd319ab24af242c3f&quot;
    digest: &quot;sha256:8d7b41c4ca45860254b3c19e20210462ef89479bb6331d6760c4e609d651b29c&quot;
    useDigest: true
    pullPolicy: &quot;IfNotPresent&quot;
  # -- The priority class to use for the nodeinit pod.
  priorityClassName: &quot;&quot;
  # -- node-init update strategy
  updateStrategy:
    type: RollingUpdate
  # -- Additional nodeinit environment variables.
  extraEnv: []
  # -- Additional nodeinit volumes.
  extraVolumes: []
  # -- Additional nodeinit volumeMounts.
  extraVolumeMounts: []
  # -- Affinity for cilium-nodeinit
  affinity: {}
  # -- Node labels for nodeinit pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  # -- Node tolerations for nodeinit scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
    - operator: Exists
      # - key: &quot;key&quot;
      #   operator: &quot;Equal|Exists&quot;
      #   value: &quot;value&quot;
      #   effect: &quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)&quot;
  # -- Annotations to be added to all top-level nodeinit objects (resources under templates/cilium-nodeinit)
  annotations: {}
  # -- Annotations to be added to node-init pods.
  podAnnotations: {}
  # -- Labels to be added to node-init pods.
  podLabels: {}
  # -- Security Context for cilium-node-init pods.
  podSecurityContext:
    # -- AppArmorProfile options for the `cilium-node-init` and init containers
    appArmorProfile:
      type: &quot;Unconfined&quot;
  # -- nodeinit resource limits &amp; requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
  # -- Security context to be added to nodeinit pods.
  securityContext:
    privileged: false
    seLinuxOptions:
      level: &#x27;s0&#x27;
      # Running with spc_t since we have removed the privileged mode.
      # Users can change it to a different type as long as they have the
      # type available on the system.
      type: &#x27;spc_t&#x27;
    capabilities:
      add:
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # Used for nsenter
        - NET_ADMIN
        - SYS_ADMIN
        - SYS_CHROOT
        - SYS_PTRACE
  # -- bootstrapFile is the location of the file where the bootstrap timestamp is
  # written by the node-init DaemonSet
  bootstrapFile: &quot;/tmp/cilium-bootstrap.d/cilium-bootstrap-time&quot;
  # -- startup offers way to customize startup nodeinit script (pre and post position)
  startup:
    preScript: &quot;&quot;
    postScript: &quot;&quot;
  # -- prestop offers way to customize prestop nodeinit script (pre and post position)
  prestop:
    preScript: &quot;&quot;
    postScript: &quot;&quot;
preflight:
  # -- Enable Cilium pre-flight resources (required for upgrade)
  enabled: false
  # -- Cilium pre-flight image.
  image:
    # @schema
    # type: [null, string]
    # @schema
    override: ~
    repository: &quot;quay.io/cilium/cilium&quot;
    tag: &quot;v1.17.5&quot;
    # cilium-digest
    digest: &quot;sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6&quot;
    useDigest: true
    pullPolicy: &quot;IfNotPresent&quot;
  # -- The priority class to use for the preflight pod.
  priorityClassName: &quot;&quot;
  # -- preflight update strategy
  updateStrategy:
    type: RollingUpdate
  # -- Additional preflight environment variables.
  extraEnv: []
  # -- Additional preflight volumes.
  extraVolumes: []
  # -- Additional preflight volumeMounts.
  extraVolumeMounts: []
  # -- Affinity for cilium-preflight
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium
  # -- Node labels for preflight pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  # -- Node tolerations for preflight scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
    - operator: Exists
      # - key: &quot;key&quot;
      #   operator: &quot;Equal|Exists&quot;
      #   value: &quot;value&quot;
      #   effect: &quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)&quot;
  # -- Annotations to be added to all top-level preflight objects (resources under templates/cilium-preflight)
  annotations: {}
  # -- Security context to be added to preflight pods.
  podSecurityContext: {}
  # -- Annotations to be added to preflight pods
  podAnnotations: {}
  # -- Labels to be added to the preflight pod.
  podLabels: {}
  # PodDisruptionBudget settings
  podDisruptionBudget:
    # -- enable PodDisruptionBudget
    # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    enabled: false
    # @schema
    # type: [null, integer, string]
    # @schema
    # -- Minimum number/percentage of pods that should remain scheduled.
    # When it&#x27;s set, maxUnavailable must be disabled by `maxUnavailable: null`
    minAvailable: null
    # @schema
    # type: [null, integer, string]
    # @schema
    # -- Maximum number/percentage of pods that may be made unavailable
    maxUnavailable: 1
  # -- preflight resource limits &amp; requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
  #   limits:
  #     cpu: 4000m
  #     memory: 4Gi
  #   requests:
  #     cpu: 100m
  #     memory: 512Mi

  readinessProbe:
    # -- For how long kubelet should wait before performing the first probe
    initialDelaySeconds: 5
    # -- interval between checks of the readiness probe
    periodSeconds: 5
  # -- Security context to be added to preflight pods
  securityContext: {}
  #   runAsUser: 0

  # -- Path to write the `--tofqdns-pre-cache` file to.
  tofqdnsPreCache: &quot;&quot;
  # -- Configure termination grace period for preflight Deployment and DaemonSet.
  terminationGracePeriodSeconds: 1
  # -- By default we should always validate the installed CNPs before upgrading
  # Cilium. This will make sure the user will have the policies deployed in the
  # cluster with the right schema.
  validateCNPs: true
# -- Explicitly enable or disable priority class.
# .Capabilities.KubeVersion is unsettable in `helm template` calls,
# it depends on k8s libraries version that Helm was compiled against.
# This option allows to explicitly disable setting the priority class, which
# is useful for rendering charts for gke clusters in advance.
enableCriticalPriorityClass: true
# disableEnvoyVersionCheck removes the check for Envoy, which can be useful
# on AArch64 as the images do not currently ship a version of Envoy.
#disableEnvoyVersionCheck: false
clustermesh:
  # -- Deploy clustermesh-apiserver for clustermesh
  useAPIServer: false
  # -- The maximum number of clusters to support in a ClusterMesh. This value
  # cannot be changed on running clusters, and all clusters in a ClusterMesh
  # must be configured with the same value. Values &gt; 255 will decrease the
  # maximum allocatable cluster-local identities.
  # Supported values are 255 and 511.
  maxConnectedClusters: 255
  # -- Enable the synchronization of Kubernetes EndpointSlices corresponding to
  # the remote endpoints of appropriately-annotated global services through ClusterMesh
  enableEndpointSliceSynchronization: false
  # -- Enable Multi-Cluster Services API support
  enableMCSAPISupport: false
  # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
  annotations: {}
  # -- Clustermesh explicit configuration.
  config:
    # -- Enable the Clustermesh explicit configuration.
    enabled: false
    # -- Default dns domain for the Clustermesh API servers
    # This is used in the case cluster addresses are not provided
    # and IPs are used.
    domain: mesh.cilium.io
    # -- List of clusters to be peered in the mesh.
    clusters: []
    # clusters:
    # # -- Name of the cluster
    # - name: cluster1
    # # -- Address of the cluster, use this if you created DNS records for
    # # the cluster Clustermesh API server.
    #   address: cluster1.mesh.cilium.io
    # # -- Port of the cluster Clustermesh API server.
    #   port: 2379
    # # -- IPs of the cluster Clustermesh API server, use multiple ones when
    # # you have multiple IPs to access the Clustermesh API server.
    #   ips:
    #   - 172.18.255.201
    # # -- base64 encoded PEM values for the cluster client certificate, private key and certificate authority.
    # # These fields can (and should) be omitted in case the CA is shared across clusters. In that case, the
    # # &quot;remote&quot; private key and certificate available in the local cluster are automatically used instead.
    #   tls:
    #     cert: &quot;&quot;
    #     key: &quot;&quot;
    #     caCert: &quot;&quot;
  apiserver:
    # -- Clustermesh API server image.
    image:
      # @schema
      # type: [null, string]
      # @schema
      override: ~
      repository: &quot;quay.io/cilium/clustermesh-apiserver&quot;
      tag: &quot;v1.17.5&quot;
      # clustermesh-apiserver-digest
      digest: &quot;sha256:78dc40b9cb8d7b1ad21a76ff3e11541809acda2ac4ef94150cc832100edc247d&quot;
      useDigest: true
      pullPolicy: &quot;IfNotPresent&quot;
    # -- TCP port for the clustermesh-apiserver health API.
    healthPort: 9880
    # -- Configuration for the clustermesh-apiserver readiness probe.
    readinessProbe: {}
    etcd:
      # The etcd binary is included in the clustermesh API server image, so the same image from above is reused.
      # Independent override isn&#x27;t supported, because clustermesh-apiserver is tested against the etcd version it is
      # built with.

      # -- Specifies the resources for etcd container in the apiserver
      resources: {}
      #   requests:
      #     cpu: 200m
      #     memory: 256Mi
      #   limits:
      #     cpu: 1000m
      #     memory: 256Mi

      # -- Security context to be added to clustermesh-apiserver etcd containers
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
      # -- lifecycle setting for the etcd container
      lifecycle: {}
      init:
        # -- Specifies the resources for etcd init container in the apiserver
        resources: {}
        #   requests:
        #     cpu: 100m
        #     memory: 100Mi
        #   limits:
        #     cpu: 100m
        #     memory: 100Mi

        # -- Additional arguments to `clustermesh-apiserver etcdinit`.
        extraArgs: []
        # -- Additional environment variables to `clustermesh-apiserver etcdinit`.
        extraEnv: []
      # @schema
      # enum: [Disk, Memory]
      # @schema
      # -- Specifies whether etcd data is stored in a temporary volume backed by
      # the node&#x27;s default medium, such as disk, SSD or network storage (Disk), or
      # RAM (Memory). The Memory option enables improved etcd read and write
      # performance at the cost of additional memory usage, which counts against
      # the memory limits of the container.
      storageMedium: Disk
    kvstoremesh:
      # -- Enable KVStoreMesh. KVStoreMesh caches the information retrieved
      # from the remote clusters in the local etcd instance.
      enabled: true
      # -- TCP port for the KVStoreMesh health API.
      healthPort: 9881
      # -- Configuration for the KVStoreMesh readiness probe.
      readinessProbe: {}
      # -- Additional KVStoreMesh arguments.
      extraArgs: []
      # -- Additional KVStoreMesh environment variables.
      extraEnv: []
      # -- Resource requests and limits for the KVStoreMesh container
      resources: {}
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M

      # -- Additional KVStoreMesh volumeMounts.
      extraVolumeMounts: []
      # -- KVStoreMesh Security context
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
      # -- lifecycle setting for the KVStoreMesh container
      lifecycle: {}
    service:
      # -- The type of service used for apiserver access.
      type: NodePort
      # -- Optional port to use as the node port for apiserver access.
      #
      # WARNING: make sure to configure a different NodePort in each cluster if
      # kube-proxy replacement is enabled, as Cilium is currently affected by a known
      # bug (#24692) when NodePorts are handled by the KPR implementation. If a service
      # with the same NodePort exists both in the local and the remote cluster, all
      # traffic originating from inside the cluster and targeting the corresponding
      # NodePort will be redirected to a local backend, regardless of whether the
      # destination node belongs to the local or the remote cluster.
      nodePort: 32379
      # -- Annotations for the clustermesh-apiserver service.
      # Example annotations to configure an internal load balancer on different cloud providers:
      # * AKS: service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;
      # * EKS: service.beta.kubernetes.io/aws-load-balancer-scheme: &quot;internal&quot;
      # * GKE: networking.gke.io/load-balancer-type: &quot;Internal&quot;
      annotations: {}
      # -- Labels for the clustermesh-apiserver service.
      labels: {}
      # @schema
      # enum: [Local, Cluster]
      # @schema
      # -- The externalTrafficPolicy of service used for apiserver access.
      externalTrafficPolicy: Cluster
      # @schema
      # enum: [Local, Cluster]
      # @schema
      # -- The internalTrafficPolicy of service used for apiserver access.
      internalTrafficPolicy: Cluster
      # @schema
      # enum: [HAOnly, Always, Never]
      # @schema
      # -- Defines when to enable session affinity.
      # Each replica in a clustermesh-apiserver deployment runs its own discrete
      # etcd cluster. Remote clients connect to one of the replicas through a
      # shared Kubernetes Service. A client reconnecting to a different backend
      # will require a full resync to ensure data integrity. Session affinity
      # can reduce the likelihood of this happening, but may not be supported
      # by all cloud providers.
      # Possible values:
      #  - &quot;HAOnly&quot; (default) Only enable session affinity for deployments with more than 1 replica.
      #  - &quot;Always&quot; Always enable session affinity.
      #  - &quot;Never&quot; Never enable session affinity. Useful in environments where
      #            session affinity is not supported, but may lead to slightly
      #            degraded performance due to more frequent reconnections.
      enableSessionAffinity: &quot;HAOnly&quot;
      # @schema
      # type: [null, string]
      # @schema
      # -- Configure a loadBalancerClass.
      # Allows to configure the loadBalancerClass on the clustermesh-apiserver
      # LB service in case the Service type is set to LoadBalancer
      # (requires Kubernetes 1.24+).
      loadBalancerClass: ~
      # @schema
      # type: [null, string]
      # @schema
      # -- Configure a specific loadBalancerIP.
      # Allows to configure a specific loadBalancerIP on the clustermesh-apiserver
      # LB service in case the Service type is set to LoadBalancer.
      loadBalancerIP: ~
      # -- Configure loadBalancerSourceRanges.
      # Allows to configure the source IP ranges allowed to access the
      # clustermesh-apiserver LB service in case the Service type is set to LoadBalancer.
      loadBalancerSourceRanges: []
    # -- Number of replicas run for the clustermesh-apiserver deployment.
    replicas: 1
    # -- lifecycle setting for the apiserver container
    lifecycle: {}
    # -- terminationGracePeriodSeconds for the clustermesh-apiserver deployment
    terminationGracePeriodSeconds: 30
    # -- Additional clustermesh-apiserver arguments.
    extraArgs: []
    # -- Additional clustermesh-apiserver environment variables.
    extraEnv: []
    # -- Additional clustermesh-apiserver volumes.
    extraVolumes: []
    # -- Additional clustermesh-apiserver volumeMounts.
    extraVolumeMounts: []
    # -- Security context to be added to clustermesh-apiserver containers
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
    # -- Security context to be added to clustermesh-apiserver pods
    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 65532
      runAsGroup: 65532
      fsGroup: 65532
    # -- Annotations to be added to clustermesh-apiserver pods
    podAnnotations: {}
    # -- Labels to be added to clustermesh-apiserver pods
    podLabels: {}
    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it&#x27;s set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # @schema
      # type: [null, integer, string]
      # @schema
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1
    # -- Resource requests and limits for the clustermesh-apiserver
    resources: {}
    #   requests:
    #     cpu: 100m
    #     memory: 64Mi
    #   limits:
    #     cpu: 1000m
    #     memory: 1024M

    # -- Affinity for clustermesh.apiserver
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  k8s-app: clustermesh-apiserver
              topologyKey: kubernetes.io/hostname
    # -- Pod topology spread constraints for clustermesh-apiserver
    topologySpreadConstraints: []
    #   - maxSkew: 1
    #     topologyKey: topology.kubernetes.io/zone
    #     whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux
    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []
    # -- clustermesh-apiserver update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        # @schema
        # type: [integer, string]
        # @schema
        maxSurge: 1
        # @schema
        # type: [integer, string]
        # @schema
        maxUnavailable: 0
    # -- The priority class to use for clustermesh-apiserver
    priorityClassName: &quot;&quot;
    tls:
      # -- Configure the clustermesh authentication mode.
      # Supported values:
      # - legacy:     All clusters access remote clustermesh instances with the same
      #               username (i.e., remote). The &quot;remote&quot; certificate must be
      #               generated with CN=remote if provided manually.
      # - migration:  Intermediate mode required to upgrade from legacy to cluster
      #               (and vice versa) with no disruption. Specifically, it enables
      #               the creation of the per-cluster usernames, while still using
      #               the common one for authentication. The &quot;remote&quot; certificate must
      #               be generated with CN=remote if provided manually (same as legacy).
      # - cluster:    Each cluster accesses remote etcd instances with a username
      #               depending on the local cluster name (i.e., remote-&lt;cluster-name&gt;).
      #               The &quot;remote&quot; certificate must be generated with CN=remote-&lt;cluster-name&gt;
      #               if provided manually. Cluster mode is meaningful only when the same
      #               CA is shared across all clusters part of the mesh.
      authMode: legacy
      # -- Allow users to provide their own certificates
      # Users may need to provide their certificates using
      # a mechanism that requires they provide their own secrets.
      # This setting does not apply to any of the auto-generated
      # mechanisms below, it only restricts the creation of secrets
      # via the `tls-provided` templates.
      enableSecrets: true
      # -- Configure automatic TLS certificates generation.
      # A Kubernetes CronJob is used the generate any
      # certificates not provided by the user at installation
      # time.
      auto:
        # -- When set to true, automatically generate a CA and certificates to
        # enable mTLS between clustermesh-apiserver and external workload instances.
        # If set to false, the certs to be provided by setting appropriate values below.
        enabled: true
        # Sets the method to auto-generate certificates. Supported values:
        # - helm:         This method uses Helm to generate all certificates.
        # - cronJob:      This method uses a Kubernetes CronJob the generate any
        #                 certificates not provided by the user at installation
        #                 time.
        # - certmanager:  This method use cert-manager to generate &amp; rotate certificates.
        method: helm
        # -- Generated certificates validity duration in days.
        certValidityDuration: 1095
        # -- Schedule for certificates regeneration (regardless of their expiration date).
        # Only used if method is &quot;cronJob&quot;. If nil, then no recurring job will be created.
        # Instead, only the one-shot job is deployed to generate the certificates at
        # installation time.
        #
        # Due to the out-of-band distribution of client certs to external workloads the
        # CA is (re)regenerated only if it is not provided as a helm value and the k8s
        # secret is manually deleted.
        #
        # Defaults to none. Commented syntax gives midnight of the first day of every
        # fourth month. For syntax, see
        # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
        # schedule: &quot;0 0 1 */4 *&quot;

        # [Example]
        # certManagerIssuerRef:
        #   group: cert-manager.io
        #   kind: ClusterIssuer
        #   name: ca-issuer
        # -- certmanager issuer used when clustermesh.apiserver.tls.auto.method=certmanager.
        certManagerIssuerRef: {}
      # -- base64 encoded PEM values for the clustermesh-apiserver server certificate and private key.
      # Used if &#x27;auto&#x27; is not enabled.
      server:
        cert: &quot;&quot;
        key: &quot;&quot;
        # -- Extra DNS names added to certificate when it&#x27;s auto generated
        extraDnsNames: []
        # -- Extra IP addresses added to certificate when it&#x27;s auto generated
        extraIpAddresses: []
      # -- base64 encoded PEM values for the clustermesh-apiserver admin certificate and private key.
      # Used if &#x27;auto&#x27; is not enabled.
      admin:
        cert: &quot;&quot;
        key: &quot;&quot;
      # -- base64 encoded PEM values for the clustermesh-apiserver client certificate and private key.
      # Used if &#x27;auto&#x27; is not enabled.
      client:
        cert: &quot;&quot;
        key: &quot;&quot;
      # -- base64 encoded PEM values for the clustermesh-apiserver remote cluster certificate and private key.
      # Used if &#x27;auto&#x27; is not enabled.
      remote:
        cert: &quot;&quot;
        key: &quot;&quot;
    # clustermesh-apiserver Prometheus metrics configuration
    metrics:
      # -- Enables exporting apiserver metrics in OpenMetrics format.
      enabled: true
      # -- Configure the port the apiserver metric server listens on.
      port: 9962
      kvstoremesh:
        # -- Enables exporting KVStoreMesh metrics in OpenMetrics format.
        enabled: true
        # -- Configure the port the KVStoreMesh metric server listens on.
        port: 9964
      etcd:
        # -- Enables exporting etcd metrics in OpenMetrics format.
        enabled: true
        # -- Set level of detail for etcd metrics; specify &#x27;extensive&#x27; to include server side gRPC histogram metrics.
        mode: basic
        # -- Configure the port the etcd metric server listens on.
        port: 9963
      serviceMonitor:
        # -- Enable service monitor.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor clustermesh-apiserver
        labels: {}
        # -- Annotations to add to ServiceMonitor clustermesh-apiserver
        annotations: {}
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: &quot;&quot;

        # -- Interval for scrape metrics (apiserver metrics)
        interval: &quot;10s&quot;
        # @schema
        # type: [null, array]
        # @schema
        # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
        relabelings: ~
        # @schema
        # type: [null, array]
        # @schema
        # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
        metricRelabelings: ~
        kvstoremesh:
          # -- Interval for scrape metrics (KVStoreMesh metrics)
          interval: &quot;10s&quot;
          # @schema
          # type: [null, array]
          # @schema
          # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
          relabelings: ~
          # @schema
          # type: [null, array]
          # @schema
          # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
          metricRelabelings: ~
        etcd:
          # -- Interval for scrape metrics (etcd metrics)
          interval: &quot;10s&quot;
          # @schema
          # type: [null, array]
          # @schema
          # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
          relabelings: ~
          # @schema
          # type: [null, array]
          # @schema
          # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
          metricRelabelings: ~
# -- Configure external workloads support
externalWorkloads:
  # -- Enable support for external workloads, such as VMs (false by default).
  enabled: false
# -- Configure cgroup related configuration
cgroup:
  autoMount:
    # -- Enable auto mount of cgroup2 filesystem.
    # When `autoMount` is enabled, cgroup2 filesystem is mounted at
    # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.
    # If users disable `autoMount`, it&#x27;s expected that users have mounted
    # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the
    # volume will be mounted inside the cilium agent pod at the same path.
    enabled: true
    # -- Init Container Cgroup Automount resource limits &amp; requests
    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi
  # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)
  hostRoot: /run/cilium/cgroupv2
# -- Configure sysctl override described in #20072.
sysctlfix:
  # -- Enable the sysctl override. When enabled, the init container will mount the /proc of the host so that the `sysctlfix` utility can execute.
  enabled: true
# -- Configure whether to enable auto detect of terminating state for endpoints
# in order to support graceful termination.
enableK8sTerminatingEndpoint: true
# -- Configure whether to unload DNS policy rules on graceful shutdown
# dnsPolicyUnloadOnShutdown: false

# -- Configure the key of the taint indicating that Cilium is not ready on the node.
# When set to a value starting with `ignore-taint.cluster-autoscaler.kubernetes.io/`, the Cluster Autoscaler will ignore the taint on its decisions, allowing the cluster to scale up.
agentNotReadyTaintKey: &quot;node.cilium.io/agent-not-ready&quot;
dnsProxy:
  # -- Timeout (in seconds) when closing the connection between the DNS proxy and the upstream server. If set to 0, the connection is closed immediately (with TCP RST). If set to -1, the connection is closed asynchronously in the background.
  socketLingerTimeout: 10
  # -- DNS response code for rejecting DNS requests, available options are &#x27;[nameError refused]&#x27;.
  dnsRejectResponseCode: refused
  # -- Allow the DNS proxy to compress responses to endpoints that are larger than 512 Bytes or the EDNS0 option, if present.
  enableDnsCompression: true
  # -- Maximum number of IPs to maintain per FQDN name for each endpoint.
  endpointMaxIpPerHostname: 1000
  # -- Time during which idle but previously active connections with expired DNS lookups are still considered alive.
  idleConnectionGracePeriod: 0s
  # -- Maximum number of IPs to retain for expired DNS lookups with still-active connections.
  maxDeferredConnectionDeletes: 10000
  # -- The minimum time, in seconds, to use DNS data for toFQDNs policies. If
  # the upstream DNS server returns a DNS record with a shorter TTL, Cilium
  # overwrites the TTL with this value. Setting this value to zero means that
  # Cilium will honor the TTLs returned by the upstream DNS server.
  minTtl: 0
  # -- DNS cache data at this path is preloaded on agent startup.
  preCache: &quot;&quot;
  # -- Global port on which the in-agent DNS proxy should listen. Default 0 is a OS-assigned port.
  proxyPort: 0
  # -- The maximum time the DNS proxy holds an allowed DNS response before sending it along. Responses are sent as soon as the datapath is updated with the new IP information.
  proxyResponseMaxDelay: 100ms
  # -- DNS proxy operation mode (true/false, or unset to use version dependent defaults)
  # enableTransparentMode: true
# -- SCTP Configuration Values
sctp:
  # -- Enable SCTP support. NOTE: Currently, SCTP support does not support rewriting ports or multihoming.
  enabled: false
# -- Enable Non-Default-Deny policies
enableNonDefaultDenyPolicies: true
# Configuration for types of authentication for Cilium (beta)
authentication:
  # -- Enable authentication processing and garbage collection.
  # Note that if disabled, policy enforcement will still block requests that require authentication.
  # But the resulting authentication requests for these requests will not be processed, therefore the requests not be allowed.
  enabled: true
  # -- Buffer size of the channel Cilium uses to receive authentication events from the signal map.
  queueSize: 1024
  # -- Buffer size of the channel Cilium uses to receive certificate expiration events from auth handlers.
  rotatedIdentitiesQueueSize: 1024
  # -- Interval for garbage collection of auth map entries.
  gcInterval: &quot;5m0s&quot;
  # Configuration for Cilium&#x27;s service-to-service mutual authentication using TLS handshakes.
  # Note that this is not full mTLS support without also enabling encryption of some form.
  # Current encryption options are WireGuard or IPsec, configured in encryption block above.
  mutual:
    # -- Port on the agent where mutual authentication handshakes between agents will be performed
    port: 4250
    # -- Timeout for connecting to the remote node TCP socket
    connectTimeout: 5s
    # Settings for SPIRE
    spire:
      # -- Enable SPIRE integration (beta)
      enabled: false
      # -- Annotations to be added to all top-level spire objects (resources under templates/spire)
      annotations: {}
      # Settings to control the SPIRE installation and configuration
      install:
        # -- Enable SPIRE installation.
        # This will only take effect only if authentication.mutual.spire.enabled is true
        enabled: true
        # -- SPIRE namespace to install into
        namespace: cilium-spire
        # -- SPIRE namespace already exists. Set to true if Helm should not create, manage, and import the SPIRE namespace.
        existingNamespace: false
        # -- init container image of SPIRE agent and server
        initImage:
          # @schema
          # type: [null, string]
          # @schema
          override: ~
          repository: &quot;docker.io/library/busybox&quot;
          tag: &quot;1.37.0&quot;
          digest: &quot;sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b&quot;
          useDigest: true
          pullPolicy: &quot;IfNotPresent&quot;
        # SPIRE agent configuration
        agent:
          # -- The priority class to use for the spire agent
          priorityClassName: &quot;&quot;
          # -- SPIRE agent image
          image:
            # @schema
            # type: [null, string]
            # @schema
            override: ~
            repository: &quot;ghcr.io/spiffe/spire-agent&quot;
            tag: &quot;1.9.6&quot;
            digest: &quot;sha256:5106ac601272a88684db14daf7f54b9a45f31f77bb16a906bd5e87756ee7b97c&quot;
            useDigest: true
            pullPolicy: &quot;IfNotPresent&quot;
          # -- SPIRE agent service account
          serviceAccount:
            create: true
            name: spire-agent
          # -- SPIRE agent annotations
          annotations: {}
          # -- SPIRE agent labels
          labels: {}
          # -- container resource limits &amp; requests
          resources: {}
          # -- SPIRE Workload Attestor kubelet verification.
          skipKubeletVerification: true
          # -- SPIRE agent tolerations configuration
          # By default it follows the same tolerations as the agent itself
          # to allow the Cilium agent on this node to connect to SPIRE.
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations:
            - key: node.kubernetes.io/not-ready
              effect: NoSchedule
            - key: node-role.kubernetes.io/master
              effect: NoSchedule
            - key: node-role.kubernetes.io/control-plane
              effect: NoSchedule
            - key: node.cloudprovider.kubernetes.io/uninitialized
              effect: NoSchedule
              value: &quot;true&quot;
            - key: CriticalAddonsOnly
              operator: &quot;Exists&quot;
          # -- SPIRE agent affinity configuration
          affinity: {}
          # -- SPIRE agent nodeSelector configuration
          # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector: {}
          # -- Security context to be added to spire agent pods.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
          podSecurityContext: {}
          # -- Security context to be added to spire agent containers.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
          securityContext: {}
        server:
          # -- The priority class to use for the spire server
          priorityClassName: &quot;&quot;
          # -- SPIRE server image
          image:
            # @schema
            # type: [null, string]
            # @schema
            override: ~
            repository: &quot;ghcr.io/spiffe/spire-server&quot;
            tag: &quot;1.9.6&quot;
            digest: &quot;sha256:59a0b92b39773515e25e68a46c40d3b931b9c1860bc445a79ceb45a805cab8b4&quot;
            useDigest: true
            pullPolicy: &quot;IfNotPresent&quot;
          # -- SPIRE server service account
          serviceAccount:
            create: true
            name: spire-server
          # -- SPIRE server init containers
          initContainers: []
          # -- SPIRE server annotations
          annotations: {}
          # -- SPIRE server labels
          labels: {}
          # SPIRE server service configuration
          # -- container resource limits &amp; requests
          resources: {}
          service:
            # -- Service type for the SPIRE server service
            type: ClusterIP
            # -- Annotations to be added to the SPIRE server service
            annotations: {}
            # -- Labels to be added to the SPIRE server service
            labels: {}
          # -- SPIRE server affinity configuration
          affinity: {}
          # -- SPIRE server nodeSelector configuration
          # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector: {}
          # -- SPIRE server tolerations configuration
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations: []
          # SPIRE server datastorage configuration
          dataStorage:
            # -- Enable SPIRE server data storage
            enabled: true
            # -- Size of the SPIRE server data storage
            size: 1Gi
            # -- Access mode of the SPIRE server data storage
            accessMode: ReadWriteOnce
            # @schema
            # type: [null, string]
            # @schema
            # -- StorageClass of the SPIRE server data storage
            storageClass: null
          # -- Security context to be added to spire server pods.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
          podSecurityContext: {}
          # -- Security context to be added to spire server containers.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
          securityContext: {}
          # SPIRE CA configuration
          ca:
            # -- SPIRE CA key type
            # AWS requires the use of RSA. EC cryptography is not supported
            keyType: &quot;rsa-4096&quot;
            # -- SPIRE CA Subject
            subject:
              country: &quot;US&quot;
              organization: &quot;SPIRE&quot;
              commonName: &quot;Cilium SPIRE CA&quot;
      # @schema
      # type: [null, string]
      # @schema
      # -- SPIRE server address used by Cilium Operator
      #
      # If k8s Service DNS along with port number is used (e.g. &lt;service-name&gt;.&lt;namespace&gt;.svc(.*):&lt;port-number&gt; format),
      # Cilium Operator will resolve its address by looking up the clusterIP from Service resource.
      #
      # Example values: 10.0.0.1:8081, spire-server.cilium-spire.svc:8081
      serverAddress: ~
      # -- SPIFFE trust domain to use for fetching certificates
      trustDomain: spiffe.cilium
      # -- SPIRE socket path where the SPIRE delegated api agent is listening
      adminSocketPath: /run/spire/sockets/admin.sock
      # -- SPIRE socket path where the SPIRE workload agent is listening.
      # Applies to both the Cilium Agent and Operator
      agentSocketPath: /run/spire/sockets/agent/agent.sock
      # -- SPIRE connection timeout
      connectionTimeout: 30s
# -- Enable Internal Traffic Policy
enableInternalTrafficPolicy: true
# -- Enable LoadBalancer IP Address Management
enableLBIPAM: true
</code></pre><p id="234b8b2c-1c23-803d-8561-c20f27aed70b" class="">
</p></details></li></ul><figure id="234b8b2c-1c23-80ea-9faa-fa0fb2c84e00" class="image"><a href="images/image%2011.png"><img style="width:864px" src="images/image%2011.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8026-b2ab-d8435bd588f2" class="code"><code class="language-Bash"># Cilium 설치 with Helm
helm repo add cilium https://helm.cilium.io/

# 모든 NIC 지정 + bpf.masq=true + NoIptablesRules
# kubeProxyReplacement=true -&gt; kube-proxy 사용하지 않음
# routingMode=native -&gt; vxlan, geneve등의 overlay network 구성하지 않고 direct 통신
# ipv4NativeRoutingCIDR=172.20.0.0/16 -&gt; pod ip range
helm install cilium cilium/cilium --version 1.17.5 --namespace kube-system \
--set k8sServiceHost=192.168.10.100 --set k8sServicePort=6443 \
--set kubeProxyReplacement=true \
--set routingMode=native \
--set autoDirectNodeRoutes=true \
--set ipam.mode=&quot;cluster-pool&quot; \
--set ipam.operator.clusterPoolIPv4PodCIDRList={&quot;172.20.0.0/16&quot;} \
--set ipv4NativeRoutingCIDR=172.20.0.0/16 \
--set endpointRoutes.enabled=true \
--set installNoConntrackIptablesRules=true \
--set bpf.masquerade=true \
--set ipv6.enabled=false

# 확인
helm get values cilium -n kube-system
helm list -A
kubectl get crd
watch -d kubectl get pod -A

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg status --verbose
KVStore:                Disabled   
Kubernetes:             Ok         1.33 (v1.33.3) [linux/arm64]
Kubernetes APIs:        [&quot;EndpointSliceOrEndpoint&quot;, &quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;, &quot;cilium/v2::CiliumEndpoint&quot;, &quot;cilium/v2::CiliumNetworkPolicy&quot;, &quot;cilium/v2::CiliumNode&quot;, &quot;cilium/v2alpha1::CiliumCIDRGroup&quot;, &quot;core/v1::Namespace&quot;, &quot;core/v1::Pods&quot;, &quot;core/v1::Service&quot;, &quot;networking.k8s.io/v1::NetworkPolicy&quot;]
KubeProxyReplacement:   True   [eth0    10.0.2.15 fd17:625c:f037:2:a00:27ff:fe71:19d8 fe80::a00:27ff:fe71:19d8, eth1   192.168.10.101 fe80::a00:27ff:fe0a:237f (Direct Routing)]
Host firewall:          Disabled
SRv6:                   Disabled
CNI Chaining:           none
CNI Config file:        successfully wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist
Cilium:                 Ok   1.17.5 (v1.17.5-69aab28c)
NodeMonitor:            Listening for events on 2 CPUs with 64x4096 of shared memory
Cilium health daemon:   Ok   
IPAM:                   IPv4: 4/254 allocated from 172.20.0.0/24, 
Allocated addresses:
  172.20.0.161 (kube-system/coredns-674b8bbfcf-hqfb2)
  172.20.0.170 (router)
  172.20.0.192 (health)
  172.20.0.206 (kube-system/coredns-674b8bbfcf-cbhgv)
IPv4 BIG TCP:           Disabled
IPv6 BIG TCP:           Disabled
BandwidthManager:       Disabled
Routing:                Network: Native   Host: BPF
Attach Mode:            TCX
Device Mode:            veth
Masquerading:           BPF   [eth0, eth1]   172.20.0.0/16 [IPv4: Enabled, IPv6: Disabled]
...

# 노드에 iptables 확인
# cni가 추가됬음에도 별다른 iptables rule이 추가되지않음.cilium(ebpf)는 iptables rule을 사용하지 않음!!
iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N CILIUM_OUTPUT_nat
-N CILIUM_POST_nat
-N CILIUM_PRE_nat
-N KUBE-KUBELET-CANARY
-A PREROUTING -m comment --comment &quot;cilium-feeder: CILIUM_PRE_nat&quot; -j CILIUM_PRE_nat
-A OUTPUT -m comment --comment &quot;cilium-feeder: CILIUM_OUTPUT_nat&quot; -j CILIUM_OUTPUT_nat
-A POSTROUTING -m comment --comment &quot;cilium-feeder: CILIUM_POST_nat&quot; -j CILIUM_POST_nat

for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i sudo iptables -t nat -S ; echo; done

iptables-save
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i sudo iptables-save ; echo; done
</code></pre></li></ul><p id="234b8b2c-1c23-80b8-bb86-c47372b8b52f" class="">
</p><ul id="234b8b2c-1c23-8049-b40e-e295741aa10a" class="bulleted-list"><li style="list-style-type:disc">PodCIDR IPAM 확인 - <a href="https://docs.cilium.io/en/stable/network/concepts/ipam/cluster-pool/">ClusterScope</a><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-80c7-9fa5-e5c45756b410" class="code"><code class="language-Bash">#
kubectl get nodes -o jsonpath=&#x27;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.spec.podCIDR}{&quot;\n&quot;}{end}&#x27;
k8s-ctr 10.244.0.0/24
k8s-w1  10.244.1.0/24
k8s-w2  10.244.2.0/24

# 파드 IP 확인
kubectl get pod -owide
NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          22m   10.244.0.2   k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-697b545f57-hbkmr   1/1     Running   0          22m   10.244.2.4   k8s-w2    &lt;none&gt;           &lt;none&gt;
webpod-697b545f57-vx27c   1/1     Running   0          22m   10.244.1.2   k8s-w1    &lt;none&gt;           &lt;none&gt;

#
kubectl get ciliumnodes
kubectl get ciliumnodes -o json | grep podCIDRs -A2
                    &quot;podCIDRs&quot;: [
                        &quot;172.20.0.0/24&quot;
--
                    &quot;podCIDRs&quot;: [
                        &quot;172.20.1.0/24&quot;
--
                    &quot;podCIDRs&quot;: [

kubectl rollout restart deployment webpod

# pod ip ranage가 cilium에서 설정한 ip range로 변경됨!
kubectl get pod -owide
NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          23h   10.244.0.2    k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-24fj4   1/1     Running   0          5s    172.20.0.75   k8s-w1    &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-hdf8d   1/1     Running   0          8s    172.20.2.64   k8s-w2    &lt;none&gt;           &lt;none&gt;

# k8s-ctr 노드에 curl-pod 파드 배포
kubectl delete pod curl-pod --grace-period=0

cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: [&quot;tail&quot;]
    args: [&quot;-f&quot;, &quot;/dev/null&quot;]
  terminationGracePeriodSeconds: 0
EOF

kubectl get pod -owide
kubectl get ciliumendpoints
NAME                      READY   STATUS    RESTARTS   AGE    IP            NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          54s    172.20.1.56   k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-24fj4   1/1     Running   0          116s   172.20.0.75   k8s-w1    &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-hdf8d   1/1     Running   0          119s   172.20.2.64   k8s-w2    &lt;none&gt;           &lt;none&gt;
NAME                      SECURITY IDENTITY   ENDPOINT STATE   IPV4          IPV6
curl-pod                  17349               ready            172.20.1.56   
webpod-6956b86f4b-24fj4   4792                ready            172.20.0.75   
webpod-6956b86f4b-hdf8d   4792                ready            172.20.2.64 

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg endpoint list

kubectl exec -it -n kube-system po/cilium-w7shz -c cilium-agent -- cilium-dbg endpoint list
ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4           STATUS   
           ENFORCEMENT        ENFORCEMENT                                                                                                                     
632        Disabled           Disabled          1          reserved:host                                                                                      ready   
1535       Disabled           Disabled          4792       k8s:app=webpod                                                                      172.20.0.75    ready   
                                                           k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default                                     
                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                   
                                                           k8s:io.cilium.k8s.policy.serviceaccount=default                                                            
                                                           k8s:io.kubernetes.pod.namespace=default                                                                    
2071       Disabled           Disabled          6692       k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system          172.20.0.206   ready   
                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                   
                                                           k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                            
                                                           k8s:io.kubernetes.pod.namespace=kube-system                                                                
                                                           k8s:k8s-app=kube-dns                                                                                       
3052       Disabled           Disabled          6692       k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system          172.20.0.161   ready   
                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                   
                                                           k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                            
                                                           k8s:io.kubernetes.pod.namespace=kube-system                                                                
                                                           k8s:k8s-app=kube-dns                                                                                       
3465       Disabled           Disabled          4          reserved:health                                                                     172.20.0.192   ready   

kubectl exec -it -n kube-system po/cilium-z6h2h -c cilium-agent -- cilium-dbg endpoint list
ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                              IPv6   IPv4           STATUS   
           ENFORCEMENT        ENFORCEMENT                                                                                                                 
676        Disabled           Disabled          1          k8s:node-role.kubernetes.io/control-plane                                                      ready   
                                                           k8s:node.kubernetes.io/exclude-from-external-load-balancers                                            
                                                           reserved:host                                                                                          
2332       Disabled           Disabled          4          reserved:health                                                                 172.20.1.229   ready   
4031       Disabled           Disabled          17349      k8s:app=curl                                                                    172.20.1.56    ready   
                                                           k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default                                 
                                                           k8s:io.cilium.k8s.policy.cluster=default                                                               
                                                           k8s:io.cilium.k8s.policy.serviceaccount=default                                                        
                                                           k8s:io.kubernetes.pod.namespace=default 

kubectl exec -it -n kube-system po/cilium-xlfxh -c cilium-agent -- cilium-dbg endpoint list
ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                              IPv6   IPv4          STATUS   
           ENFORCEMENT        ENFORCEMENT                                                                                                                
1356       Disabled           Disabled          1          reserved:host                                                                                 ready   
2320       Disabled           Disabled          4792       k8s:app=webpod                                                                  172.20.2.64   ready   
                                                           k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default                                
                                                           k8s:io.cilium.k8s.policy.cluster=default                                                              
                                                           k8s:io.cilium.k8s.policy.serviceaccount=default                                                       
                                                           k8s:io.kubernetes.pod.namespace=default                                                               
3782       Disabled           Disabled          4          reserved:health                                                                 172.20.2.70   ready   

# 통신 확인
kubectl exec -it curl-pod -- curl webpod | grep Hostname
</code></pre></li></ul><ul id="234b8b2c-1c23-803e-ac73-e92f1adcf906" class="bulleted-list"><li style="list-style-type:disc"><code>도전과제</code> kind k8s 에 Flannel CNI 환경에서 최소 중단 전략을 통해 Cilium CNI 마이그레이션(노드 단위) 가이드 실습 따라해보기 - <a href="https://docs.cilium.io/en/stable/installation/k8s-install-migration/">Docs</a></li></ul></details></li></ul><ul id="234b8b2c-1c23-8047-a2ba-f129db51573a" class="toggle"><li><details open=""><summary><strong>Cilium 설치 확인</strong></summary><ul id="234b8b2c-1c23-80d5-93c5-d9d7339f20f4" class="bulleted-list"><li style="list-style-type:disc">cilium cli <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-806c-8a21-c1041caebd1f" class="code"><code class="language-Bash"># cilium cli 설치
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ &quot;$(uname -m)&quot; = &quot;aarch64&quot; ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz &gt;/dev/null 2&gt;&amp;1
tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz

# cilium 상태 확인
which cilium
cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 3, Ready: 3/3, Available: 3/3
DaemonSet              cilium-envoy             Desired: 3, Ready: 3/3, Available: 3/3
Deployment             cilium-operator          Desired: 2, Ready: 2/2, Available: 2/2
Containers:            cilium                   Running: 3
                       cilium-envoy             Running: 3
                       cilium-operator          Running: 2
                       clustermesh-apiserver    
                       hubble-relay             
Cluster Pods:          5/5 managed by Cilium
Helm chart version:    1.17.5
Image versions         cilium             quay.io/cilium/cilium:v1.17.5@sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6: 3
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.32.6-1749271279-0864395884b263913eac200ee2048fd985f8e626@sha256:9f69e290a7ea3d4edf9192acd81694089af048ae0d8a67fb63bd62dc1d72203e: 3
                       cilium-operator    quay.io/cilium/operator-generic:v1.17.5@sha256:f954c97eeb1b47ed67d08cc8fb4108fb829f869373cbb3e698a7f8ef1085b09e: 2

cilium config view
kubectl get cm -n kube-system cilium-config -o json | jq

#
cilium config set debug true &amp;&amp; watch kubectl get pod -A
cilium config view | grep -i debug


# cilium daemon = cilium-dbg
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg config
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg status --verbose
...
KubeProxyReplacement:   True   [eth0    10.0.2.15 fd17:625c:f037:2:a00:27ff:fe71:19d8 fe80::a00:27ff:fe71:19d8, eth1   192.168.10.102 fe80::a00:27ff:fef6:fcbc (Direct Routing)]
Routing:                Network: Native   Host: BPF
Attach Mode:            TCX
Device Mode:            veth
Masquerading:           BPF   [eth0, eth1]   172.20.0.0/16 [IPv4: Enabled, IPv6: Disabled]
...
KubeProxyReplacement Details:
  Status:                 True
  Socket LB:              Enabled
  Socket LB Tracing:      Enabled
  Socket LB Coverage:     Full
  Devices:                eth0    10.0.2.15 fd17:625c:f037:2:a00:27ff:fe71:19d8 fe80::a00:27ff:fe71:19d8, eth1   192.168.10.102 fe80::a00:27ff:fef6:fcbc (Direct Routing)
  Mode:                   SNAT
  Backend Selection:      Random
  Session Affinity:       Enabled
  Graceful Termination:   Enabled
  NAT46/64 Support:       Disabled
  XDP Acceleration:       Disabled
  Services:
  - ClusterIP:      Enabled
  - NodePort:       Enabled (Range: 30000-32767) 
  - LoadBalancer:   Enabled 
  - externalIPs:    Enabled 
  - HostPort:       Enabled
...</code></pre><p id="234b8b2c-1c23-800a-a2d8-e0e75bc78e9b" class="">
</p></li></ul><ul id="234b8b2c-1c23-80fd-98a7-f01f366a955c" class="bulleted-list"><li style="list-style-type:disc">네트워크 기본 정보 : cilium_host, cilium_net, cilium_health<figure id="234b8b2c-1c23-807b-b471-fe428bed194f" class="image"><a href="images/Untitled%2036.png"><img style="width:1200px" src="images/Untitled%2036.png"/></a></figure><figure id="234b8b2c-1c23-8063-b7e4-d2a85d188ad9" class="image"><a href="images/Untitled%2037.png"><img style="width:768px" src="images/Untitled%2037.png"/></a><figcaption><a href="http://arthurchiao.art/blog/ctrip-network-arch-evolution/">http://arthurchiao.art/blog/ctrip-network-arch-evolution/</a></figcaption></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8012-839c-d02e05915cc9" class="code"><code class="language-Bash">#
ip -c addr
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c addr ; echo; done

#
ip -c addr show cilium_net
ip -c addr show cilium_host
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c addr show cilium_net  ; echo; done
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c addr show cilium_host ; echo; done

# lxc_health 인터페이스는 veth 로 cilium(NET NS 0, 호스트와 다름)과 veth pair 이다 - 링크
# cilium 인터페이스에 파드 IP가 할당되어 있으며, cilium-health-responder 로 동작한다
ip -c addr show lxc_health
for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c addr show lxc_health  ; echo; done

# IP 확인
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg status --verbose
...
Cluster health:   3/3 reachable   (2025-07-18T16:27:49Z)
Name              IP              Node   Endpoints
  k8s-w1 (localhost):
    Host connectivity to 192.168.10.101:
      ICMP to stack:   OK, RTT=78.625µs
      HTTP to agent:   OK, RTT=72.25µs
    Endpoint connectivity to 172.20.0.192:
      ICMP to stack:   OK, RTT=144.5µs
      HTTP to agent:   OK, RTT=283.125µs
  k8s-ctr:
    Host connectivity to 192.168.10.100:
      ICMP to stack:   OK, RTT=534.25µs
      HTTP to agent:   OK, RTT=616.167µs
    Endpoint connectivity to 172.20.1.229:
      ICMP to stack:   OK, RTT=955.875µs
      HTTP to agent:   OK, RTT=897.042µs
  k8s-w2:
    Host connectivity to 192.168.10.102:
      ICMP to stack:   OK, RTT=299.542µs
      HTTP to agent:   OK, RTT=2.601958ms
    Endpoint connectivity to 172.20.2.70:
      ICMP to stack:   OK, RTT=545.333µs
      HTTP to agent:   OK, RTT=984.833µs
...

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg endpoint list | grep health
169        Disabled           Disabled          4          reserved:health                                                                     172.20.0.192   ready   

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg status --all-addresses
...
Allocated addresses:
  172.20.0.161 (kube-system/coredns-674b8bbfcf-hqfb2 [restored])
  172.20.0.170 (router)
  172.20.0.192 (health)
  172.20.0.206 (kube-system/coredns-674b8bbfcf-cbhgv [restored])
  172.20.0.75 (default/webpod-6956b86f4b-24fj4 [restored])

# Check health info in CT/NAT tables : ICMP records in Conntrack (CT) table and NAT table
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium bpf ct list global | grep ICMP |head -n4
ICMP IN 10.0.2.15:48077 -&gt; 172.20.0.192:0 expires=10603 Packets=0 Bytes=0 RxFlagsSeen=0x00 LastRxReport=10543 TxFlagsSeen=0x00 LastTxReport=10543 Flags=0x0000 [ ] RevNAT=0 SourceSecurityID=1 IfIndex=0 BackendID=0 
ICMP OUT 192.168.10.101:45322 -&gt; 192.168.10.100:0 expires=10633 Packets=0 Bytes=0 RxFlagsSeen=0x00 LastRxReport=10573 TxFlagsSeen=0x00 LastTxReport=10573 Flags=0x0000 [ ] RevNAT=0 SourceSecurityID=0 IfIndex=0 BackendID=0 
ICMP IN 192.168.10.102:43932 -&gt; 172.20.0.192:0 expires=10843 Packets=0 Bytes=0 RxFlagsSeen=0x00 LastRxReport=10783 TxFlagsSeen=0x00 LastTxReport=10783 Flags=0x0000 [ ] RevNAT=0 SourceSecurityID=6 IfIndex=0 BackendID=0 
ICMP OUT 192.168.10.101:51029 -&gt; 192.168.10.102:0 expires=10583 Packets=0 Bytes=0 RxFlagsSeen=0x00 LastRxReport=10523 TxFlagsSeen=0x00 LastTxReport=10523 Flags=0x0000 [ ] RevNAT=0 SourceSecurityID=0 IfIndex=0 BackendID=0 

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium bpf nat list | grep ICMP |head -n4
ICMP IN 192.168.10.102:0 -&gt; 192.168.10.101:38160 XLATE_DST 192.168.10.101:38160 Created=363sec ago NeedsCT=1
ICMP IN 172.20.1.229:0 -&gt; 192.168.10.101:46065 XLATE_DST 192.168.10.101:46065 Created=263sec ago NeedsCT=1
ICMP OUT 192.168.10.101:45311 -&gt; 172.20.1.229:0 XLATE_SRC 192.168.10.101:45311 Created=183sec ago NeedsCT=1
ICMP IN 172.20.1.229:0 -&gt; 192.168.10.101:44362 XLATE_DST 192.168.10.101:44362 Created=393sec ago NeedsCT=1

#</code></pre><figure id="234b8b2c-1c23-80d3-9bcc-f0f90ae7ef74" class="image"><a href="images/image%2012.png"><img style="width:1008px" src="images/image%2012.png"/></a><figcaption><a href="https://arthurchiao.art/blog/cilium-code-health-probe/">https://arthurchiao.art/blog/cilium-code-health-probe/</a></figcaption></figure><p id="234b8b2c-1c23-80f7-8482-c91948c0dc96" class="">
</p></li></ul><ul id="234b8b2c-1c23-8027-b51a-c77715d25318" class="bulleted-list"><li style="list-style-type:disc">routing<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-8027-9bfb-f74f9c62c890" class="code"><code class="language-Bash"># Native-Routing + autoDirectNodeRoutes=true
# 다른 node의 pod ip range로 통신이 해당 node의 eth1로 바로 간다.
ip -c route | grep 172.20 | grep eth1
172.20.0.0/24 via 192.168.10.101 dev eth1 proto kernel 
172.20.2.0/24 via 192.168.10.102 dev eth1 proto kernel 

for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c route | grep 172.20 | grep eth1 ; echo; done
&gt;&gt; node : k8s-w1 &lt;&lt;
172.20.1.0/24 via 192.168.10.100 dev eth1 proto kernel 
172.20.2.0/24 via 192.168.10.102 dev eth1 proto kernel 

&gt;&gt; node : k8s-w2 &lt;&lt;
172.20.0.0/24 via 192.168.10.101 dev eth1 proto kernel 
172.20.1.0/24 via 192.168.10.100 dev eth1 proto kernel 

# hostNetwork 를 사용하지 않는 파드의 경우 endpointRoutes.enabled=true 설정으로 lxcY 인터페이스 생성됨
kubectl get ciliumendpoints -A
NAMESPACE     NAME                       SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6
default       curl-pod                   17349               ready            172.20.1.56    
default       webpod-6956b86f4b-24fj4    4792                ready            172.20.0.75    
default       webpod-6956b86f4b-hdf8d    4792                ready            172.20.2.64    
kube-system   coredns-674b8bbfcf-cbhgv   6692                ready            172.20.0.206   
kube-system   coredns-674b8bbfcf-hqfb2   6692                ready            172.20.0.161  

ip -c route | grep lxc
172.20.1.56 dev lxc576a05ffa79c proto kernel scope link 
172.20.1.229 dev lxc_health proto kernel scope link

for i in w1 w2 ; do echo &quot;&gt;&gt; node : k8s-$i &lt;&lt;&quot;; sshpass -p &#x27;vagrant&#x27; ssh vagrant@k8s-$i ip -c route | grep lxc ; echo; done
</code></pre></li></ul><p id="234b8b2c-1c23-8042-b791-d386d1c18e74" class="">
</p></details></li></ul><ul id="234b8b2c-1c23-80c2-adab-d484707a631b" class="toggle"><li><details open=""><summary>Cilium CMD Cheatsheet - <a href="https://docs.cilium.io/en/stable/cheatsheet/">Docs</a> , CMD Reference - <a href="https://docs.cilium.io/en/stable/cmdref/">Docs</a></summary><figure id="234b8b2c-1c23-80a7-b326-e8f48f011896"><div class="source"><a href="images/Isovalent_-_Cilium_Cheat_Sheet.pdf">Isovalent - Cilium Cheat Sheet.pdf</a></div></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="234b8b2c-1c23-807a-9511-ea5603db24ab" class="code"><code class="language-Bash"># cilium 파드 이름
export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-ctr -o jsonpath=&#x27;{.items[0].metadata.name}&#x27;)
export CILIUMPOD1=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-w1  -o jsonpath=&#x27;{.items[0].metadata.name}&#x27;)
export CILIUMPOD2=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-w2  -o jsonpath=&#x27;{.items[0].metadata.name}&#x27;)
echo $CILIUMPOD0 $CILIUMPOD1 $CILIUMPOD2

# 단축키(alias) 지정
alias c0=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium&quot;
alias c1=&quot;kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- cilium&quot;
alias c2=&quot;kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- cilium&quot;

alias c0bpf=&quot;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- bpftool&quot;
alias c1bpf=&quot;kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- bpftool&quot;
alias c2bpf=&quot;kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- bpftool&quot;


# endpoint
c0 endpoint list
c0 endpoint list -o json
c1 endpoint list
c2 endpoint list

c1 endpoint get &lt;id&gt;
c1 endpoint log &lt;id&gt;

## Enable debugging output on the cilium-dbg monitor for this endpoint
c1 endpoint config &lt;id&gt; Debug=true


# monitor
c1 monitor
c1 monitor -v
c1 monitor -v -v

## Filter for only the events related to endpoint
c1 monitor --related-to=&lt;id&gt;

## Show notifications only for dropped packet events
c1 monitor --type drop

## Don’t dissect packet payload, display payload in hex information
c1 monitor -v -v --hex

## Layer7
c1 monitor -v --type l7


# Manage IP addresses and associated information - IP List
c0 ip list

# IDENTITY :  1(host), 2(world), 4(health), 6(remote), 파드마다 개별 ID
c0 ip list -n

# Retrieve information about an identity
c0 identity list

# 엔드포인트 기준 ID
c0 identity list --endpoints

# 엔드포인트 설정 확인 및 변경
c0 endpoint config &lt;엔트포인트ID&gt;

# 엔드포인트 상세 정보 확인
c0 endpoint get &lt;엔트포인트ID&gt;

# 엔드포인트 로그 확인
c0 endpoint log &lt;엔트포인트ID&gt;

# Show bpf filesystem mount details
c0 bpf fs show

# bfp 마운트 폴더 확인
tree /sys/fs/bpf


# Get list of loadbalancer services
c0 service list
c1 service list
c2 service list

## Or you can get the loadbalancer information using bpf list
c0 bpf lb list
c1 bpf lb list
c2 bpf lb list

## List reverse NAT entries
c1 bpf lb list --revnat
c2 bpf lb list --revnat


# List connection tracking entries
c0 bpf ct list global
c1 bpf ct list global
c2 bpf ct list global

# Flush connection tracking entries
c0 bpf ct flush
c1 bpf ct flush
c2 bpf ct flush


# List all NAT mapping entries
c0 bpf nat list
c1 bpf nat list
c2 bpf nat list

# Flush all NAT mapping entries
c0 bpf nat flush
c1 bpf nat flush
c2 bpf nat flush

# Manage the IPCache mappings for IP/CIDR &lt;-&gt; Identity
c0 bpf ipcache list# Display cgroup metadata maintained by Cilium
c0 cgroups list
c1 cgroups list
c2 cgroups list


# List all open BPF maps
c0 map list
c1 map list --verbose
c2 map list --verbose

c1 map events cilium_lb4_services_v2
c1 map events cilium_lb4_reverse_nat
c1 map events cilium_lxc
c1 map events cilium_ipcache


# List all metrics
c1 metrics list


# List contents of a policy BPF map : Dump all policy maps
c0 bpf policy get --all
c1 bpf policy get --all -n
c2 bpf policy get --all -n


# Dump StateDB contents as JSON
c0 statedb dump


#
c0 shell -- db/show devices
c1 shell -- db/show devices
c2 shell -- db/show devices
</code></pre><p id="234b8b2c-1c23-8085-bebc-c386bc480c64" class="">
</p></details></li></ul><h3 id="235b8b2c-1c23-8035-aa55-ed9707862378" class="">4. 통신 확인</h3><ul id="235b8b2c-1c23-8077-9e0e-e2afaa97e769" class="toggle"><li><details open=""><summary><strong>4.1. eBPF kernel hooks - 참조(</strong><a href="https://velog.io/@_gyullbb/Cilium">https://velog.io/@_gyullbb/Cilium</a>)</summary><p id="235b8b2c-1c23-8001-8a1e-c33e3ffb1a14" class="">eBPF는 리눅스 커널의 다양한 지점에 프로그램을 hook, 즉 연결을 해서 특정 이벤트가 발생할 때 커스텀 로직을 실행할 수 있게 해준다.</p><p id="235b8b2c-1c23-80e9-91bd-d925b5e486ab" class="">각 계층 별로 걸 수 있는 eBPF Hook 예시를 확인해본다.</p><p id="235b8b2c-1c23-8009-9677-ce769945a76e" class=""><div class="indented"><p id="235b8b2c-1c23-8024-b1b9-fce0c31cd3b6" class=""><strong>4.1.1. Driver 계층</strong></p><p id="235b8b2c-1c23-80bb-8e99-d38b48a6a865" class="">네트워크 장치 드라이버 또는 하드웨어와의 인터페이스에서 BPF 프로그램을 직접 실행한다.<div class="indented"><p id="235b8b2c-1c23-8070-aa0b-f2b1f2644786" class=""><strong>4.1.1-1. XDP</strong></p><p id="235b8b2c-1c23-80b6-b26d-e7767ba5de84" class="">위치: NIC(Network Interface Card) 드라이버 수준에서 직접 실행</p><p id="235b8b2c-1c23-80a3-a364-ed845c040dea" class="">용도: 초고속 패킷 처리</p><p id="235b8b2c-1c23-805c-9757-f3a1bd73832c" class="">예시: 고성능 로드 밸런서(NIC)에서 패킷을 바로 분산 처리 가능</p></div></p><p id="235b8b2c-1c23-809e-acec-f3cdef081034" class="">
</p><p id="235b8b2c-1c23-802a-abce-cb482621a9d1" class=""><strong>4.1.2. 네트워크 계층</strong><div class="indented"><p id="235b8b2c-1c23-80df-987e-ef30fef7aeea" class=""><strong>4.1.2-1. TC(Traffic Control)</strong></p><ul id="235b8b2c-1c23-8034-9c41-d7d529b5064c" class="bulleted-list"><li style="list-style-type:disc">위치: 네트워크 트래픽의 Ingress(수신) 및 Egress(송신) 지점</li></ul><ul id="235b8b2c-1c23-8019-a67d-ed3fedd64ce7" class="bulleted-list"><li style="list-style-type:disc">용도: 패킷 필터링, QoS(품질 보장), 로드 밸런싱, 정책 기반 라우팅</li></ul><ul id="235b8b2c-1c23-8078-b18d-e3fe1581b517" class="bulleted-list"><li style="list-style-type:disc">예시: Cilium에서 네트워크 보안 정책 구현</li></ul><p id="235b8b2c-1c23-8040-9966-d92595582b9f" class=""><strong>4.1.2-2. XDP(eXpress Data Path)</strong></p><ul id="235b8b2c-1c23-80b1-88ce-cac8a4c36f33" class="bulleted-list"><li style="list-style-type:disc">위치: 네트워크 인터페이스에서 패킷 수신 직후, 커널의 네트워크 스택에 도달하기 전에 실행</li></ul><ul id="235b8b2c-1c23-8038-a1f2-e51b8e35c04d" class="bulleted-list"><li style="list-style-type:disc">용도: 고성능 네트워크 필터링 및 정책 구현</li></ul><ul id="235b8b2c-1c23-804a-8b28-efca2976b5bf" class="bulleted-list"><li style="list-style-type:disc">예시: DDoS 방어(악성 패킷이 스택에 전달되기 전에 drop), L4 로드 밸런싱(목적지 서버에 대한 초기 라우팅 수행)</li></ul></div></p></div></p><p id="235b8b2c-1c23-80a4-aad1-e7195bc31dfe" class=""><div class="indented"><p id="235b8b2c-1c23-8004-b2c0-de16055f45f4" class=""><strong>4.1.3. 소켓 계층</strong></p><p id="235b8b2c-1c23-800b-8252-f79885372a41" class="">소켓에서 발생하는 이벤트에 Hook을 걸어 애플리케이션 계층 통신 제어가 가능하다.<div class="indented"><p id="235b8b2c-1c23-806e-8c5d-d8acd7b76266" class=""><strong>4.1.3-1. Socket Filtering</strong></p><ul id="235b8b2c-1c23-8061-abde-cbb2ec9df32d" class="bulleted-list"><li style="list-style-type:disc">위치: Socket의 Send/Receive 지점</li></ul><ul id="235b8b2c-1c23-80c9-8e50-cc71a88b12e7" class="bulleted-list"><li style="list-style-type:disc">용도: 특정 애플리케이션 소켓에 대한 패킷을 필터링하거나 수정</li></ul><ul id="235b8b2c-1c23-80a2-908b-c5a0d4b81f2d" class="bulleted-list"><li style="list-style-type:disc">예시: 특정 포트나 프로토콜의 소켓 트래픽을 모니터링하거나 차단</li></ul><p id="235b8b2c-1c23-8068-a79a-d4b36fd67934" class=""><strong>4.1.3-2. Socket Options</strong></p><ul id="235b8b2c-1c23-8094-a855-f1cabc514f6b" class="bulleted-list"><li style="list-style-type:disc">위치: 소켓 옵션을 설정하는 지점에 BPF 프로그램 연결</li></ul><ul id="235b8b2c-1c23-80f3-bbe6-ce62f3ce2a65" class="bulleted-list"><li style="list-style-type:disc">용도: 애플리케이션별 커스텀 필터링 로직 구현</li></ul><ul id="235b8b2c-1c23-8088-a2c6-e9cd63e7cf01" class="bulleted-list"><li style="list-style-type:disc">예시: 특정 애플리케이션 트래픽에 대한 방화벽 규칙 적용</li></ul></div></p><p id="235b8b2c-1c23-80ae-97d9-fa66e5853fce" class="">
</p><p id="235b8b2c-1c23-80f9-8770-e9eff9f1a8aa" class=""><strong>4.1.4. System call 계층</strong></p><p id="235b8b2c-1c23-808f-a4f2-f1f749f6a295" class="">시스템 호출 인터페이스에 Hook을 걸어 프로세스와 커널 간 상호작용을 제어한다.<div class="indented"><p id="235b8b2c-1c23-80a1-af5c-d7323e66a03a" class=""><strong>4.1.4-1. kprobe / kretprobe</strong></p><ul id="235b8b2c-1c23-801c-a72a-d484ed012f28" class="bulleted-list"><li style="list-style-type:disc">위치: 커널 함수 진입(kprobe)과 종료(kretprobe) 지점</li></ul><ul id="235b8b2c-1c23-8063-bb4d-eb321e2379fb" class="bulleted-list"><li style="list-style-type:disc">용도: 커널 함수가 호출될 때 파라미터를 추적하거나 로깅</li></ul><ul id="235b8b2c-1c23-8065-8e14-cfd5ea5a569a" class="bulleted-list"><li style="list-style-type:disc">예시: sys_execve 함수에 kprobe를 걸어 프로세스 생성 시 로깅</li></ul><p id="235b8b2c-1c23-8000-81bc-fd86de5237c9" class=""><strong>4.1.4-2. tracepoints</strong></p><ul id="235b8b2c-1c23-80c3-87da-d5cf898e2d1a" class="bulleted-list"><li style="list-style-type:disc">위치: 커널의 특정 이벤트가 발생하는 지점</li></ul><ul id="235b8b2c-1c23-803d-bbee-fa823f9f0580" class="bulleted-list"><li style="list-style-type:disc">용도: I/O 작업, 네트워크 트래픽, 프로세스 생성 등의 이벤트 추적</li></ul><ul id="235b8b2c-1c23-80c9-b149-dffcc8979587" class="bulleted-list"><li style="list-style-type:disc">예시: 시스템 호출 모니터링, 특정 파일 접근 추적</li></ul></div></p></div></p><p id="235b8b2c-1c23-80e1-b159-f774f0f2990b" class=""><div class="indented"><p id="235b8b2c-1c23-806b-81d0-c596c586758c" class=""><strong>4.1.5. Filesystem 및 Process 계층</strong></p><p id="235b8b2c-1c23-805d-8733-f8002bd2ca8a" class="">파일 접근과 프로세스 이벤트에 Hook을 걸어 보안 정책을 적용하거나 모니터링한다.<div class="indented"><p id="235b8b2c-1c23-808c-87ee-c5478ba0b74a" class=""><strong>4.1.5-1. LSM (Linux Security Modules) Hooks</strong></p><ul id="235b8b2c-1c23-80a6-93c1-fb25bc9a8e83" class="bulleted-list"><li style="list-style-type:disc">위치: 커널의 파일 접근, 프로세스 권한 변경, 네트워크 접속 등 보안 관련 지점</li></ul><ul id="235b8b2c-1c23-800f-b3ec-f7d23625ea2c" class="bulleted-list"><li style="list-style-type:disc">용도: 보안 정책 구현 (e.g., SELinux, AppArmor)</li></ul><ul id="235b8b2c-1c23-8019-be02-e4e1f23d1c99" class="bulleted-list"><li style="list-style-type:disc">예시: 특정 프로세스의 파일 접근 차단, 네트워크 사용 제한</li></ul><p id="235b8b2c-1c23-808d-963f-d610b71e7f5d" class=""><strong>4.1.5-2. cgroup BPF</strong></p><ul id="235b8b2c-1c23-802f-b617-f54c386deff8" class="bulleted-list"><li style="list-style-type:disc">위치: cgroup에 연결된 리소스 제한과 네트워크 트래픽 제어 지점</li></ul><ul id="235b8b2c-1c23-807c-a265-c3496ebbec39" class="bulleted-list"><li style="list-style-type:disc">용도: 특정 컨테이너나 프로세스 그룹에 대해 네트워크 및 CPU 사용을 제한</li></ul><ul id="235b8b2c-1c23-800a-a37e-d03428c3d9fe" class="bulleted-list"><li style="list-style-type:disc">예시: Kubernetes 컨테이너 네트워크 정책 설정</li></ul><p id="235b8b2c-1c23-8013-a1f5-ff4aa76ca11f" class="">
</p></div></p><p id="235b8b2c-1c23-8035-9145-ef1caa31cc5f" class=""><strong>4.1.6. UserSpace 계층</strong></p><p id="235b8b2c-1c23-80a2-8b68-fd14b6c0b2dc" class="">eBPF는 커널 내에서 동작하는 프로그램이지만, userspace와 커널 간의 통신을 통해 애플리케이션 레벨의 트래픽을 추적하고 제어할 수 있다.<div class="indented"><p id="235b8b2c-1c23-80da-b2a4-e49bfe5bf10f" class="">4.1.6-1. uprobe / uretprobe</p><ul id="235b8b2c-1c23-804b-b733-ed35b60e4db1" class="bulleted-list"><li style="list-style-type:disc">위치: userspace의 특정 함수에 Hook을 걸어 모니터링.</li></ul><ul id="235b8b2c-1c23-8077-b9fb-e3818326d158" class="bulleted-list"><li style="list-style-type:disc">예시: 데이터베이스 클라이언트 함수에 uprobe를 걸어 쿼리 지연 시간 측정 가능</li></ul><p id="235b8b2c-1c23-80bf-9d2e-e98089c83a2c" class="">4.1.6-2.Envoy + eBPF</p><p id="235b8b2c-1c23-801c-a163-f703f19078b8" class="">Envoy와 eBPF를 연동해 애플리케이션의 L7 트래픽을 제어.</p><ul id="235b8b2c-1c23-8034-93af-cc7721a6471d" class="bulleted-list"><li style="list-style-type:disc">예시: HTTP/gRPC 호출 추적, 보안 정책 적용.</li></ul><p id="235b8b2c-1c23-80ec-b507-fb160cc3b2c4" class="">
</p></div></p></div></p></details></li></ul><ul id="235b8b2c-1c23-80ab-a420-f2a8b695f629" class="toggle"><li><details open=""><summary>4.2. 노드 간 ‘<strong>파드 → 파드’</strong> 통신 확인 - <a href="https://docs.cilium.io/en/stable/network/ebpf/lifeofapacket/">Docs</a> , 추천 글(통신 과정 분석) <a href="https://velog.io/@_gyullbb/Cilium">https://velog.io/@_gyullbb/Cilium</a></summary><figure id="235b8b2c-1c23-80a1-83ef-eebcb0b3d06a" class="image"><a href="images/cilium_bpf_egress.svg"><img style="width:1248px" src="images/cilium_bpf_egress.svg"/></a><figcaption>파드에서 빠져나갈때</figcaption></figure><figure id="235b8b2c-1c23-80b0-a973-d69e5f57fe9f" class="image"><a href="images/cilium_bpf_ingress.svg"><img style="width:1392px" src="images/cilium_bpf_ingress.svg"/></a><figcaption>파드로 인입 시</figcaption></figure><ul id="235b8b2c-1c23-80fd-9589-e79271034191" class="bulleted-list"><li style="list-style-type:disc">cilium 정보 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="235b8b2c-1c23-8008-ba14-d062aae02bba" class="code"><code class="language-Bash"># 엔드포인트 정보 확인
kubectl get pod -owide
NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          14h   172.20.1.56   k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-24fj4   1/1     Running   0          14h   172.20.0.75   k8s-w1    &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-hdf8d   1/1     Running   0          14h   172.20.2.64   k8s-w2    &lt;none&gt;           &lt;none&gt;

kubectl get svc,ep webpod
WEBPOD1IP=172.20.0.150

# BPF maps : 목적지 파드와 통신 시 어느곳으로 보내야 될지 확인할 수 있다
c0 map get cilium_ipcache

# webpod-6956b86f4b-24fj4의 pod ip 정보 / identity는 pod의 id / tunnelendpoint는 node ip
c0 map get cilium_ipcache | grep $WEBPOD1IP
172.20.0.75/32      identity=4792 encryptkey=0 tunnelendpoint=192.168.10.101 flags=&lt;none&gt;   sync    

# curl-pod 의 LXC 변수 지정
LXC=&lt;k8s-ctr의 가장 나중에 lxc 이름&gt;
LXC=lxc576a05ffa79c


# Node’s eBPF programs
## list of eBPF programs
c0bpf net show
c0bpf net show | grep $LXC 
lxc576a05ffa79c(12) tcx/ingress cil_from_container prog_id 1444 link_id 22 
lxc576a05ffa79c(12) tcx/egress cil_to_container prog_id 1445 link_id 235 

## Use bpftool prog show id to view additional information about a program, including a list of attached eBPF maps:
c0bpf prog show id &lt;출력된 prog id 입력&gt;

c0bpf prog show id 1444
1444: sched_cls  name cil_from_container  tag 41989045bb171bee  gpl
        loaded_at 2025-07-19T04:06:27+0000  uid 0
        xlated 752B  jited 784B  memlock 4096B  map_ids 219,218,41
        btf_id 460

c0bpf prog show id 1445
1445: sched_cls  name cil_to_container  tag 0b3125767ba1861c  gpl
        loaded_at 2025-07-19T04:06:27+0000  uid 0
        xlated 1448B  jited 1144B  memlock 4096B  map_ids 219,41,218
        btf_id 461

c0bpf map list
...
41: percpu_hash  name cilium_metrics  flags 0x1
        key 8B  value 16B  max_entries 1024  memlock 19024B
...
218: prog_array  name cilium_calls_04  flags 0x0
        key 4B  value 4B  max_entries 50  memlock 720B
        owner_prog_type sched_cls  owner jited
219: array  name .rodata.config  flags 0x480
        key 4B  value 64B  max_entries 1  memlock 8192B
        btf_id 453  frozen
...</code></pre><p id="235b8b2c-1c23-8006-a00c-dfc8960cda21" class="">
</p></li></ul><ul id="235b8b2c-1c23-8071-9ef7-d63f521a3548" class="bulleted-list"><li style="list-style-type:disc">다른 노드 간 ‘파드 → 파드’ 통신 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="235b8b2c-1c23-80b8-b755-c1890c68fa60" class="code"><code class="language-Bash"># vagrant ssh k8s-w1 , # vagrant ssh k8s-w2 각각 터미널 접속 후 아래 실행
ngrep -tW byline -d eth1 &#x27;&#x27; &#x27;tcp port 80&#x27;

# [k8s-ctr] curl-pod 에서 curl 요청 시도
kubectl exec -it curl-pod -- curl $WEBPOD1IP

# pod ip들 확인
kubectl get po -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          14h   172.20.1.56   k8s-ctr   &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-24fj4   1/1     Running   0          14h   172.20.0.75   k8s-w1    &lt;none&gt;           &lt;none&gt;
webpod-6956b86f4b-hdf8d   1/1     Running   0          14h   172.20.2.64   k8s-w2    &lt;none&gt;           &lt;none&gt;

# 각각 터미널에서 출력 확인 : 파드의 소스 IP와 목적지 IP가 다른 노드의 서버 NIC에서 확인! : Native-Routung 
## k8s-w1에서 확인 =&gt; curl pod ip에서 webpod1 ip로 직접 통신 되는것 확인!
interface: eth1 (192.168.10.0/255.255.255.0)
filter: ( tcp port 80 ) and ((ip || ip6) || (vlan &amp;&amp; (ip || ip6)))
####
T 2025/07/19 15:23:54.104990 172.20.1.56:38634 -&gt; 172.20.0.75:80 [AP] #4
GET / HTTP/1.1.
Host: 172.20.0.75.
User-Agent: curl/8.14.1.
Accept: */*.
.

##
T 2025/07/19 15:23:54.106736 172.20.0.75:80 -&gt; 172.20.1.56:38634 [AP] #6
HTTP/1.1 200 OK.
Date: Sat, 19 Jul 2025 06:23:54 GMT.
Content-Length: 206.
Content-Type: text/plain; charset=utf-8.
.
Hostname: webpod-6956b86f4b-24fj4
IP: 127.0.0.1
IP: ::1
IP: 172.20.0.75
IP: fe80::b4cf:afff:fe16:31e
RemoteAddr: 172.20.1.56:38634
GET / HTTP/1.1.
Host: 172.20.0.75.
User-Agent: curl/8.14.1.
Accept: */*.
.</code></pre><p id="235b8b2c-1c23-8028-866b-d8a538075992" class="">
</p></li></ul></details></li></ul><ul id="235b8b2c-1c23-8073-8045-e63699fb6917" class="toggle"><li><details open=""><summary>다른 노드 간 ‘<mark class="highlight-blue"><strong>파드 → 서비스(ClusterIP)</strong></mark>’ 통신 확인* <strong> </strong><a href="https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-%EA%B8%B0%EB%B2%95">https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-%EA%B8%B0%EB%B2%95</a></summary><ul id="235b8b2c-1c23-809e-80e7-cf0f4b81531b" class="bulleted-list"><li style="list-style-type:disc">그림 왼쪽(<strong>네트워크</strong> 기반 로드밸런싱) vs 오른쪽(<strong>소켓</strong> 기반 로드밸런싱)<figure id="235b8b2c-1c23-8048-a036-fa3aaf360581" class="image"><a href="images/Untitled%2038.png"><img style="width:1104px" src="images/Untitled%2038.png"/></a></figure><p id="235b8b2c-1c23-803f-9347-d689f7ecce01" class="">
</p></li></ul><ul id="235b8b2c-1c23-8077-9ba0-c21046e9b3d4" class="bulleted-list"><li style="list-style-type:disc">Pod1 안에서 동작하는 앱이 <strong>connect() 시스템콜</strong>을 이용하여 소켓을 연결할 때 목적지 주소가 서비스 주소(10.10.8.55)이면 소켓의 목적지 주소를 바로 백엔드 주소(10.0.0.31)로 설정한다. </li></ul><ul id="235b8b2c-1c23-80ba-abb9-d0813fd67372" class="bulleted-list"><li style="list-style-type:disc">이후 앱에서 해당 소켓을 통해 보내는 모든 패킷의 목적지 주소는 이미 백엔드 주소(10.0.0.31)로 설정되어 있기 때문에 중간에 <strong>DNAT 변환 및 역변환 과정이 필요없어진다.</strong></li></ul><ul id="235b8b2c-1c23-808a-8ca6-e3516eda8994" class="bulleted-list"><li style="list-style-type:disc"><strong>destination NAT</strong> translation happens at the <strong>syscall</strong> level, <strong>before</strong> the packet is even built by the <strong>kernel</strong>.</li></ul><ul id="235b8b2c-1c23-8065-984f-f97d30ebd63c" class="bulleted-list"><li style="list-style-type:disc"><strong>Socket operations</strong> : <mark class="highlight-red"><strong>BPF socket operations program</strong></mark> 은 <mark class="highlight-red"><strong>root cgroup 에 연결</strong></mark>되며 TCP <strong>event</strong>(ESTABLISHED) 에서 실행한다.</li></ul><ul id="235b8b2c-1c23-80a6-8dde-c9a898f837ee" class="bulleted-list"><li style="list-style-type:disc"><strong>Socket send/recv</strong> : The socket send/recv hook 은 <mark class="highlight-red"><strong>TCP</strong></mark> socket 의 모든 <strong>송수신</strong> 작업에서 실행, <strong>hook</strong> 에서 <strong>검사/삭제/리다이렉션</strong>을 할 수 있다<figure id="235b8b2c-1c23-809c-8584-e5d9749129d8" class="image"><a href="images/Untitled%2011.png"><img style="width:1056px" src="images/Untitled%2011.png"/></a><figcaption><a href="https://cilium.io/blog/2020/11/10/ebpf-future-of-networking/">https://cilium.io/blog/2020/11/10/ebpf-future-of-networking/</a></figcaption></figure><p id="235b8b2c-1c23-80a3-bdb5-fc2d3e354dc7" class="">
</p></li></ul><ul id="235b8b2c-1c23-804d-a192-f17ab8ecf715" class="bulleted-list"><li style="list-style-type:disc">파드 네임스페이스에서 Socket-Based LoadBalancing 기법<figure id="235b8b2c-1c23-806e-968c-dfdbabba157d" class="image"><a href="images/Untitled%2039.png"><img style="width:1104px" src="images/Untitled%2039.png"/></a><figcaption><a href="https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-%EA%B8%B0%EB%B2%95">https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-기법</a></figcaption></figure><p id="235b8b2c-1c23-8051-b127-f1882da98244" class="">
</p></li></ul><ul id="235b8b2c-1c23-80b8-af17-ef01ff01c073" class="bulleted-list"><li style="list-style-type:disc">connect() 와 sendto() 소켓 함수에 연결된 프로그램(connect4, sendmsg4)에서는 소켓의 목적지 주소를 백엔드 주소와 포트로 변환하고, cilium_lb4_backends 맵에 백엔드 주소와 포트를 등록해놓는다. </li></ul><ul id="235b8b2c-1c23-8093-bcc9-efdc734d210e" class="bulleted-list"><li style="list-style-type:disc">이후 recvmsg() 소켓 함수에 연결된 프로그램(recvmsg4)에서는 cilium_lb4_reverse_nat 맵을 이용해서 목적지 주소와 포트를 다시 서비스 주소와 포트로 변환함<figure id="235b8b2c-1c23-80f2-b054-fbb7b089a0d5" class="image"><a href="images/Untitled%2040.png"><img style="width:960px" src="images/Untitled%2040.png"/></a><figcaption><a href="https://k8s.networkop.co.uk/services/clusterip/dataplane/ebpf/">https://k8s.networkop.co.uk/services/clusterip/dataplane/ebpf/</a></figcaption></figure><p id="235b8b2c-1c23-8051-95ea-f8134152cfd9" class="">
</p></li></ul><ul id="235b8b2c-1c23-8002-8fa6-f254b90ec611" class="bulleted-list"><li style="list-style-type:disc">실습 동작 확인<figure id="235b8b2c-1c23-8064-acde-e2a97b394dde" class="image"><a href="images/image%2013.png"><img style="width:960px" src="images/image%2013.png"/></a></figure><figure id="235b8b2c-1c23-80cc-9d0b-fa8a1f8132a5" class="image"><a href="images/image%2014.png"><img style="width:960px" src="images/image%2014.png"/></a><figcaption><a href="https://daniel00324.tistory.com/15">https://daniel00324.tistory.com/15</a></figcaption></figure><figure id="235b8b2c-1c23-80f3-b7fc-d8011d139bda" class="image"><a href="images/image%2015.png"><img style="width:960px" src="images/image%2015.png"/></a><figcaption><a href="https://daniel00324.tistory.com/15">https://daniel00324.tistory.com/15</a></figcaption></figure></li></ul><p id="235b8b2c-1c23-8013-8cf4-eb98d306301b" class="">
</p><ul id="235b8b2c-1c23-8020-9445-ed2ae1883a50" class="bulleted-list"><li style="list-style-type:disc">실습 확인<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="235b8b2c-1c23-801a-8c45-ec4579fe71e6" class="code"><code class="language-Bash"># curl 호출
kubectl exec -it curl-pod -- curl webpod

# 신규 터미널 : 파드에서 SVC(ClusterIP) 접속 시 tcpdump 로 확인 : ClusterIP가 소켓 레벨에서 이미 Endpoint 로 변경되었음을 확인!
kubectl exec curl-pod -- tcpdump -enni any -q
06:19:43.214116 eth0  Out ifindex 17 0e:0e:1f:0b:b4:70 172.20.2.70.33950 &gt; 172.20.1.25.80: tcp 0
06:19:43.215346 eth0  In  ifindex 17 6a:20:d8:21:a8:1e 172.20.1.25.80 &gt; 172.20.2.70.33950: tcp 0


### Socket-Based LoadBalancing 관련 설정들 확인

# cilium agent가 root cgroup을 mount한다.
cilium config view | grep cgroup
cgroup-root                                       /run/cilium/cgroupv2

c0bpf cgroup tree
CgroupPath
ID       AttachType      AttachFlags     Name           
/sys/fs/cgroup
1000     cgroup_device   multi           sd_devices                     
1004     cgroup_device   multi 

# 앞서 설명했듯이 -&gt; Socket operations : BPF socket operations program 은 root cgroup 에 연결되며 TCP event(ESTABLISHED) 에서 실행됨
kc get po -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
...
cilium-envoy-m2dtg                 1/1     Running   0          15h
cilium-envoy-vtnvn                 1/1     Running   0          15h
cilium-envoy-vtq7x                 1/1     Running   0          15h
cilium-ft6rv                       1/1     Running   0          14h
cilium-t2f2s                       1/1     Running   0          14h
cilium-wfskr                       1/1     Running   0          14h
...

kubectl describe po -n kube-system cilium-ft6rv
...
Init Containers:
  config:
    Container ID:  containerd://494e3aa52b1bea930658e50f94c98c928c8d0ac467d1bcd8760e015d4e8a4e25
    Image:         quay.io/cilium/cilium:v1.17.5@sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6
    Image ID:      quay.io/cilium/cilium@sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
...
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c6hxq (ro)
  mount-cgroup:
    Container ID:  containerd://67f2acc2563804528e1295f73b20f5ae67b3f604cd73b7c1489695cbf3fdd538
    Image:         quay.io/cilium/cilium:v1.17.5@sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6
    Image ID:      quay.io/cilium/cilium@sha256:baf8541723ee0b72d6c489c741c81a6fdc5228940d66cb76ef5ea2ce3c639ea6
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -ec
      cp /usr/bin/cilium-mount /hostbin/cilium-mount;
      nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt &quot;${BIN_PATH}/cilium-mount&quot; $CGROUP_ROOT;
      rm /hostbin/cilium-mount
      
# k8s에 존재하는 모든 service에서 socket-based loadbalancing이 가능!!
c0 status --verbose
...
KubeProxyReplacement Details:
  Status:                 True
  Socket LB:              Enabled
  Socket LB Tracing:      Enabled
  Socket LB Coverage:     Full
  Devices:                eth0    10.0.2.15 fd17:625c:f037:2:a00:27ff:fe71:19d8 fe80::a00:27ff:fe71:19d8, eth1   192.168.10.100 fe80::a00:27ff:fe25:6cff (Direct Routing)
  Mode:                   SNAT
  Backend Selection:      Random
  Session Affinity:       Enabled
  Graceful Termination:   Enabled
  NAT46/64 Support:       Disabled
  XDP Acceleration:       Disabled
  Services:
  - ClusterIP:      Enabled
  - NodePort:       Enabled (Range: 30000-32767) 
  - LoadBalancer:   Enabled 
  - externalIPs:    Enabled 
  - HostPort:       Enabled
  Annotations:
  - service.cilium.io/node
  - service.cilium.io/src-ranges-policy
  - service.cilium.io/type

# syacall 호출 확인
kubectl exec curl-pod -- strace -c curl -s webpod
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 15.49    0.000472          13        35           munmap
 12.67    0.000386          42         9           ppoll
 12.64    0.000385         128         3         1 connect
 11.72    0.000357           7        47        30 openat
 10.73    0.000327          23        14           rt_sigprocmask
  6.37    0.000194          64         3           sendto
  5.68    0.000173           7        22           close
  4.53    0.000138           2        63           mmap
  3.94    0.000120          20         6         3 recvfrom
  3.71    0.000113           9        12           fstat
  3.15    0.000096          96         1           writev
  2.76    0.000084          84         1           execve
  1.54    0.000047           1        27           read
  1.54    0.000047           1        24           fcntl
  0.72    0.000022           1        14           mprotect
  0.72    0.000022          22         1           set_tid_address
  0.69    0.000021           7         3         3 ioctl
  0.66    0.000020           5         4           brk
  0.49    0.000015           0        28           rt_sigaction
  0.10    0.000003           0         5           getsockname
  0.07    0.000002           0         5           setsockopt
  0.07    0.000002           0         4           socket
  0.03    0.000001           1         1           getsockopt
  0.00    0.000000           0         1           eventfd2
  0.00    0.000000           0        10           lseek
  0.00    0.000000           0         3           readv
  0.00    0.000000           0         1           newfstatat
  0.00    0.000000           0         1           getuid
  0.00    0.000000           0         2           geteuid
  0.00    0.000000           0         1           getgid
  0.00    0.000000           0         1           getegid
  0.00    0.000000           0         1           getrandom
------ ----------- ----------- --------- --------- ----------------
100.00    0.003047           8       353        37 total

# 상세 출력
kubectl exec curl-pod -- strace -s 65535 -f -tt curl -s webpod
...

# 특정 이벤트 필터링 : -e
# coredns로 domain resolve 요청 -&gt; webpod service clusterIP로 connect 요청
kubectl exec curl-pod -- strace -e trace=connect     curl -s webpod
connect(4, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr(&quot;10.96.0.10&quot;)}, 16) = 0
connect(5, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;10.96.227.33&quot;)}, 16) = 0
connect(4, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;10.96.227.33&quot;)}, 16) = -1 EINPROGRESS (Operation in progress)
...

# getsockname으로 client ip와 port를 확인
kubectl exec curl-pod -- strace -e trace=getsockname curl -s webpod
getsockname(4, {sa_family=AF_INET, sin_port=htons(55139), sin_addr=inet_addr(&quot;172.20.1.56&quot;)}, [128 =&gt; 16]) = 0
getsockname(5, {sa_family=AF_INET, sin_port=htons(60469), sin_addr=inet_addr(&quot;172.20.1.56&quot;)}, [16]) = 0
getsockname(4, {sa_family=AF_INET, sin_port=htons(57374), sin_addr=inet_addr(&quot;172.20.1.56&quot;)}, [128 =&gt; 16]) = 0
getsockname(4, {sa_family=AF_INET, sin_port=htons(57374), sin_addr=inet_addr(&quot;172.20.1.56&quot;)}, [128 =&gt; 16]) = 0
getsockname(4, {sa_family=AF_INET, sin_port=htons(57374), sin_addr=inet_addr(&quot;172.20.1.56&quot;)}, [128 =&gt; 16]) = 0

# k8s-w1에서 ngrep을 통해 실제 src ip:[port] -&gt; dst ip:[port] 확인
ngrep -tW byline -d eth1 &#x27;&#x27; &#x27;tcp port 80&#x27;
interface: eth1 (192.168.10.0/255.255.255.0)
filter: ( tcp port 80 ) and ((ip || ip6) || (vlan &amp;&amp; (ip || ip6)))
####
T 2025/07/19 23:05:13.410773 172.20.1.56:57374 -&gt; 172.20.0.75:80 [AP] #4
GET / HTTP/1.1.
Host: webpod.
User-Agent: curl/8.14.1.
Accept: */*.
.

##
T 2025/07/19 23:05:13.411403 172.20.0.75:80 -&gt; 172.20.1.56:57374 [AP] #6
...

kubectl exec curl-pod -- strace -e trace=getsockopt curl -s webpod 
getsockopt(4, SOL_SOCKET, SO_ERROR, [0], [4]) = 0 # 소켓 연결 성공
</code></pre><ul id="235b8b2c-1c23-8051-a1c9-ff9421d562d1" class="toggle"><li><details open=""><summary>(심화) <strong>strace</strong> : 시스템 콜 트레이싱 도구(디버거) , 시스템 콜 상태(커널 모드로 실행 중이거나 대기 상태) </summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="235b8b2c-1c23-8087-9439-d569c57629a8" class="code"><code class="language-Bash"># 중단점 트레이싱 : -ttt(첫 열에 기준시간으로부터 흐른 시간 표시) , -T(마지막 필드 time에 시스템 콜에 걸린 시간을 표시) , -p PID(프로세스 ID가 PID 인 프로세스를 트레이싱)
strace -ttt -T -p 1884

# 시스템 콜별 통계
strace -c -p 1884

# 그냥 사용해보기
strace ls

# 옵션 사용해보기 : -s(출력 string 결과 최댓값 지정), -tt(첫 열에 기준시간으로부터 흐른 시간 표시, ms단위), -f(멀티 스레드,멀티 프로레스의 자식 프로세스의 시스템 콜 추적)
strace -s 65535 -f -T -tt -o &lt;파일명&gt; -p &lt;pid&gt;

# hostname 명령 분석하기 : -o &lt;파일명&gt; 출력 결과를 파일로 떨구기
strace -s 65535 -f -T -tt -o hostname_f_trace hostname -f

# 특정 이벤트 : -e
strace -e trace=connect curl ipinfo.io</code></pre></details></li></ul><p id="235b8b2c-1c23-802e-adca-c855461798b7" class="">
</p></li></ul><ul id="235b8b2c-1c23-8058-b2b8-eaa2793ba6d8" class="toggle"><li><details open=""><summary>(정보) 소켓 기반 로드밸런싱 사용 시 <strong>Istio</strong>(EnvoyProxy)와 같은 사이드카 <strong>우회</strong> 문제 - <a href="https://velog.io/@haruband/K8S-Cilium-Socket-based-LoadBalancing-%EC%97%90-%EC%9D%98%ED%95%9C-Istio-Envoy-%EC%9A%B0%ED%9A%8C-%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D">링크</a></summary><figure id="235b8b2c-1c23-80f8-ab4e-f81fd20ec5ce" class="image"><a href="images/Untitled%2041.png"><img style="width:816px" src="images/Untitled%2041.png"/></a></figure><ul id="235b8b2c-1c23-80f4-9afb-e2cb1d1892f8" class="bulleted-list"><li style="list-style-type:disc">해결 방안 : 파드 네임스페이스에서는 소켓 기반 로드밸런싱을 사용하지 않는다 → 호스트 네임스페이스만 사용하게 설정!<ul id="235b8b2c-1c23-80bc-9904-f4b6f8962c0c" class="bulleted-list"><li style="list-style-type:circle">단, HTTP 경우 Envoy 의 HTTP 필터가 HTTP 패킷의 host 헤더로 필터링하여 패킷의 목적지 주소가 서비스 IP에서 백엔드 IP로 변환이 잘된다</li></ul><ul id="235b8b2c-1c23-8051-ba8c-e98feba55fd1" class="bulleted-list"><li style="list-style-type:circle">하지만,  HTTP가 아닌 TCP 서비스(예. Telnet 등)은 위 환경에서 문제가 발생한다.</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="235b8b2c-1c23-8080-af0a-c7f01c443978" class="code"><code class="language-Bash"># 설정
VERSION=1.17.5
helm upgrade cilium cilium/cilium --version $VERSION --namespace kube-system --reuse-values \
  --set socketLB.hostNamespaceOnly=true
kubectl -n kube-system rollout restart ds/cilium

cilium config view | grep bpf-lb-sock-hostns-only
bpf-lb-sock-hostns-only                        true

kubectl -n kube-system exec ds/cilium -- cilium-dbg status --verbose
...
KubeProxyReplacement Details:
  Status:                 True
  Socket LB:              Enabled
  Socket LB Tracing:      Enabled
  Socket LB Coverage:     Hostns-only

## pod namespace에서 확인
# 확인 curl-pod에서 webpod service 로 요청
kubectl exec curl-pod -- curl webpod
IP: 127.0.0.1
IP: ::1
IP: 172.20.2.64
IP: fe80::44f4:96ff:fe97:440a
RemoteAddr: 172.20.1.56:44252
GET / HTTP/1.1
Host: webpod
User-Agent: curl/8.14.1
Accept: */*

# 파드에서 SVC(ClusterIP) 접속 시 tcpdump 로 확인 &gt;&gt; 파드 내부 캡쳐인데, WEBPOD SVC(10.96.227.33) 트래픽이 보인다!
# 또한 coredns에 대한 요청도 pod ip가 아닌 coredns service ip가 보인다!!
kubectl exec curl-pod -- tcpdump -enni any -q
tcpdump: WARNING: any: That device doesn&#x27;t support promiscuous mode
(Promiscuous mode not supported on the &quot;any&quot; device)
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
14:59:55.393236 eth0  Out ifindex 11 82:62:17:f4:0a:9a 172.20.1.56.35345 &gt; 10.96.0.10.53: UDP, length 73
14:59:55.393432 eth0  Out ifindex 11 82:62:17:f4:0a:9a 172.20.1.56.35345 &gt; 10.96.0.10.53: UDP, length 73
14:59:55.393988 eth0  In  ifindex 11 aa:1a:b3:e8:d4:45 10.96.0.10.53 &gt; 172.20.1.56.35345: UDP, length 166
14:59:55.394235 eth0  In  ifindex 11 aa:1a:b3:e8:d4:45 10.96.0.10.53 &gt; 172.20.1.56.35345: UDP, length 121
14:59:55.394492 eth0  Out ifindex 11 82:62:17:f4:0a:9a 172.20.1.56.44252 &gt; 10.96.227.33.80: tcp 0
14:59:55.394970 eth0  In  ifindex 11 aa:1a:b3:e8:d4:45 10.96.227.33.80 &gt; 172.20.1.56.44252: tcp 0
...

## host namespace에서 확인
# host에서 직접 webpod service ip로 호출
root@k8s-ctr:~# curl 10.96.227.33
Hostname: webpod-6956b86f4b-hdf8d
IP: 127.0.0.1
IP: ::1
IP: 172.20.2.64
IP: fe80::44f4:96ff:fe97:440a
RemoteAddr: 192.168.10.100:49146
GET / HTTP/1.1
Host: 10.96.227.33
User-Agent: curl/8.5.0
Accept: */*

# host에서 젝접 SVC(ClusterIP) 접속 시 tcpdump 로 확인 &gt;&gt; 목적지 주소가 서비스 IP에서 백엔드 IP(172.20.2.64.80)로 변환이되어 트래픽이 보인다!
root@k8s-ctr:~# tcpdump -enni any -q dst port 80
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
00:08:47.777246 eth1  Out ifindex 3 08:00:27:25:6c:ff 192.168.10.100.49146 &gt; 172.20.2.64.80: tcp 0
00:08:47.777875 eth1  Out ifindex 3 08:00:27:25:6c:ff 192.168.10.100.49146 &gt; 172.20.2.64.80: tcp 0
00:08:47.777915 eth1  Out ifindex 3 08:00:27:25:6c:ff 192.168.10.100.49146 &gt; 172.20.2.64.80: tcp 75</code></pre><p id="235b8b2c-1c23-80ba-8672-de1f1f474dbb" class="">
</p></details></li></ul><ul id="235b8b2c-1c23-8083-a9e7-e3aad1174a88" class="toggle"><li><details open=""><summary>socket-LB Issue : Service ClusterIP로 NFS and SMB mounts(Longhorn, Portworx, and Robin) 사용하는 경우 마운트 X - <a href="https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#limitations">Docs</a> , <a href="https://github.com/cilium/cilium/issues/21541">Issue</a></summary><ul id="235b8b2c-1c23-804e-9cbb-d55be363f626" class="bulleted-list"><li style="list-style-type:disc">Cilium’s eBPF kube-proxy replacement relies upon the socket-LB feature which uses eBPF cgroup hooks to implement the service translation. Using it with libceph deployments currently requires support for the getpeername(2) hook address translation in eBPF, which is only available for kernels v5.8 and higher.</li></ul><ul id="235b8b2c-1c23-8087-b82d-eeec8ff29768" class="bulleted-list"><li style="list-style-type:disc"><strong>NFS and SMB mounts</strong> may break when mounted to a <code>Service</code> <strong>cluster IP</strong> while using socket-LB. This issue is known to impact <strong>Longhorn, Portworx, and Robin</strong>, but may impact other storage systems that implement <code>ReadWriteMany</code> volumes using this pattern. To avoid this problem, ensure that the following commits are part of your underlying kernel:<ul id="235b8b2c-1c23-80a3-ab0d-f1acbdc31975" class="bulleted-list"><li style="list-style-type:circle"><code>0bdf399342c5 (&quot;net: Avoid address overwrite in kernel_connect&quot;)</code></li></ul><ul id="235b8b2c-1c23-80a4-90e1-e32d29eb6f86" class="bulleted-list"><li style="list-style-type:circle"><code>86a7e0b69bd5 (&quot;net: prevent rewrite of msg_name in sock_sendmsg()&quot;)</code></li></ul><ul id="235b8b2c-1c23-805a-b85f-cdc58724c11d" class="bulleted-list"><li style="list-style-type:circle"><code>01b2885d9415 (&quot;net: Save and restore msg_namelen in sock_sendmsg&quot;)</code></li></ul><ul id="235b8b2c-1c23-80df-b0c9-fbfcb19c16b4" class="bulleted-list"><li style="list-style-type:circle"><code>cedc019b9f26 (&quot;smb: use kernel_connect() and kernel_bind()&quot;)</code> (SMB only)</li></ul><p id="235b8b2c-1c23-8056-b741-c4c1f97178c6" class="">These patches have been backported to all stable kernels and some distro-specific kernels:<em><mark class="highlight-blue"><strong> ⇒ 아래 버전 이상 시 해결</strong></mark></em></p><ul id="235b8b2c-1c23-80ab-af34-f36ee126825f" class="bulleted-list"><li style="list-style-type:circle"><strong>Ubuntu</strong>: <code>5.4.0-187-generic</code>, <code>5.15.0-113-generic</code>, <code>6.5.0-41-generic</code> or newer.</li></ul><ul id="235b8b2c-1c23-80ee-abdf-c6769496b9cb" class="bulleted-list"><li style="list-style-type:circle"><strong>RHEL 8</strong>: <code>4.18.0-553.8.1.el8_10.x86_64</code> or newer (RHEL 8.10+).</li></ul><ul id="235b8b2c-1c23-808c-883b-eced2b01d691" class="bulleted-list"><li style="list-style-type:circle"><strong>RHEL 9</strong>: <code>kernel-5.14.0-427.31.1.el9_4</code> or newer (RHEL 9.4+).</li></ul><p id="235b8b2c-1c23-80a1-a778-ef2b0ec337d8" class="">For a more detailed discussion see <a href="https://github.com/cilium/cilium/issues/21541">GitHub issue 21541</a>.</p></li></ul></details></li></ul><ul id="235b8b2c-1c23-8062-a69e-d460a4bbd33b" class="bulleted-list"><li style="list-style-type:disc"><code>도전과제</code> 파드 내 Socket-Based LB 기법이 문제가 되는 환경을 재연(istio, Longhorn 등)해보고, 파드내 Socket LB를 Off해서 해결해보자</li></ul><ul id="235b8b2c-1c23-8009-9bdf-dd4c441131d0" class="bulleted-list"><li style="list-style-type:disc"><strong>실습 환경 삭제</strong> : <code>vagrant destroy -f &amp;&amp; rm -rf .vagrant</code></li></ul><ul id="235b8b2c-1c23-806e-9a41-e0561bb28589" class="bulleted-list"><li style="list-style-type:disc">참고<p id="235b8b2c-1c23-80ed-ba64-db02ef2228a0" class=""><strong>[Youtube] echo Life of Packet with Cilium</strong><div class="indented"><figure id="235b8b2c-1c23-8087-b6c1-c5662fd4f530"><div class="source"><a href="https://www.youtube.com/watch?v=Pju0MQRblmc">https://www.youtube.com/watch?v=Pju0MQRblmc</a></div></figure><figure id="235b8b2c-1c23-80ed-9427-d5cdf8661dee"><div class="source"><a href="https://www.youtube.com/watch?v=SGfMEpjq07Q">https://www.youtube.com/watch?v=SGfMEpjq07Q</a></div></figure><figure id="235b8b2c-1c23-8052-93b2-ff11647e9be4"><div class="source"><a href="https://www.youtube.com/watch?v=0BKU6avwS98">https://www.youtube.com/watch?v=0BKU6avwS98</a></div></figure><p id="235b8b2c-1c23-804b-97fd-e0d2ae4ed7ce" class="">
</p></div></p></li></ul></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>